{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76955651",
   "metadata": {},
   "source": [
    "## **Laboratorio 6**\n",
    "- JoaquÃ­n Campos - 22155\n",
    "- SofÃ­a GarcÃ­a - 22210\n",
    "- Julio GarcÃ­a Salas - 22076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f1bdb",
   "metadata": {},
   "source": [
    "## **Inciso 1 y 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba436c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasos 1â€“2 (con detecciÃ³n de codificaciÃ³n y JSON doble)\n",
    "# - Lee traficogt.txt y tioberny.txt (UTF-16 / UTF-8, etc.)\n",
    "# - Parseo robusto: JSON array, JSONL, JSON doblemente codificado, o texto plano\n",
    "# - Normaliza a DataFrame con campos clave para siguientes pasos\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# -------------------- LocalizaciÃ³n de archivos --------------------\n",
    "def resolve_paths() -> List[Path]:\n",
    "    local = [Path(\"traficogt.txt\"), Path(\"tioberny.txt\")]\n",
    "    mnt = [Path(\"/mnt/data/traficogt.txt\"), Path(\"/mnt/data/tioberny.txt\")]\n",
    "    out = []\n",
    "    for pl, pm in zip(local, mnt):\n",
    "        out.append(pl if pl.exists() else (pm if pm.exists() else pl))\n",
    "    return out\n",
    "\n",
    "DATA_PATHS = resolve_paths()\n",
    "OUT_PARQUET = Path(\"tweets_raw.parquet\")\n",
    "OUT_SAMPLE_CSV = Path(\"tweets_raw_sample.csv\")\n",
    "\n",
    "# -------------------- Utilidades de lectura/decodificaciÃ³n --------------------\n",
    "CANDIDATE_ENCODINGS = [\"utf-8\", \"utf-16\", \"utf-16-le\", \"utf-16-be\", \"latin-1\"]\n",
    "\n",
    "def read_text_auto(path: Path) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Lee bytes y prueba varias codificaciones. Si la decodificaciÃ³n produce muchos \\x00,\n",
    "    intenta con UTF-16 variantes. Devuelve (texto, encoding_usada).\n",
    "    \"\"\"\n",
    "    data = path.read_bytes()\n",
    "    last_err = None\n",
    "    # HeurÃ­stica rÃ¡pida: si hay muchos 0x00, probablemente es UTF-16\n",
    "    zero_ratio = data.count(0) / max(1, len(data))\n",
    "    preferred = ([\"utf-16\"] + CANDIDATE_ENCODINGS) if zero_ratio > 0.01 else CANDIDATE_ENCODINGS\n",
    "    tried = []\n",
    "    for enc in preferred:\n",
    "        try:\n",
    "            txt = data.decode(enc, errors=\"strict\")\n",
    "            # si aÃºn hay muchÃ­simos NUL tras decodificar, seguimos probando\n",
    "            if txt.count(\"\\x00\") > 10:\n",
    "                tried.append((enc, \"nul_after_decode\"))\n",
    "                continue\n",
    "            return txt, enc\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            tried.append((enc, \"error\"))\n",
    "            continue\n",
    "    # fallback permisivo\n",
    "    return data.decode(\"utf-8\", errors=\"ignore\"), \"utf-8(ignore)\"\n",
    "\n",
    "# -------------------- Utilidades de parseo JSON --------------------\n",
    "def json_maybe_twice(s: str):\n",
    "    \"\"\"\n",
    "    Intenta json.loads; si devuelve un string que comienza con '{' o '[',\n",
    "    intenta decodificar una segunda vez (JSON doblemente codificado).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, str):\n",
    "            st = obj.strip()\n",
    "            if (st.startswith(\"{\") and st.endswith(\"}\")) or (st.startswith(\"[\") and st.endswith(\"]\")):\n",
    "                try:\n",
    "                    return json.loads(st)\n",
    "                except Exception:\n",
    "                    return obj\n",
    "        return obj\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def read_any_json_whole(raw_stripped: str):\n",
    "    \"\"\"Parsea todo el contenido como JSON array/obj (con soporte a doble carga).\"\"\"\n",
    "    obj = json_maybe_twice(raw_stripped)\n",
    "    if obj is None:\n",
    "        return None\n",
    "    if isinstance(obj, list):\n",
    "        return [x for x in obj if isinstance(x, dict)]\n",
    "    if isinstance(obj, dict):\n",
    "        if \"tweets\" in obj and isinstance(obj[\"tweets\"], list):\n",
    "            return [x for x in obj[\"tweets\"] if isinstance(x, dict)]\n",
    "        if \"data\" in obj and isinstance(obj[\"data\"], list):\n",
    "            return [x for x in obj[\"data\"] if isinstance(x, dict)]\n",
    "        return [obj]\n",
    "    return None\n",
    "\n",
    "def read_jsonl_lines(raw: str):\n",
    "    \"\"\"JSONL: una entrada por lÃ­nea; soporta doble decodificaciÃ³n.\"\"\"\n",
    "    recs = []\n",
    "    ok = 0\n",
    "    for line in raw.splitlines():\n",
    "        s = line.strip().rstrip(\",\")\n",
    "        if not s:\n",
    "            continue\n",
    "        obj = json_maybe_twice(s)\n",
    "        if isinstance(obj, dict):\n",
    "            recs.append(obj); ok += 1\n",
    "    return recs, ok\n",
    "\n",
    "def extract_json_from_line(line: str):\n",
    "    \"\"\"Intenta rescatar el primer bloque {...} o [...] de una lÃ­nea y decodificarlo.\"\"\"\n",
    "    s = line.strip()\n",
    "    start_obj = s.find(\"{\"); end_obj = s.rfind(\"}\")\n",
    "    start_arr = s.find(\"[\"); end_arr = s.rfind(\"]\")\n",
    "    cand = None\n",
    "    if start_obj != -1 and end_obj > start_obj:\n",
    "        cand = s[start_obj:end_obj+1]\n",
    "    elif start_arr != -1 and end_arr > start_arr:\n",
    "        cand = s[start_arr:end_arr+1]\n",
    "    if cand:\n",
    "        return json_maybe_twice(cand)\n",
    "    return None\n",
    "\n",
    "# -------------------- NormalizaciÃ³n de registros --------------------\n",
    "def norm_username(u: Optional[str]) -> Optional[str]:\n",
    "    if not u:\n",
    "        return u\n",
    "    u = u.strip()\n",
    "    if u.startswith(\"@\"):\n",
    "        u = u[1:]\n",
    "    return u.lower()\n",
    "\n",
    "def extract_list_usernames(mentioned: Any) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    if isinstance(mentioned, list):\n",
    "        for m in mentioned:\n",
    "            if isinstance(m, dict):\n",
    "                un = m.get(\"username\") or m.get(\"screen_name\") or m.get(\"name\")\n",
    "                if un: out.append(norm_username(un))\n",
    "            elif isinstance(m, str):\n",
    "                out.append(norm_username(m))\n",
    "    return [x for x in out if x]\n",
    "\n",
    "def hashtags_to_list(h: Any) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    if isinstance(h, list):\n",
    "        for item in h:\n",
    "            if isinstance(item, str):\n",
    "                out.append(item.lstrip(\"#\").lower())\n",
    "            elif isinstance(item, dict):\n",
    "                txt = item.get(\"text\") or item.get(\"tag\")\n",
    "                if txt: out.append(str(txt).lstrip(\"#\").lower())\n",
    "    return out\n",
    "\n",
    "def get_text(rec: Dict[str, Any]) -> Optional[str]:\n",
    "    for k in (\"rawContent\", \"full_text\", \"text\"):\n",
    "        val = rec.get(k)\n",
    "        if isinstance(val, str) and val.strip():\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "def get_user_obj(rec: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    u = rec.get(\"user\")\n",
    "    return u if isinstance(u, dict) else None\n",
    "\n",
    "def safe_int(x):\n",
    "    try: return int(x)\n",
    "    except Exception: return None\n",
    "\n",
    "MENTION_RE = re.compile(r\"@([A-Za-z0-9_]{1,15})\")\n",
    "HASHTAG_RE = re.compile(r\"#([A-Za-z0-9_]+)\")\n",
    "\n",
    "def rows_from_json(records: List[Dict[str, Any]], source_file: str) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for r in records:\n",
    "        u = get_user_obj(r)\n",
    "        uname = u.get(\"username\") or u.get(\"screen_name\") or u.get(\"name\") if u else None\n",
    "        uid = u.get(\"id\") or u.get(\"id_str\") if u else None\n",
    "\n",
    "        # Mentions de diferentes formatos\n",
    "        mentions = []\n",
    "        if \"mentionedUsers\" in r:\n",
    "            mentions = extract_list_usernames(r.get(\"mentionedUsers\"))\n",
    "        elif \"entities\" in r and isinstance(r[\"entities\"], dict):\n",
    "            mentions = extract_list_usernames(r[\"entities\"].get(\"user_mentions\"))\n",
    "\n",
    "        # Hashtags\n",
    "        if \"hashtags\" in r:\n",
    "            tags = hashtags_to_list(r.get(\"hashtags\"))\n",
    "        elif \"entities\" in r and isinstance(r[\"entities\"], dict):\n",
    "            tags = hashtags_to_list(r[\"entities\"].get(\"hashtags\"))\n",
    "        else:\n",
    "            tags = []\n",
    "\n",
    "        # RT / Quote\n",
    "        rt = r.get(\"retweetedTweet\") or r.get(\"retweeted_status\")\n",
    "        qt = r.get(\"quotedTweet\") or r.get(\"quoted_status\")\n",
    "        is_rt = rt is not None\n",
    "        is_qt = qt is not None\n",
    "        rt_user = norm_username((rt or {}).get(\"user\", {}).get(\"username\") if isinstance(rt, dict) else None)\n",
    "        qt_user = norm_username((qt or {}).get(\"user\", {}).get(\"username\") if isinstance(qt, dict) else None)\n",
    "\n",
    "        # Reply\n",
    "        in_reply_to_user = r.get(\"inReplyToUser\")\n",
    "        reply_to_username = None\n",
    "        if isinstance(in_reply_to_user, dict):\n",
    "            reply_to_username = norm_username(in_reply_to_user.get(\"username\"))\n",
    "        if not reply_to_username and r.get(\"in_reply_to_screen_name\"):\n",
    "            reply_to_username = norm_username(r.get(\"in_reply_to_screen_name\"))\n",
    "\n",
    "        # MÃ©tricas\n",
    "        like_count  = safe_int(r.get(\"likeCount\")   or r.get(\"favorite_count\"))\n",
    "        rt_count    = safe_int(r.get(\"retweetCount\")or r.get(\"retweet_count\"))\n",
    "        reply_count = safe_int(r.get(\"replyCount\")  or r.get(\"reply_count\"))\n",
    "        quote_count = safe_int(r.get(\"quoteCount\")  or r.get(\"quote_count\"))\n",
    "        view_count  = safe_int(r.get(\"viewCount\")   or r.get(\"views\"))\n",
    "\n",
    "        # Fecha\n",
    "        date_raw = r.get(\"date\") or r.get(\"created_at\")\n",
    "        try:    date_parsed = pd.to_datetime(date_raw)\n",
    "        except: date_parsed = pd.NaT\n",
    "\n",
    "        rows.append({\n",
    "            \"source_file\": source_file,\n",
    "            \"tweet_id\": r.get(\"id\") or r.get(\"id_str\"),\n",
    "            \"date\": date_parsed,\n",
    "            \"lang\": r.get(\"lang\"),\n",
    "            \"username\": norm_username(uname),\n",
    "            \"user_id\": uid,\n",
    "            \"text\": get_text(r),\n",
    "            \"mentions\": mentions,\n",
    "            \"hashtags\": tags,\n",
    "            \"is_retweet\": bool(is_rt),\n",
    "            \"is_quote\": bool(is_qt),\n",
    "            \"retweeted_user\": rt_user,\n",
    "            \"quoted_user\": qt_user,\n",
    "            \"reply_to_user\": reply_to_username,\n",
    "            \"in_reply_to_tweet_id\": r.get(\"inReplyToTweetId\") or r.get(\"in_reply_to_status_id_str\") or r.get(\"in_reply_to_status_id\"),\n",
    "            \"like_count\": like_count,\n",
    "            \"retweet_count\": rt_count,\n",
    "            \"reply_count\": reply_count,\n",
    "            \"quote_count\": quote_count,\n",
    "            \"view_count\": view_count,\n",
    "            \"raw_record\": r,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def rows_from_plaintext(lines: List[str], source_file: str) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    for i, raw in enumerate(lines, start=1):\n",
    "        txt = raw.strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        # RT al inicio\n",
    "        is_rt = False; rt_user = None\n",
    "        m_rt = re.match(r\"^\\s*RT\\s+@([A-Za-z0-9_]{1,15})\\b\", txt)\n",
    "        if m_rt:\n",
    "            is_rt = True; rt_user = m_rt.group(1).lower()\n",
    "        # menciones / hashtags\n",
    "        mentions = [m.lower() for m in MENTION_RE.findall(txt)]\n",
    "        hashtags = [h.lower() for h in HASHTAG_RE.findall(txt)]\n",
    "        # reply si inicia con @\n",
    "        reply_to_user = None\n",
    "        m_reply = re.match(r\"^\\s*@([A-Za-z0-9_]{1,15})\\b\", txt)\n",
    "        if m_reply:\n",
    "            reply_to_user = m_reply.group(1).lower()\n",
    "\n",
    "        rows.append({\n",
    "            \"source_file\": source_file,\n",
    "            \"tweet_id\": f\"{source_file}:{i}\",\n",
    "            \"date\": pd.NaT,\n",
    "            \"lang\": None,\n",
    "            \"username\": None,\n",
    "            \"user_id\": None,\n",
    "            \"text\": txt,\n",
    "            \"mentions\": mentions,\n",
    "            \"hashtags\": hashtags,\n",
    "            \"is_retweet\": is_rt,\n",
    "            \"is_quote\": False,\n",
    "            \"retweeted_user\": rt_user,\n",
    "            \"quoted_user\": None,\n",
    "            \"reply_to_user\": reply_to_user,\n",
    "            \"in_reply_to_tweet_id\": None,\n",
    "            \"like_count\": None,\n",
    "            \"retweet_count\": None,\n",
    "            \"reply_count\": None,\n",
    "            \"quote_count\": None,\n",
    "            \"view_count\": None,\n",
    "            \"raw_record\": {\"_raw_line\": txt},\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# -------------------- Proceso principal de carga --------------------\n",
    "all_rows: List[Dict[str, Any]] = []\n",
    "summary = []\n",
    "\n",
    "for p in DATA_PATHS:\n",
    "    status = \"NO_FILE\"\n",
    "    n_lines = 0; n_json_whole = 0; n_jsonl = 0; n_json_inline = 0; used_plain = 0\n",
    "    encoding_used = \"-\"\n",
    "    if p.exists():\n",
    "        status = \"OK\"\n",
    "        raw, encoding_used = read_text_auto(p)\n",
    "        lines = raw.splitlines()\n",
    "        n_lines = len(lines)\n",
    "\n",
    "        # 1) JSON global (array/obj)\n",
    "        recs = read_any_json_whole(raw.strip())\n",
    "        if isinstance(recs, list) and recs:\n",
    "            all_rows.extend(rows_from_json(recs, p.name))\n",
    "            n_json_whole = len(recs)\n",
    "        else:\n",
    "            # 2) JSONL\n",
    "            recs_jsonl, ok = read_jsonl_lines(raw)\n",
    "            if ok > 0:\n",
    "                all_rows.extend(rows_from_json(recs_jsonl, p.name))\n",
    "                n_jsonl = ok\n",
    "            else:\n",
    "                # 3) Por lÃ­nea: JSON embebido o texto plano\n",
    "                temp_rows = []\n",
    "                for line in lines:\n",
    "                    obj = extract_json_from_line(line)\n",
    "                    if isinstance(obj, dict):\n",
    "                        temp_rows.extend(rows_from_json([obj], p.name))\n",
    "                        n_json_inline += 1\n",
    "                    else:\n",
    "                        temp_rows.extend(rows_from_plaintext([line], p.name))\n",
    "                        used_plain += 1\n",
    "                all_rows.extend(temp_rows)\n",
    "\n",
    "    summary.append({\n",
    "        \"file\": p.name, \"status\": status, \"encoding\": encoding_used,\n",
    "        \"n_lines\": n_lines, \"JSON_global\": n_json_whole, \"JSONL\": n_jsonl,\n",
    "        \"JSON_inline\": n_json_inline, \"texto_plano\": used_plain\n",
    "    })\n",
    "\n",
    "# -------------------- DataFrame & salidas --------------------\n",
    "df = pd.DataFrame(all_rows)\n",
    "if not df.empty and \"date\" in df.columns:\n",
    "    df = df.sort_values(\"date\", na_position=\"last\").reset_index(drop=True)\n",
    "\n",
    "if not df.empty:\n",
    "    df.head(50).to_csv(OUT_SAMPLE_CSV, index=False)\n",
    "    try:\n",
    "        df.to_parquet(OUT_PARQUET, index=False)\n",
    "        parquet_path = str(OUT_PARQUET.resolve())\n",
    "    except Exception:\n",
    "        parquet_path = \"(No Parquet: instala 'pyarrow' o 'fastparquet')\"\n",
    "else:\n",
    "    parquet_path = \"(DataFrame vacÃ­o)\"\n",
    "\n",
    "print(\"=== RESUMEN DE CARGA (Paso 1â€“2) ===\")\n",
    "print(\"Archivos buscados:\")\n",
    "for p in DATA_PATHS: print(\" -\", p)\n",
    "\n",
    "for s in summary:\n",
    "    print(f\"- {s['file']:15s} | estado={s['status']:7s} | codif={s['encoding']:10s} | \"\n",
    "          f\"lÃ­neas={s['n_lines']:5d} | JSON_global={s['JSON_global']:5d} | \"\n",
    "          f\"JSONL={s['JSONL']:5d} | JSON_inline={s['JSON_inline']:5d} | \"\n",
    "          f\"texto_plano={s['texto_plano']:5d}\")\n",
    "\n",
    "print(f\"\\nTotal de filas normalizadas: {len(df):,}\")\n",
    "if not df.empty:\n",
    "    print(\"Columnas:\", list(df.columns))\n",
    "    print(f\"Muestra CSV (50 filas): {str(OUT_SAMPLE_CSV.resolve())}\")\n",
    "    print(f\"Parquet: {parquet_path}\")\n",
    "else:\n",
    "    print(\"Nota: DataFrame vacÃ­o. Si sigue vacÃ­o, compÃ¡rteme 10â€“15 lÃ­neas crudas de un archivo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6a8d6",
   "metadata": {},
   "source": [
    "## **Inciso 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c62461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inciso 3 (3.1, 3.2, 3.3): Limpieza, extracciÃ³n y normalizaciÃ³n\n",
    "# - Convierte a minÃºsculas, quita URLs, emojis, puntuaciÃ³n, stopwords y nÃºmeros (opcional)\n",
    "# - Extrae/normaliza menciones, respuestas y retweets (rellena desde texto si falta)\n",
    "# - Elimina duplicados de tweets y normaliza nombres de usuario/menciones\n",
    "# - Deja columnas listas para el siguiente inciso (3.4)\n",
    "#\n",
    "# ðŸ‘‰ Al terminar imprime \"RESUMEN PREPROCESAMIENTO (3.1â€“3.3)\".\n",
    "#    PÃ©game ese bloque aquÃ­ para analizar y seguimos con 3.4.\n",
    "\n",
    "import re, json, ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# 0) Recupera df si no existe (vuelve a parsear JSONL UTF-16)\n",
    "# =========================\n",
    "def read_text_auto(path: Path):\n",
    "    data = path.read_bytes()\n",
    "    zero_ratio = data.count(0) / max(1, len(data))\n",
    "    cands = ([\"utf-16\",\"utf-16-le\",\"utf-16-be\",\"utf-8\",\"latin-1\"]\n",
    "             if zero_ratio > 0.01 else [\"utf-8\",\"utf-16\",\"utf-16-le\",\"utf-16-be\",\"latin-1\"])\n",
    "    for enc in cands:\n",
    "        try:\n",
    "            txt = data.decode(enc)\n",
    "            if txt.count(\"\\x00\") > 10:\n",
    "                continue\n",
    "            return txt\n",
    "        except Exception:\n",
    "            continue\n",
    "    return data.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def json_maybe_twice(s: str):\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, str):\n",
    "            st = obj.strip()\n",
    "            if (st.startswith(\"{\") and st.endswith(\"}\")) or (st.startswith(\"[\") and st.endswith(\"]\")):\n",
    "                try: return json.loads(st)\n",
    "                except Exception: return obj\n",
    "        return obj\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_jsonl_utf16(paths):\n",
    "    rows = []\n",
    "    for p in paths:\n",
    "        if not Path(p).exists():\n",
    "            continue\n",
    "        raw = read_text_auto(Path(p))\n",
    "        for line in raw.splitlines():\n",
    "            s = line.strip().rstrip(\",\")\n",
    "            if not s: \n",
    "                continue\n",
    "            obj = json_maybe_twice(s)\n",
    "            if not isinstance(obj, dict):\n",
    "                continue\n",
    "            r = obj\n",
    "            # username\n",
    "            u = r.get(\"user\") if isinstance(r.get(\"user\"), dict) else {}\n",
    "            uname = (u.get(\"username\") or u.get(\"screen_name\") or u.get(\"name\"))\n",
    "            # mentions\n",
    "            mentions = []\n",
    "            if \"mentionedUsers\" in r and isinstance(r[\"mentionedUsers\"], list):\n",
    "                for m in r[\"mentionedUsers\"]:\n",
    "                    if isinstance(m, dict):\n",
    "                        un = m.get(\"username\") or m.get(\"screen_name\") or m.get(\"name\")\n",
    "                        if un: mentions.append(un.lower().lstrip(\"@\"))\n",
    "            elif \"entities\" in r and isinstance(r[\"entities\"], dict):\n",
    "                for m in (r[\"entities\"].get(\"user_mentions\") or []):\n",
    "                    if isinstance(m, dict):\n",
    "                        un = m.get(\"screen_name\") or m.get(\"username\") or m.get(\"name\")\n",
    "                        if un: mentions.append(un.lower().lstrip(\"@\"))\n",
    "            # hashtags\n",
    "            tags = []\n",
    "            if isinstance(r.get(\"hashtags\"), list):\n",
    "                for h in r[\"hashtags\"]:\n",
    "                    if isinstance(h, str): tags.append(h.lstrip(\"#\").lower())\n",
    "                    elif isinstance(h, dict):\n",
    "                        t = h.get(\"text\") or h.get(\"tag\")\n",
    "                        if t: tags.append(str(t).lstrip(\"#\").lower())\n",
    "            elif \"entities\" in r and isinstance(r[\"entities\"], dict):\n",
    "                for h in (r[\"entities\"].get(\"hashtags\") or []):\n",
    "                    if isinstance(h, str): tags.append(h.lstrip(\"#\").lower())\n",
    "                    elif isinstance(h, dict):\n",
    "                        t = h.get(\"text\") or h.get(\"tag\")\n",
    "                        if t: tags.append(str(t).lstrip(\"#\").lower())\n",
    "            # retweet / quote\n",
    "            rt = r.get(\"retweetedTweet\") or r.get(\"retweeted_status\")\n",
    "            qt = r.get(\"quotedTweet\") or r.get(\"quoted_status\")\n",
    "            rt_user = (rt or {}).get(\"user\", {})\n",
    "            qt_user = (qt or {}).get(\"user\", {})\n",
    "            # reply\n",
    "            reply_to = None\n",
    "            if isinstance(r.get(\"inReplyToUser\"), dict):\n",
    "                reply_to = r[\"inReplyToUser\"].get(\"username\")\n",
    "            if not reply_to and isinstance(r.get(\"inReplyToUser\"), dict):\n",
    "                reply_to = r[\"inReplyToUser\"].get(\"screen_name\")\n",
    "            if not reply_to and r.get(\"in_reply_to_screen_name\"):\n",
    "                reply_to = r.get(\"in_reply_to_screen_name\")\n",
    "\n",
    "            def norm_user(x):\n",
    "                if not x: return None\n",
    "                x = str(x).strip()\n",
    "                if x.startswith(\"@\"): x = x[1:]\n",
    "                return x.lower()\n",
    "\n",
    "            rows.append({\n",
    "                \"source_file\": Path(p).name,\n",
    "                \"tweet_id\": r.get(\"id\") or r.get(\"id_str\"),\n",
    "                \"date\": pd.to_datetime(r.get(\"date\") or r.get(\"created_at\"), errors=\"coerce\"),\n",
    "                \"lang\": r.get(\"lang\"),\n",
    "                \"username\": norm_user(uname),\n",
    "                \"user_id\": (u.get(\"id\") or u.get(\"id_str\")) if isinstance(u, dict) else None,\n",
    "                \"text\": (r.get(\"rawContent\") or r.get(\"full_text\") or r.get(\"text\")),\n",
    "                \"mentions\": [m for m in mentions if m],\n",
    "                \"hashtags\": [t for t in tags if t],\n",
    "                \"is_retweet\": rt is not None,\n",
    "                \"is_quote\": qt is not None,\n",
    "                \"retweeted_user\": norm_user(rt_user.get(\"username\") if isinstance(rt_user, dict) else None),\n",
    "                \"quoted_user\": norm_user(qt_user.get(\"username\") if isinstance(qt_user, dict) else None),\n",
    "                \"reply_to_user\": norm_user(reply_to),\n",
    "                \"in_reply_to_tweet_id\": r.get(\"inReplyToTweetId\") or r.get(\"in_reply_to_status_id\") or r.get(\"in_reply_to_status_id_str\"),\n",
    "                \"like_count\": r.get(\"likeCount\") or r.get(\"favorite_count\"),\n",
    "                \"retweet_count\": r.get(\"retweetCount\") or r.get(\"retweet_count\"),\n",
    "                \"reply_count\": r.get(\"replyCount\") or r.get(\"reply_count\"),\n",
    "                \"quote_count\": r.get(\"quoteCount\") or r.get(\"quote_count\"),\n",
    "                \"view_count\": r.get(\"viewCount\") or r.get(\"views\"),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "try:\n",
    "    df\n",
    "    assert isinstance(df, pd.DataFrame) and not df.empty\n",
    "except Exception:\n",
    "    df = load_jsonl_utf16([\"traficogt.txt\",\"/mnt/data/traficogt.txt\",\n",
    "                           \"tioberny.txt\",\"/mnt/data/tioberny.txt\"])\n",
    "\n",
    "# =========================\n",
    "# 1) Funciones de limpieza\n",
    "# =========================\n",
    "# Fix de mojibake (p. ej., \"TÃƒÂ©cnica\" -> \"TÃ©cnica\")\n",
    "def fix_mojibake(s):\n",
    "    if not isinstance(s, str): \n",
    "        return s\n",
    "    if \"Ãƒ\" in s or \"Ã‚\" in s or \"Ã°Å¸\" in s:\n",
    "        try:\n",
    "            return s.encode(\"latin1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "EMOJI_RE = re.compile(\n",
    "    \"[\"                         # bloques comunes de emoji\n",
    "    \"\\U0001F600-\\U0001F64F\"     # emoticonos\n",
    "    \"\\U0001F300-\\U0001F5FF\"     # sÃ­mbolos & pictogramas\n",
    "    \"\\U0001F680-\\U0001F6FF\"     # transporte\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"     # banderas\n",
    "    \"\\U00002700-\\U000027BF\"     # otros\n",
    "    \"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE\n",
    ")\n",
    "# Mantener letras (incl. acentos) y espacios; quitar dÃ­gitos y puntuaciÃ³n\n",
    "KEEP_LETTERS_RE = re.compile(r\"[^a-zÃ¡Ã©Ã­Ã³ÃºÃ¼Ã±\\s]\")\n",
    "\n",
    "STOP_ES = set(\"\"\"\n",
    "a al algo algunas algunos ante antes apenas aquella aquellas aquello aquellos aqui\n",
    "asÃ­ aun aunque cada casi como con contigo contra cual cuales cualquier cuando\n",
    "de del desde donde dos el ella ellas ellos en entre era erais eran eras eres es esa\n",
    "esas ese eso esos esta estaba estabais estaban estabas estamos estan estarÃ© estarÃ¡s\n",
    "este esto estos estuvo fui fue fueron hemos han hasta hay hice hizo hoy la las le les\n",
    "lo los me mi mis mucha muchas mucho muchos muy nada ni no nosotros nosotras nuestra\n",
    "nuestro nuestras nuestros nunca o os para pero poco por porque que quien quienes se\n",
    "sea sean segÃºn ser si siempre sin sino sobre sois son soy su sus tambiÃ©n te tengo tiene\n",
    "tienen toda todas todo todos tras tu tus un una uno unos usted ustedes ya y yo\n",
    "\"\"\".split())\n",
    "\n",
    "def clean_text(s: str):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return \"\"\n",
    "    s = fix_mojibake(s)\n",
    "    s = s.lower()\n",
    "    s = URL_RE.sub(\" \", s)\n",
    "    s = EMOJI_RE.sub(\" \", s)\n",
    "    # Quitar @ y # del texto (las menciones/hashtags se quedan en columnas aparte)\n",
    "    s = s.replace(\"@\", \" \").replace(\"#\", \" \")\n",
    "    # Quitar apÃ³strofes simples/raros\n",
    "    s = s.replace(\"â€™\", \"\").replace(\"â€˜\", \"\").replace(\"Â´\",\"\").replace(\"`\",\"\").replace(\"'\", \"\")\n",
    "    # Quitar puntuaciÃ³n y nÃºmeros (si deseas conservar nÃºmeros, comenta la siguiente lÃ­nea y usa otra regex)\n",
    "    s = KEEP_LETTERS_RE.sub(\" \", s)\n",
    "    # Normalizar espacios\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def ensure_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            val = ast.literal_eval(x)\n",
    "            if isinstance(val, list): \n",
    "                return val\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def norm_user(u):\n",
    "    if not u: return None\n",
    "    u = str(u).strip()\n",
    "    if u.startswith(\"@\"): u = u[1:]\n",
    "    return u.lower()\n",
    "\n",
    "# =========================\n",
    "# 2) Aplicar preprocesamiento\n",
    "# =========================\n",
    "df_proc = df.copy()\n",
    "\n",
    "# Arregla mojibake en campos de texto/usuario que lo requieran\n",
    "for col in [\"text\",\"username\",\"retweeted_user\",\"quoted_user\",\"reply_to_user\"]:\n",
    "    if col in df_proc.columns:\n",
    "        df_proc[col] = df_proc[col].apply(lambda x: fix_mojibake(x) if isinstance(x,str) else x)\n",
    "\n",
    "# Asegura listas en menciones/hashtags\n",
    "if \"mentions\" in df_proc.columns:\n",
    "    df_proc[\"mentions\"] = df_proc[\"mentions\"].apply(ensure_list).apply(lambda xs: [norm_user(x) for x in xs if x])\n",
    "else:\n",
    "    df_proc[\"mentions\"] = [[] for _ in range(len(df_proc))]\n",
    "\n",
    "if \"hashtags\" in df_proc.columns:\n",
    "    df_proc[\"hashtags\"] = df_proc[\"hashtags\"].apply(ensure_list).apply(\n",
    "        lambda xs: [fix_mojibake(x).lstrip(\"#\").lower() for x in xs if isinstance(x,str) and x]\n",
    "    )\n",
    "else:\n",
    "    df_proc[\"hashtags\"] = [[] for _ in range(len(df_proc))]\n",
    "\n",
    "# 3.2 Completa menciones, respuestas y RT desde el texto si faltan\n",
    "MENTION_RE = re.compile(r\"@([A-Za-z0-9_]{1,15})\")\n",
    "def backfill_from_text(row):\n",
    "    txt = row.get(\"text\") or \"\"\n",
    "    # menciones desde texto\n",
    "    if isinstance(txt, str) and txt:\n",
    "        found = [m.lower() for m in MENTION_RE.findall(txt)]\n",
    "    else:\n",
    "        found = []\n",
    "    # union de las ya existentes con las del texto\n",
    "    cur = set([m for m in row[\"mentions\"] if m])\n",
    "    cur.update(found)\n",
    "    row[\"mentions\"] = sorted(cur)\n",
    "\n",
    "    # reply si inicia con @user\n",
    "    m = re.match(r\"^\\s*@([A-Za-z0-9_]{1,15})\\b\", txt or \"\")\n",
    "    if not row.get(\"reply_to_user\") and m:\n",
    "        row[\"reply_to_user\"] = norm_user(m.group(1))\n",
    "\n",
    "    # RT si inicia con \"RT @user:\"\n",
    "    m2 = re.match(r\"^\\s*rt\\s+@([A-Za-z0-9_]{1,15})\\b\", (txt or \"\").lower())\n",
    "    if (not row.get(\"is_retweet\")) and m2:\n",
    "        row[\"is_retweet\"] = True\n",
    "        row[\"retweeted_user\"] = norm_user(m2.group(1))\n",
    "    return row\n",
    "\n",
    "df_proc = df_proc.apply(backfill_from_text, axis=1)\n",
    "\n",
    "# 3.1 Limpieza de texto\n",
    "df_proc[\"text_clean\"] = df_proc[\"text\"].apply(clean_text)\n",
    "\n",
    "# Tokens (separaciÃ³n simple por espacio)\n",
    "df_proc[\"tokens\"] = df_proc[\"text_clean\"].apply(lambda s: [t for t in s.split() if t])\n",
    "\n",
    "# Stopwords\n",
    "df_proc[\"tokens_nostop\"] = df_proc[\"tokens\"].apply(lambda ts: [t for t in ts if t not in STOP_ES])\n",
    "\n",
    "# 3.3 NormalizaciÃ³n de usuarios y eliminaciÃ³n de duplicados\n",
    "for col in [\"username\",\"reply_to_user\",\"retweeted_user\",\"quoted_user\"]:\n",
    "    if col in df_proc.columns:\n",
    "        df_proc[col] = df_proc[col].apply(norm_user)\n",
    "\n",
    "n_raw = len(df_proc)\n",
    "# elimina duplicados por tweet_id si existe, si no por (source_file,text)\n",
    "if \"tweet_id\" in df_proc.columns:\n",
    "    df_proc = df_proc.drop_duplicates(subset=[\"tweet_id\"])\n",
    "else:\n",
    "    df_proc = df_proc.drop_duplicates(subset=[\"source_file\",\"text\"])\n",
    "n_after_dedup = len(df_proc)\n",
    "dup_removed = n_raw - n_after_dedup\n",
    "\n",
    "# =========================\n",
    "# 3) Resumen para reporte\n",
    "# =========================\n",
    "n_with_mentions = int((df_proc[\"mentions\"].apply(len) > 0).sum())\n",
    "n_with_reply = int(df_proc[\"reply_to_user\"].notna().sum())\n",
    "n_is_rt = int(df_proc[\"is_retweet\"].fillna(False).sum())\n",
    "\n",
    "# hashtags mÃ¡s frecuentes (top 10)\n",
    "from collections import Counter\n",
    "ht_counts = Counter(h for xs in df_proc[\"hashtags\"] for h in (xs or []))\n",
    "top_hashtags = ht_counts.most_common(10)\n",
    "\n",
    "print(\"=== RESUMEN PREPROCESAMIENTO (3.1â€“3.3) ===\")\n",
    "print(f\"Filas originales: {n_raw:,}\")\n",
    "print(f\"Duplicados removidos: {dup_removed:,}\")\n",
    "print(f\"Filas finales: {n_after_dedup:,}\")\n",
    "print(f\"Tweets con menciones: {n_with_mentions:,}\")\n",
    "print(f\"Tweets que son replies: {n_with_reply:,}\")\n",
    "print(f\"Tweets que son retweets: {n_is_rt:,}\")\n",
    "print(\"\\nTop 10 hashtags (despuÃ©s de limpieza):\")\n",
    "for tag, cnt in top_hashtags:\n",
    "    print(f\"  #{tag}: {cnt}\")\n",
    "\n",
    "# Vista rÃ¡pida de ejemplos (5)\n",
    "print(\"\\nEjemplos de 'text'  â†’  'text_clean' (5 filas):\")\n",
    "for i, row in df_proc.head(5).iterrows():\n",
    "    print(f\"- {row.get('text')!r}  ->  {row.get('text_clean')!r}\")\n",
    "\n",
    "# Guarda muestra para inspecciÃ³n manual (opcional)\n",
    "df_proc.head(200).to_csv(\"tweets_clean_sample.csv\", index=False)\n",
    "print(\"\\nArchivo de muestra (200 filas) guardado en: tweets_clean_sample.csv\")\n",
    "\n",
    "# Mantener df_proc en memoria para el siguiente inciso (3.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb3e213",
   "metadata": {},
   "source": [
    "## **Inciso 4A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4448f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda A â€” Inciso 4.1: AnÃ¡lisis Exploratorio (EDA) ===\n",
    "# - Usa df_proc de la celda anterior; si no existe, intenta reconstruirlo rÃ¡pido desde los txt.\n",
    "# - Calcula: cuentas bÃ¡sicas, top hashtags, top tokens, top usuarios, timeline, interacciones.\n",
    "# - Genera grÃ¡ficos (matplotlib) y guarda CSV/PNGs para documentaciÃ³n.\n",
    "#\n",
    "# Al final imprime: \"RESUMEN EDA (4.1)\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# ---------- Helpers de carga mÃ­nima (por si abres el notebook desde cero) ----------\n",
    "def _ensure_df_proc():\n",
    "    global df_proc\n",
    "    if \"df_proc\" in globals() and isinstance(df_proc, pd.DataFrame) and not df_proc.empty:\n",
    "        return df_proc\n",
    "\n",
    "    # Fallback 1: si dejaste la muestra limpia\n",
    "    if Path(\"tweets_clean_sample.csv\").exists():\n",
    "        tmp = pd.read_csv(\"tweets_clean_sample.csv\")\n",
    "        # columnas listas\n",
    "        for col in [\"mentions\",\"hashtags\",\"tokens\",\"tokens_nostop\"]:\n",
    "            if col in tmp.columns:\n",
    "                tmp[col] = tmp[col].apply(lambda x: [] if pd.isna(x) else eval(x) if isinstance(x,str) else [])\n",
    "        # normaliza fechas\n",
    "        if \"date\" in tmp.columns:\n",
    "            tmp[\"date\"] = pd.to_datetime(tmp[\"date\"], errors=\"coerce\")\n",
    "        dfp = tmp\n",
    "        return dfp\n",
    "\n",
    "    # Fallback 2 (reconstrucciÃ³n mÃ­nima): vuelve a leer JSONL UTF-16 y limpia bÃ¡sico\n",
    "    import json\n",
    "    def _read_bytes_auto(path: Path) -> str:\n",
    "        data = path.read_bytes()\n",
    "        if data.count(0)/max(1,len(data)) > 0.01:\n",
    "            for enc in [\"utf-16\",\"utf-16-le\",\"utf-16-be\",\"utf-8\",\"latin-1\"]:\n",
    "                try:\n",
    "                    s = data.decode(enc)\n",
    "                    if s.count(\"\\x00\") < 5: return s\n",
    "                except: pass\n",
    "            return data.decode(\"utf-16\", errors=\"ignore\")\n",
    "        return data.decode(\"utf-8\", errors=\"ignore\")\n",
    "    def _json_maybe_twice(s):\n",
    "        try:\n",
    "            o = json.loads(s)\n",
    "            if isinstance(o,str) and o.strip() and o.strip()[0] in \"{[\":\n",
    "                try: return json.loads(o)\n",
    "                except: return o\n",
    "            return o\n",
    "        except: return None\n",
    "\n",
    "    rows=[]\n",
    "    for p in [\"traficogt.txt\",\"/mnt/data/traficogt.txt\",\"tioberny.txt\",\"/mnt/data/tioberny.txt\"]:\n",
    "        p=Path(p)\n",
    "        if not p.exists(): continue\n",
    "        raw=_read_bytes_auto(p)\n",
    "        for line in raw.splitlines():\n",
    "            s=line.strip().rstrip(\",\")\n",
    "            if not s: continue\n",
    "            obj=_json_maybe_twice(s)\n",
    "            if not isinstance(obj,dict): continue\n",
    "            r=obj; u=r.get(\"user\") if isinstance(r.get(\"user\"),dict) else {}\n",
    "            def _norm(u):\n",
    "                if not u: return None\n",
    "                u=str(u).strip()\n",
    "                if u.startswith(\"@\"): u=u[1:]\n",
    "                return u.lower()\n",
    "            # mentions\n",
    "            m=[]\n",
    "            if \"mentionedUsers\" in r and isinstance(r[\"mentionedUsers\"],list):\n",
    "                for it in r[\"mentionedUsers\"]:\n",
    "                    if isinstance(it,dict):\n",
    "                        un=it.get(\"username\") or it.get(\"screen_name\") or it.get(\"name\")\n",
    "                        if un: m.append(_norm(un))\n",
    "            # hashtags\n",
    "            hs=[]\n",
    "            if isinstance(r.get(\"hashtags\"),list):\n",
    "                for h in r[\"hashtags\"]:\n",
    "                    if isinstance(h,str): hs.append(h.lstrip(\"#\").lower())\n",
    "                    elif isinstance(h,dict):\n",
    "                        t=h.get(\"text\") or h.get(\"tag\")\n",
    "                        if t: hs.append(str(t).lstrip(\"#\").lower())\n",
    "            rows.append({\n",
    "                \"source_file\": p.name,\n",
    "                \"tweet_id\": r.get(\"id\") or r.get(\"id_str\"),\n",
    "                \"date\": pd.to_datetime(r.get(\"date\") or r.get(\"created_at\"), errors=\"coerce\"),\n",
    "                \"lang\": r.get(\"lang\"),\n",
    "                \"username\": _norm((u.get(\"username\") or u.get(\"screen_name\") or u.get(\"name\")) if u else None),\n",
    "                \"text\": (r.get(\"rawContent\") or r.get(\"full_text\") or r.get(\"text\")),\n",
    "                \"mentions\": m,\n",
    "                \"hashtags\": hs,\n",
    "                \"is_retweet\": bool(r.get(\"retweetedTweet\") or r.get(\"retweeted_status\")),\n",
    "                \"reply_to_user\": _norm((r.get(\"inReplyToUser\") or {}).get(\"username\") if isinstance(r.get(\"inReplyToUser\"),dict) else r.get(\"in_reply_to_screen_name\")),\n",
    "            })\n",
    "    dfp=pd.DataFrame(rows)\n",
    "    if dfp.empty:\n",
    "        raise RuntimeError(\"No pude reconstruir df_proc. Vuelve a ejecutar la celda 3 (preprocesamiento).\")\n",
    "    # limpieza mÃ­nima de texto para tokens\n",
    "    def _fix(s):\n",
    "        if isinstance(s,str) and (\"Ãƒ\" in s or \"Ã‚\" in s): \n",
    "            try: return s.encode(\"latin1\",\"ignore\").decode(\"utf-8\",\"ignore\")\n",
    "            except: return s\n",
    "        return s\n",
    "    dfp[\"text\"]=dfp[\"text\"].apply(_fix)\n",
    "    import re\n",
    "    URL_RE=re.compile(r\"https?://\\S+|www\\.\\S+\", re.I)\n",
    "    KEEP=re.compile(r\"[^a-zÃ¡Ã©Ã­Ã³ÃºÃ¼Ã±\\s]\")\n",
    "    def _clean(s):\n",
    "        if not isinstance(s,str): return \"\"\n",
    "        s=s.lower()\n",
    "        s=URL_RE.sub(\" \", s)\n",
    "        s=s.replace(\"@\",\" \").replace(\"#\",\" \")\n",
    "        s=KEEP.sub(\" \", s)\n",
    "        s=re.sub(r\"\\s+\",\" \",s).strip()\n",
    "        return s\n",
    "    dfp[\"text_clean\"]=dfp[\"text\"].apply(_clean)\n",
    "    STOP=set(\"a al las los de del en la el y o que con por para un una uno unos unas se es no si como pero sobre ya muy sin mÃ¡s menos mi tu su sus lo me te le les es son ser fue fueron soy somos estoy estÃ¡n estaba estÃ¡n hasta hay\".split())\n",
    "    dfp[\"tokens\"]=dfp[\"text_clean\"].str.split()\n",
    "    dfp[\"tokens_nostop\"]=dfp[\"tokens\"].apply(lambda ts: [t for t in ts if t not in STOP])\n",
    "    return dfp\n",
    "\n",
    "df_eda = _ensure_df_proc().copy()\n",
    "\n",
    "# ---------- MÃ©tricas bÃ¡sicas ----------\n",
    "n_tweets = len(df_eda)\n",
    "n_users = int(df_eda[\"username\"].dropna().nunique()) if \"username\" in df_eda else 0\n",
    "n_mentions_total = int(sum(len(x or []) for x in df_eda[\"mentions\"]))\n",
    "unique_mentioned = set()\n",
    "for xs in df_eda[\"mentions\"]:\n",
    "    unique_mentioned.update(xs or [])\n",
    "n_users_mentioned = len(unique_mentioned)\n",
    "n_replies = int(df_eda[\"reply_to_user\"].notna().sum()) if \"reply_to_user\" in df_eda else 0\n",
    "n_retweets = int(df_eda[\"is_retweet\"].fillna(False).sum()) if \"is_retweet\" in df_eda else 0\n",
    "\n",
    "# ---------- Top hashtags ----------\n",
    "ht_counter = Counter(h for xs in df_eda[\"hashtags\"] for h in (xs or []))\n",
    "df_top_ht = pd.DataFrame(ht_counter.most_common(30), columns=[\"hashtag\",\"count\"])\n",
    "\n",
    "# ---------- Top tokens (sin stopwords) ----------\n",
    "tok_counter = Counter(t for xs in (df_eda[\"tokens_nostop\"] if \"tokens_nostop\" in df_eda else df_eda[\"tokens\"]) for t in (xs or []))\n",
    "df_top_tokens = pd.DataFrame(tok_counter.most_common(30), columns=[\"token\",\"count\"])\n",
    "\n",
    "# ---------- Usuarios mÃ¡s activos / mÃ¡s mencionados ----------\n",
    "df_active_users = (df_eda.dropna(subset=[\"username\"])\n",
    "                   .groupby(\"username\", as_index=False)\n",
    "                   .agg(tweets=(\"tweet_id\",\"count\")))\n",
    "# mÃ¡s mencionados\n",
    "mentioned_counter = Counter()\n",
    "for _, row in df_eda.iterrows():\n",
    "    src = row.get(\"username\")\n",
    "    for dst in row.get(\"mentions\") or []:\n",
    "        mentioned_counter[dst] += 1\n",
    "df_most_mentioned = pd.DataFrame(mentioned_counter.most_common(30), columns=[\"user\",\"mentions_received\"])\n",
    "\n",
    "# ---------- Timeline (por dÃ­a) ----------\n",
    "if \"date\" in df_eda.columns:\n",
    "    df_eda[\"date_only\"] = df_eda[\"date\"].dt.date\n",
    "    ts_daily = (df_eda.dropna(subset=[\"date_only\"])\n",
    "                .groupby(\"date_only\", as_index=False)\n",
    "                .agg(tweets=(\"tweet_id\",\"count\"),\n",
    "                     replies=(\"reply_to_user\", lambda s: int(s.notna().sum())),\n",
    "                     retweets=(\"is_retweet\", lambda s: int(pd.Series(s).fillna(False).sum()))))\n",
    "else:\n",
    "    ts_daily = pd.DataFrame()\n",
    "\n",
    "# ---------- ConstrucciÃ³n de edges de interacciÃ³n (para vistas/CSV) ----------\n",
    "edges=[]\n",
    "for _,row in df_eda.iterrows():\n",
    "    src = row.get(\"username\")\n",
    "    if not src: continue\n",
    "    # menciones\n",
    "    for dst in (row.get(\"mentions\") or []):\n",
    "        if dst: edges.append((src,dst,\"mention\"))\n",
    "    # reply\n",
    "    dst = row.get(\"reply_to_user\")\n",
    "    if dst: edges.append((src,dst,\"reply\"))\n",
    "    # retweet\n",
    "    if row.get(\"is_retweet\") and row.get(\"retweeted_user\"):\n",
    "        edges.append((src,row.get(\"retweeted_user\"),\"retweet\"))\n",
    "\n",
    "df_edges = pd.DataFrame(edges, columns=[\"src\",\"dst\",\"type\"])\n",
    "\n",
    "# ---------- Guardados ----------\n",
    "out_dir = Path(\"eda_outputs\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "df_top_ht.to_csv(out_dir/\"top_hashtags.csv\", index=False)\n",
    "df_top_tokens.to_csv(out_dir/\"top_tokens.csv\", index=False)\n",
    "df_active_users.sort_values(\"tweets\", ascending=False).head(100).to_csv(out_dir/\"top_active_users.csv\", index=False)\n",
    "df_most_mentioned.head(100).to_csv(out_dir/\"top_most_mentioned.csv\", index=False)\n",
    "df_edges.to_csv(out_dir/\"edges_mentions_replies_retweets.csv\", index=False)\n",
    "ts_daily.to_csv(out_dir/\"timeline_daily.csv\", index=False)\n",
    "\n",
    "# ---------- GrÃ¡ficas rÃ¡pidas ----------\n",
    "plt.figure()\n",
    "df_top_ht.head(15).plot(kind=\"bar\", x=\"hashtag\", y=\"count\", legend=False, rot=45)\n",
    "plt.title(\"Top 15 hashtags\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir/\"top_hashtags.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "df_top_tokens.head(15).plot(kind=\"bar\", x=\"token\", y=\"count\", legend=False, rot=45)\n",
    "plt.title(\"Top 15 tokens (sin stopwords)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir/\"top_tokens.png\")\n",
    "plt.show()\n",
    "\n",
    "if not ts_daily.empty:\n",
    "    plt.figure()\n",
    "    plt.plot(ts_daily[\"date_only\"], ts_daily[\"tweets\"])\n",
    "    plt.title(\"Tweets por dÃ­a\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir/\"timeline_tweets.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Word cloud opcional si tienes instalado wordcloud\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    text_wc = \" \".join(t for xs in df_eda.get(\"tokens_nostop\", df_eda[\"tokens\"]) for t in (xs or []))\n",
    "    wc = WordCloud(width=1200, height=600, background_color=\"white\").generate(text_wc)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Nube de palabras (opcional)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir/\"wordcloud.png\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Nube de palabras no generada (falta 'wordcloud' o no hay tokens). Si quieres, instala 'wordcloud'.\")\n",
    "\n",
    "# ---------- ImpresiÃ³n del resumen ----------\n",
    "print(\"=== RESUMEN EDA (4.1) ===\")\n",
    "print(f\"Tweets totales: {n_tweets:,}\")\n",
    "print(f\"Usuarios Ãºnicos (emitentes): {n_users:,}\")\n",
    "print(f\"Menciones totales: {n_mentions_total:,} | Usuarios distintos mencionados: {n_users_mentioned:,}\")\n",
    "print(f\"Tweets que son replies: {n_replies:,}\")\n",
    "print(f\"Tweets que son retweets: {n_retweets:,}\")\n",
    "if not df_top_ht.empty:\n",
    "    print(\"\\nTop 10 hashtags:\")\n",
    "    for h,c in df_top_ht.head(10).itertuples(index=False):\n",
    "        print(f\"  #{h}: {c}\")\n",
    "if not df_active_users.empty:\n",
    "    print(\"\\nTop 10 usuarios por # de tweets:\")\n",
    "    for u,c in df_active_users.sort_values(\"tweets\", ascending=False).head(10).itertuples(index=False):\n",
    "        print(f\"  @{u}: {c}\")\n",
    "if not df_most_mentioned.empty:\n",
    "    print(\"\\nTop 10 usuarios mÃ¡s mencionados:\")\n",
    "    for u,c in df_most_mentioned.head(10).itertuples(index=False):\n",
    "        print(f\"  @{u}: {c}\")\n",
    "\n",
    "print(\"\\nArchivos generados en:\", str(out_dir.resolve()))\n",
    "print(\" - top_hashtags.csv / .png\")\n",
    "print(\" - top_tokens.csv / .png\")\n",
    "print(\" - top_active_users.csv\")\n",
    "print(\" - top_most_mentioned.csv\")\n",
    "print(\" - edges_mentions_replies_retweets.csv\")\n",
    "print(\" - timeline_daily.csv\", \"(y timeline_tweets.png si hay fechas)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d29115",
   "metadata": {},
   "source": [
    "## **Inciso 4b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda B â€” Inciso 4.2: Preguntas interesantes y resultados calculados ===\n",
    "# Imprime 3 preguntas sugeridas y los datos que permiten responderlas.\n",
    "# Al final: \"PREGUNTAS Y RESPUESTAS (4.2) â€“ RESULTADOS\"\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def _ensure_df_eda_again():\n",
    "    if \"df_eda\" in globals():\n",
    "        return df_eda\n",
    "    elif \"df_proc\" in globals():\n",
    "        return df_proc\n",
    "    else:\n",
    "        raise RuntimeError(\"Ejecuta primero la Celda A.\")\n",
    "\n",
    "D = _ensure_df_eda_again()\n",
    "\n",
    "# 1) Â¿QuÃ© usuarios son mÃ¡s mencionados (potenciales hubs de atenciÃ³n)?\n",
    "mentioned_counter = Counter()\n",
    "for xs in D[\"mentions\"]:\n",
    "    if not isinstance(xs, list): \n",
    "        continue\n",
    "    for m in xs:\n",
    "        if m: mentioned_counter[m] += 1\n",
    "df_q1 = pd.DataFrame(mentioned_counter.most_common(20), columns=[\"usuario\",\"menciones_recibidas\"])\n",
    "\n",
    "# 2) Â¿QuiÃ©nes son los usuarios mÃ¡s activos (mÃ¡s tweets emitidos) y quÃ© % de sus tweets son replies?\n",
    "df_q2 = (D.dropna(subset=[\"username\"])\n",
    "         .groupby(\"username\", as_index=False)\n",
    "         .agg(tweets=(\"tweet_id\",\"count\"),\n",
    "              replies=(\"reply_to_user\", lambda s: int(s.notna().sum()))))\n",
    "df_q2[\"pct_reply\"] = (df_q2[\"replies\"] / df_q2[\"tweets\"] * 100).round(1)\n",
    "df_q2 = df_q2.sort_values([\"tweets\",\"pct_reply\"], ascending=[False, False]).head(20)\n",
    "\n",
    "# 3) Â¿CuÃ¡les son los hashtags mÃ¡s usados y su presencia relativa?\n",
    "ht_counter = Counter(h for xs in D[\"hashtags\"] for h in (xs or []))\n",
    "total_ht = sum(ht_counter.values()) or 1\n",
    "df_q3 = (pd.DataFrame(ht_counter.most_common(20), columns=[\"hashtag\",\"freq\"])\n",
    "           .assign(pct=lambda x: (x[\"freq\"]/total_ht*100).round(2)))\n",
    "\n",
    "print(\"=== PREGUNTAS Y RESPUESTAS (4.2) â€“ RESULTADOS ===\")\n",
    "print(\"\\nP1) Usuarios mÃ¡s mencionados (Top 10):\")\n",
    "print(df_q1.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nP2) Usuarios mÃ¡s activos y % de replies (Top 10):\")\n",
    "print(df_q2.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nP3) Hashtags mÃ¡s usados y participaciÃ³n (Top 10):\")\n",
    "print(df_q3.head(10).to_string(index=False))\n",
    "\n",
    "# Guardados\n",
    "out_dir = Path(\"eda_outputs\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "df_q1.to_csv(out_dir/\"q1_mas_mencionados.csv\", index=False)\n",
    "df_q2.to_csv(out_dir/\"q2_mas_activos_y_pct_reply.csv\", index=False)\n",
    "df_q3.to_csv(out_dir/\"q3_hashtags_top_con_pct.csv\", index=False)\n",
    "print(f\"\\nArchivos: {str((out_dir/'q1_mas_mencionados.csv').resolve())}, \"\n",
    "      f\"{str((out_dir/'q2_mas_activos_y_pct_reply.csv').resolve())}, \"\n",
    "      f\"{str((out_dir/'q3_hashtags_top_con_pct.csv').resolve())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0c600",
   "metadata": {},
   "source": [
    "# AnÃ¡lisis Exploratorio (Inciso 4) â€” Tweets de *traficogt* y *tioberny*\n",
    "\n",
    "> **Entrada**: 10,539 tweets; 4,291 usuarios emisores; 28,037 menciones (1,856 usuarios distintos mencionados); 8,552 *replies*; 0 *retweets* detectados.  \n",
    "> **Fuente**: resultados de las Celdas 4.1 y 4.2 (grÃ¡ficas, tablas y CSV generados).\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Volumen y temporalidad\n",
    "\n",
    "- La serie **â€œTweets por dÃ­aâ€** muestra actividad casi nula antes de 2023 y **picos muy marcados en 2024** (dÃ­as con >1,000 tweets).  \n",
    "- Esto sugiere **coyunturas noticiosas/polÃ­ticas** recientes que disparan la conversaciÃ³n.  \n",
    "- RecomendaciÃ³n: marcar los **dÃ­as pico** y contrastar con sucesos (congresales, judiciales, protestas, bloqueos, etc.).\n",
    "\n",
    "**Indicadores rÃ¡pidos**\n",
    "\n",
    "- **% de replies**: â‰ˆ **81%** (8,552/10,539) â†’ conversaciÃ³n altamente **dialogante** (respuestas directas).  \n",
    "- **Menciones por tweet**: â‰ˆ **2.66** (28,037/10,539) â†’ alta **interacciÃ³n dirigida** entre usuarios.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Temas, vocabulario y etiquetas\n",
    "\n",
    "- En la nube de palabras y el *top* de tokens (sin *stopwords*) destacan **usuarios/cuentas e instituciones**:  \n",
    "  `barevalodeleon`, `traficogt`, `guatemalagob`, `mpguatemala`, `presidente`, `ubaldomacu`, `santipalomov`.  \n",
    "  â†’ Foco en **figuras pÃºblicas**, **cuentas institucionales** y **noticias**.\n",
    "\n",
    "- **Hashtags principales** (frecuencia y % del total):\n",
    "  1. `#guatemala` (40; 4.17%)  \n",
    "  2. `#ahora` (36; 3.75%)  \n",
    "  3. `#urgente` (33; 3.44%)  \n",
    "  4. `#guatemalasaleadelante` (24; 2.50%)  \n",
    "  5. `#traficogt` (22; 2.29%)  \n",
    "  6. `#ahoralh` (19; 1.98%)  \n",
    "  7. `#minfinsaleadelante` (17; 1.77%)  \n",
    "  8. `#renunciengolpistas` (15; 1.56%)  \n",
    "  9. `#presupuesto2025` (14; 1.46%)  \n",
    "  10. `#unpresupuestoparalapoblaciÃ³n` (13; 1.35%)\n",
    "\n",
    "**Lectura**: mezcla de etiquetas **noticiosas genÃ©ricas** (#ahora/#urgente), **institucionales** y **coyunturales** (presupuesto/consignas).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Actividad vs. atenciÃ³n: quiÃ©n habla y a quiÃ©n escuchan\n",
    "\n",
    "**Usuarios mÃ¡s activos (emiten mÃ¡s tweets)**  \n",
    "`@traficogt` (781), `@batallonjalapa` (134), `@mildred_gaitan` (105), `@lahoragt` (74), `@chofito63569841` (49), `@angeln8` (46), `@prensacomunitar` (45), `@papaabumario` (43), `@elrevoltijogt` (40), `@hellboy17oc` (39).\n",
    "\n",
    "- `@traficogt` concentra â‰ˆ **7.4%** de los tweets y solo **5%** son *replies* â†’ **rol difusor/broadcast**.  \n",
    "- Varias cuentas *top* son **100% replies** (p. ej., `@batallonjalapa`, `@mildred_gaitan`, `@chofito63569841`), lo que sugiere **participaciÃ³n reactiva** (respuesta directa, potencial coordinaciÃ³n/activismo o usuarios muy conversacionales).\n",
    "\n",
    "**Usuarios mÃ¡s mencionados (reciben mÃ¡s atenciÃ³n)**  \n",
    "`@barevalodeleon` (**5,168**), `@traficogt` (**4,232**), `@guatemalagob` (961), `@mpguatemala` (602), `@ubaldomacu` (572), `@santipalomov` (518), `@fjimenezmingob` (411), `@drgiammattei` (318), `@mingobguate` (309), `@congresoguate` (309).\n",
    "\n",
    "- **ConcentraciÃ³n**: las 2 cuentas principales acumulan ~**33â€“34%** de todas las menciones; el **top-10** supera **~48%** â†’ **estructura fuertemente centralizada** alrededor de pocos **hubs** (figuras pÃºblicas y cuentas informativas/institucionales).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) DinÃ¡mica de interacciÃ³n esperada en la red\n",
    "\n",
    "Dado el predominio de **menciones** y **replies**, el grafo dirigido (usuarios como nodos; â€œmenciona/responde aâ€ como aristas) probablemente muestre:\n",
    "\n",
    "- **Alta indegree** en **@barevalodeleon**, **@traficogt**, **@guatemalagob**, **@mpguatemala** (hubs de atenciÃ³n).  \n",
    "- **Comunidades** articuladas alrededor de **medios/noticieros** y de **cuentas institucionales**; probablemente puentes entre comunidades a travÃ©s de estas cuentas.  \n",
    "- **DifusiÃ³n** inferida por menciones/respuestas (no por RTs), pues **no se detectaron retweets nativos** en la recolecciÃ³n.\n",
    "\n",
    "> En el inciso 3.4/5 construiremos el grafo con esas interacciones (menciones + replies) y calcularemos **densidad, diÃ¡metro y coeficiente de agrupamiento**, ademÃ¡s de centralidades para revelar â€œpoderâ€/influencia.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Respuestas a las 3 preguntas (4.2)\n",
    "\n",
    "**P1. Â¿QuiÃ©nes concentran la atenciÃ³n?**  \n",
    "Top menciones: **@barevalodeleon (5,168)** y **@traficogt (4,232)**; les siguen **@guatemalagob** y **@mpguatemala**. La curva es muy concentrada (estructura de hubs).\n",
    "\n",
    "**P2. Â¿QuiÃ©nes son mÃ¡s activos y quÃ© tan dialogantes?**  \n",
    "`@traficogt` lidera en volumen con bajo % de *replies* (**5%**), tÃ­pico de **cuenta difusora**. Varias cuentas *top* son **100% replies**, lo que indica **participaciÃ³n reactiva** (interpelaciÃ³n a otros, conversaciÃ³n o campaÃ±as de respuesta).\n",
    "\n",
    "**P3. Â¿QuÃ© hashtags organizan la conversaciÃ³n?**  \n",
    "Predominio de **etiquetas noticiosas** (#ahora/#urgente), **institucionales** (#guatemalasaleadelante, #minfinsaleadelante) y **temas coyunturales** (#presupuesto2025, consignas). â†’ Los **picos diarios** probablemente se explican por **eventos** asociados a estas etiquetas.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Limitaciones y consideraciones\n",
    "\n",
    "- **Retweets = 0**: la colecciÃ³n/parseo no incluye RTs nativos â†’ la nociÃ³n de â€œdifusiÃ³nâ€ se analizarÃ¡ vÃ­a **menciones** y **replies**.  \n",
    "- **Cobertura temporal** sesgada a 2023â€“2024 (picos en 2024).  \n",
    "- **Calidad de texto**: se corrigiÃ³ **mojibake** y se normalizÃ³ UTF-16â†’UTF-8; limpieza de URLs, emojis, puntuaciÃ³n, *stopwords* y nÃºmeros.  \n",
    "- Se eliminaron **84 duplicados** (â‰ˆ0.8%).\n",
    "\n",
    "---\n",
    "\n",
    "## 7) LÃ­neas de investigaciÃ³n sugeridas\n",
    "\n",
    "1. **Picos** temporales: aislar dÃ­as >P95 de actividad y revisar **hashtags/usuarios dominantes**.  \n",
    "2. **Comunidades**: detectar mÃ³dulos en el grafo (Louvain/Leiden) y perfilar sus **temas/medios**.  \n",
    "3. **Hubs y puentes**: comparar **in-degree/out-degree**, **PageRank** y **betweenness** de cuentas institucionales y figuras pÃºblicas.  \n",
    "4. **ConversaciÃ³n dirigida**: medir quÃ© fracciÃ³n de *replies* se concentra en los **top hubs**.  \n",
    "5. **Estabilidad**: comparar mÃ©tricas por **mes** para ver **persistencia** de influencia vs. **eventos puntuales**.\n",
    "\n",
    "---\n",
    "\n",
    "### ConclusiÃ³n\n",
    "\n",
    "El corpus exhibe una **conversaciÃ³n intensa y dirigida**, con **altas tasas de respuesta** y **concentraciÃ³n de menciones** en pocas cuentas (hubs informativos/institucionales y figuras pÃºblicas). La **temporalidad** apunta a eventos coyunturales recientes. El siguiente paso natural es construir el **grafo dirigido** (menciones + replies) y analizar su **topologÃ­a** (densidad, diÃ¡metro, *clustering*) y **centralidades** para evidenciar relaciones de poder e influencia en la red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eca938",
   "metadata": {},
   "source": [
    "## **Inciso 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inciso 5 (5.1 + 5.2) â€” TopologÃ­a de la red: construcciÃ³n, visualizaciÃ³n y mÃ©tricas ===\n",
    "# Requisitos:\n",
    "#   - Haber generado 'eda_outputs/edges_mentions_replies_retweets.csv' en el inciso 4 (Celda A),\n",
    "#     o bien tener 'df_proc' en memoria (reconstruiremos el edge list si hace falta).\n",
    "# QuÃ© hace:\n",
    "#   1) Carga/arma aristas dirigidas (src -> dst) de menciones, replies y retweets (si existieran).\n",
    "#   2) Construye un grafo dirigido con pesos (conteos de interacciones).\n",
    "#   3) Calcula mÃ©tricas clave: densidad, diÃ¡metro (en el mayor componente), coef. de agrupamiento,\n",
    "#      in/out-degree (ponderados y no), PageRank y betweenness (aprox. para escalar).\n",
    "#   4) Detecta comunidades (Louvain si disponible, si no Greedy Modularity) en un subgrafo top-200.\n",
    "#   5) Visualiza el subgrafo (color = comunidad; tamaÃ±o = PageRank) y guarda artefactos.\n",
    "#\n",
    "# Al terminar, imprime un bloque: \"=== RESUMEN TOPOLOGÃA (5.1â€“5.2) ===\"\n",
    "# EnvÃ­ame ese bloque y te hago el anÃ¡lisis en Markdown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# -----------------------\n",
    "# 0) Cargar el edge list\n",
    "# -----------------------\n",
    "def _norm_user(u):\n",
    "    if u is None or (isinstance(u, float) and np.isnan(u)):\n",
    "        return None\n",
    "    u = str(u).strip()\n",
    "    if not u:\n",
    "        return None\n",
    "    if u.startswith(\"@\"):\n",
    "        u = u[1:]\n",
    "    return u.lower()\n",
    "\n",
    "edges_path = Path(\"eda_outputs/edges_mentions_replies_retweets.csv\")\n",
    "if edges_path.exists():\n",
    "    df_edges = pd.read_csv(edges_path)\n",
    "else:\n",
    "    # Fallback: reconstruir rÃ¡pidamente desde df_proc si estÃ¡ en memoria\n",
    "    if \"df_proc\" not in globals():\n",
    "        raise RuntimeError(\"No encuentro 'eda_outputs/edges_mentions_replies_retweets.csv' ni 'df_proc'. Vuelve a ejecutar la Celda A del inciso 4.\")\n",
    "    rows = []\n",
    "    for _, row in df_proc.iterrows():\n",
    "        src = _norm_user(row.get(\"username\"))\n",
    "        if not src:\n",
    "            continue\n",
    "        # menciones\n",
    "        for dst in (row.get(\"mentions\") or []):\n",
    "            dst = _norm_user(dst)\n",
    "            if dst:\n",
    "                rows.append((src, dst, \"mention\"))\n",
    "        # reply\n",
    "        dst = _norm_user(row.get(\"reply_to_user\"))\n",
    "        if dst:\n",
    "            rows.append((src, dst, \"reply\"))\n",
    "        # retweet\n",
    "        if bool(row.get(\"is_retweet\")) and row.get(\"retweeted_user\"):\n",
    "            dst = _norm_user(row.get(\"retweeted_user\"))\n",
    "            if dst:\n",
    "                rows.append((src, dst, \"retweet\"))\n",
    "    df_edges = pd.DataFrame(rows, columns=[\"src\",\"dst\",\"type\"])\n",
    "\n",
    "# NormalizaciÃ³n bÃ¡sica\n",
    "if \"type\" not in df_edges.columns:\n",
    "    df_edges[\"type\"] = \"mention\"\n",
    "df_edges[\"src\"] = df_edges[\"src\"].apply(_norm_user)\n",
    "df_edges[\"dst\"] = df_edges[\"dst\"].apply(_norm_user)\n",
    "df_edges = df_edges.dropna(subset=[\"src\",\"dst\"])\n",
    "df_edges = df_edges[df_edges[\"src\"] != df_edges[\"dst\"]]  # quita self-loops\n",
    "df_edges[\"type\"] = df_edges[\"type\"].fillna(\"mention\").str.lower()\n",
    "\n",
    "# Conteo por tipo (para el resumen)\n",
    "type_counts = df_edges.groupby(\"type\").size().to_dict()\n",
    "\n",
    "# Agregar pesos por (src,dst,type) y por (src,dst)\n",
    "df_edges_type = (\n",
    "    df_edges\n",
    "    .groupby([\"src\",\"dst\",\"type\"], as_index=False)\n",
    "    .size()\n",
    "    .rename(columns={\"size\":\"weight\"})\n",
    ")\n",
    "df_edges_total = (\n",
    "    df_edges\n",
    "    .groupby([\"src\",\"dst\"], as_index=False)\n",
    "    .size()\n",
    "    .rename(columns={\"size\":\"weight\"})\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1) Construir grafo dirigido con pesos\n",
    "# ---------------------------------------\n",
    "G = nx.DiGraph()\n",
    "for r in df_edges_type.itertuples(index=False):\n",
    "    # Nota: si hay mÃºltiples tipos para el mismo par, aÃ±adimos/actualizamos acumulando peso total en 'weight'\n",
    "    if G.has_edge(r.src, r.dst):\n",
    "        G[r.src][r.dst][\"weight\"] = G[r.src][r.dst].get(\"weight\", 0) + r.weight\n",
    "        # Guardamos una lista de tipos para referencia\n",
    "        prev_types = set(G[r.src][r.dst].get(\"types\", []))\n",
    "        prev_types.add(r.type)\n",
    "        G[r.src][r.dst][\"types\"] = list(prev_types)\n",
    "    else:\n",
    "        G.add_edge(r.src, r.dst, weight=r.weight, types=[r.type])\n",
    "\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges()\n",
    "density_dir = nx.density(G)  # m / (n*(n-1))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) MÃ©tricas: clustering, diÃ¡metro y componentes\n",
    "# ----------------------------------------------------\n",
    "# Usamos versiÃ³n no dirigida para clustering/diÃ¡metro\n",
    "Gu = G.to_undirected()\n",
    "avg_clustering = nx.average_clustering(Gu) if n_nodes > 0 else float(\"nan\")\n",
    "\n",
    "# DiÃ¡metro en el mayor componente conexo (no dirigido)\n",
    "diameter_val = None\n",
    "if n_nodes > 1 and n_edges > 0:\n",
    "    cc = list(nx.connected_components(Gu))\n",
    "    if cc:\n",
    "        largest_cc_nodes = max(cc, key=len)\n",
    "        Gcc = Gu.subgraph(largest_cc_nodes).copy()\n",
    "        try:\n",
    "            # Si el componente no es muy grande, cÃ¡lculo exacto\n",
    "            if Gcc.number_of_nodes() <= 2000:\n",
    "                diameter_val = nx.diameter(Gcc)\n",
    "            else:\n",
    "                # AproximaciÃ³n para grafos grandes\n",
    "                from networkx.algorithms.approximation.distance_measures import diameter as approx_diameter\n",
    "                diameter_val = approx_diameter(Gcc)\n",
    "        except Exception:\n",
    "            diameter_val = None\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) Centralidades y rankings\n",
    "# ----------------------------------------------------\n",
    "# In/Out-degree (no ponderado y ponderado)\n",
    "in_deg = dict(G.in_degree(weight=None))\n",
    "out_deg = dict(G.out_degree(weight=None))\n",
    "in_deg_w = dict(G.in_degree(weight=\"weight\"))\n",
    "out_deg_w = dict(G.out_degree(weight=\"weight\"))\n",
    "\n",
    "# PageRank (ponderado)\n",
    "pagerank = nx.pagerank(G, alpha=0.85, weight=\"weight\") if n_nodes > 0 else {}\n",
    "\n",
    "# Betweenness aproximado (no dirigido) para escalar mejor\n",
    "k_sample = min(500, max(10, int(n_nodes * 0.2)))  # muestreo de pivotes\n",
    "try:\n",
    "    betw = nx.betweenness_centrality(Gu, k=k_sample, seed=42)\n",
    "except Exception:\n",
    "    betw = {}\n",
    "\n",
    "# DataFrame de mÃ©tricas por nodo\n",
    "metrics = pd.DataFrame({\"user\": list(G.nodes())})\n",
    "metrics[\"indegree\"] = metrics[\"user\"].map(in_deg).fillna(0).astype(int)\n",
    "metrics[\"outdegree\"] = metrics[\"user\"].map(out_deg).fillna(0).astype(int)\n",
    "metrics[\"indegree_w\"] = metrics[\"user\"].map(in_deg_w).fillna(0).astype(int)\n",
    "metrics[\"outdegree_w\"] = metrics[\"user\"].map(out_deg_w).fillna(0).astype(int)\n",
    "metrics[\"pagerank\"] = metrics[\"user\"].map(pagerank).fillna(0.0)\n",
    "metrics[\"betweenness\"] = metrics[\"user\"].map(betw).fillna(0.0)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) Comunidades y visualizaciÃ³n (subgrafo top-200)\n",
    "# ----------------------------------------------------\n",
    "topN = 200\n",
    "top_nodes = (\n",
    "    metrics.sort_values([\"indegree_w\",\"pagerank\"], ascending=False)\n",
    "           .head(topN)[\"user\"]\n",
    "           .tolist()\n",
    ")\n",
    "H = G.subgraph(top_nodes).copy()\n",
    "Hu = H.to_undirected()\n",
    "\n",
    "# DetecciÃ³n de comunidades\n",
    "try:\n",
    "    from networkx.algorithms.community import louvain_communities\n",
    "    comms = louvain_communities(Hu, seed=42)\n",
    "except Exception:\n",
    "    from networkx.algorithms.community import greedy_modularity_communities\n",
    "    comms = list(greedy_modularity_communities(Hu))\n",
    "\n",
    "comm_map = {}\n",
    "for cid, com in enumerate(comms):\n",
    "    for n in com:\n",
    "        comm_map[n] = cid\n",
    "\n",
    "# Layout y dibujo\n",
    "pos = nx.spring_layout(Hu, seed=42)\n",
    "node_sizes = []\n",
    "for n in Hu.nodes():\n",
    "    pr = metrics.loc[metrics[\"user\"] == n, \"pagerank\"]\n",
    "    pr = pr.values[0] if len(pr) else 0.0\n",
    "    node_sizes.append(200 + 8000 * pr)  # tamaÃ±o ~ PageRank\n",
    "\n",
    "node_colors = [comm_map.get(n, -1) for n in Hu.nodes()]\n",
    "edge_widths = [max(0.4, min(4.0, H[u][v].get(\"weight\",1) / 5.0)) for u,v in H.edges()]\n",
    "\n",
    "out_dir = Path(\"net_outputs\"); out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "nx.draw_networkx_edges(Hu, pos, alpha=0.15, width=edge_widths)\n",
    "nodes = nx.draw_networkx_nodes(Hu, pos, node_size=node_sizes, node_color=node_colors, cmap=plt.cm.tab20, alpha=0.9)\n",
    "# Etiquetas solo para los 20 nodos con mayor indegree ponderado\n",
    "label_nodes = (\n",
    "    metrics.sort_values(\"indegree_w\", ascending=False)\n",
    "           .head(20)[\"user\"].tolist()\n",
    ")\n",
    "nx.draw_networkx_labels(Hu, pos, labels={n:n for n in Hu.nodes() if n in label_nodes}, font_size=9)\n",
    "plt.title(\"Grafo de interacciones (subgrafo top-200 por in-degree ponderado)\\nColor=Comunidad  |  TamaÃ±o=PageRank  |  Arista=Peso\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"graph_top200.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5) Guardar artefactos y reportar resumen\n",
    "# ----------------------------------------------------\n",
    "metrics.sort_values(\"indegree_w\", ascending=False).to_csv(out_dir / \"node_metrics.csv\", index=False)\n",
    "df_edges_total.to_csv(out_dir / \"edge_weights.csv\", index=False)\n",
    "\n",
    "print(\"=== RESUMEN TOPOLOGÃA (5.1â€“5.2) ===\")\n",
    "print(f\"Nodos: {n_nodes:,}  |  Aristas (dirigidas): {n_edges:,}  |  Densidad (dirigida): {density_dir:.6f}\")\n",
    "print(f\"Coeficiente de agrupamiento (promedio, no dirigido): {avg_clustering:.4f}\")\n",
    "print(\"DiÃ¡metro (en el mayor componente no dirigido):\", \"N/A\" if diameter_val is None else diameter_val)\n",
    "print(\"Interacciones por tipo:\", type_counts)\n",
    "\n",
    "print(\"\\nTop 10 por in-degree ponderado:\")\n",
    "print(metrics.sort_values(\"indegree_w\", ascending=False).head(10)[[\"user\",\"indegree_w\",\"pagerank\",\"betweenness\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 por PageRank:\")\n",
    "print(metrics.sort_values(\"pagerank\", ascending=False).head(10)[[\"user\",\"pagerank\",\"indegree_w\",\"outdegree_w\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 por betweenness (aprox, no dirigido):\")\n",
    "print(metrics.sort_values(\"betweenness\", ascending=False).head(10)[[\"user\",\"betweenness\",\"indegree_w\",\"outdegree_w\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nArtefactos guardados en:\", str(out_dir.resolve()))\n",
    "print(\" - net_outputs/graph_top200.png\")\n",
    "print(\" - net_outputs/node_metrics.csv\")\n",
    "print(\" - net_outputs/edge_weights.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0faedfb",
   "metadata": {},
   "source": [
    "# Inciso 5 â€” AnÃ¡lisis de la topologÃ­a de la red (menciones + respuestas)\n",
    "\n",
    "> **Datos del grafo**  \n",
    "> Nodos: **5,213** Â· Aristas dirigidas: **19,476** Â· **Densidad**: 0.000717  \n",
    "> **Coef. de agrupamiento (promedio, no dirigido)**: **0.3129** Â· **DiÃ¡metro** (mayor componente): **7**  \n",
    "> **Interacciones por tipo**: menciones = **28,035**, respuestas = **8,409** (RT nativos no presentes en la muestra)\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 ConstrucciÃ³n y visualizaciÃ³n de grafos\n",
    "\n",
    "- La visualizaciÃ³n del **subgrafo top-200 por in-degree ponderado** muestra un **nÃºcleo denso** donde se concentran las interacciones y mÃºltiples **comunidades** (colores distintos).  \n",
    "  - Nodos etiquetados visibles: `barevalodeleon`, `traficogt`, `guatemalagob`, `mpguatemala`, etc.  \n",
    "  - Los **nodos perifÃ©ricos** alrededor del dibujo aparecen aislados porque, aunque reciben muchas interacciones desde *fuera* del top-200, **no forman lazos fuertes entre sÃ­** dentro de este subgrafo.\n",
    "- **Baja densidad (0.000717)**: tÃ­pica de redes de menciÃ³n/respuesta a gran escala (sparsas) donde pocos pares se conectan directamente.\n",
    "- **Agrupamiento medio alto (0.313)**: sugiere **formaciÃ³n de clÃºsteres**/comunidades locales (p. ej., cuentas institucionales, medios, activismo).\n",
    "- **DiÃ¡metro = 7** sobre el mayor componente: propiedad **â€œsmall-worldâ€** â€” la mayorÃ­a de los nodos se conectan en pocos saltos a travÃ©s de hubs.\n",
    "\n",
    "**Lectura de poder / visibilidad en la figura**\n",
    "- El **tamaÃ±o** (PageRank) y la **posiciÃ³n central** del nÃºcleo resaltan a **@barevalodeleon** y **@traficogt** como focos de atenciÃ³n.  \n",
    "- La mezcla de colores cerca del nÃºcleo indica que **varias comunidades** estÃ¡n **acopladas** por estos hubs (actÃºan como puntos de convergencia entre grupos temÃ¡ticos).\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 MÃ©tricas de red clave e interpretaciÃ³n\n",
    "\n",
    "### Hubs de atenciÃ³n (in-degree ponderado)\n",
    "Top por **indegree_w**:\n",
    "1. **@barevalodeleon** (5,917) â€” PR **0.1305**, betweenness **0.5678**, **outdegree_w = 0**  \n",
    "2. **@traficogt** (5,622) â€” PR **0.0719**, betweenness **0.4770**, **outdegree_w = 150**  \n",
    "3. **@guatemalagob** (1,171) â€” PR **0.0177**  \n",
    "4. **@ubaldomacu** (1,022) â€” PR **0.0088**  \n",
    "5. **@mpguatemala** (605) â€” PR **0.0109**\n",
    "\n",
    "**Insight**  \n",
    "- **@barevalodeleon** y **@traficogt** concentran **gran parte de las menciones/respuestas**.  \n",
    "  - `@barevalodeleon` domina en **in-degree** y **PageRank** aun con **cero salida**, tÃ­pico de **â€œsumideroâ€ de atenciÃ³n** (muchos hablan *a* Ã©l/ella, casi no responde).  \n",
    "  - `@traficogt` combina **alta atenciÃ³n** con **actividad de salida** (outdegree_w=150), perfil de **difusor** que ademÃ¡s conecta comunidades.\n",
    "\n",
    "### Influencia estructural (PageRank y betweenness)\n",
    "Top por **PageRank** incluye, ademÃ¡s de los dos anteriores, cuentas **institucionales**:\n",
    "- `@guatemalagob`, `@drgiammattei`, `@mpguatemala`, `@cc_guatemala`, `@usaidguate`, `@ivanduque`.  \n",
    "â†’ Indica que, aun sin ser los mÃ¡s mencionados, **canalizan flujos de interacciÃ³n** relevantes en el grafo.\n",
    "\n",
    "Top por **betweenness** (puentes):\n",
    "- AdemÃ¡s de `@barevalodeleon` y `@traficogt`, destacan **@mildred_gaitan** y **@batallonjalapa** con **betweenness alto** pero **indegree bajo** y **outdegree muy alto** (398 y 461).  \n",
    "â†’ **Nodos â€œconectoresâ€**: responden a muchos y **unen clusters** que de otro modo quedarÃ­an separados (posible activismo/brigadas de respuesta, cuentas muy conversacionales o coordinadores).\n",
    "\n",
    "### ConclusiÃ³n de mÃ©tricas\n",
    "- La red estÃ¡ **altamente centralizada** en dos hubs principales, con **comunidades bien definidas** alrededor de **instituciones** y **figuras pÃºblicas**.  \n",
    "- La combinaciÃ³n de **clustering elevado** y **diÃ¡metro corto** confirma un **patrÃ³n small-world**: pocos nodos estratÃ©gicos bastan para conectar a la mayorÃ­a.  \n",
    "- La ausencia de RT nativos en los datos **desplaza la nociÃ³n de difusiÃ³n** hacia **menciones y respuestas**; aun asÃ­, los **hubs y conectores** emergen con claridad.\n",
    "\n",
    "---\n",
    "\n",
    "## Recomendaciones de profundizaciÃ³n\n",
    "\n",
    "1. **Comunidades**: cuantificar tamaÃ±os y etiquetarlas por **hashtags/tokens predominantes** para perfilar temas (Louvain/Leiden).  \n",
    "2. **Roles**: contrastar **indegree_w vs. outdegree_w** para clasificar **sumideros de atenciÃ³n**, **difusores** y **conectores**.  \n",
    "3. **EvoluciÃ³n**: repetir mÃ©tricas por **ventanas temporales** (mensual/semanal) para detectar **picos** y **cambios de liderazgo**.  \n",
    "4. **Robustez**: si se dispone de RTs nativos en otra extracciÃ³n, incorporar **retweets** para medir **alcance** y **cascadeo**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69637ad1",
   "metadata": {},
   "source": [
    "# Inciso 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5df0e",
   "metadata": {},
   "source": [
    "## 6.1. IdentificaciÃ³n y anÃ¡lisis de comunidades (elecciÃ³n del algoritmo)\n",
    "\n",
    "### Â¿QuÃ© algoritmo usar?\n",
    "\n",
    "El **mÃ©todo Louvain** (Blondel et al., 2008) es el estÃ¡ndar histÃ³rico mÃ¡s difundido para detecciÃ³n de comunidades por su rapidez y calidad al optimizar modularidad; aparece en reseÃ±as clÃ¡sicas y estÃ¡ disponible en librerÃ­as comunes (e.g., *NetworkX*), lo que facilita la reproducibilidad.  \n",
    "\n",
    "**Referencias:**\n",
    "- [arXiv](https://arxiv.org/)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 â€” DetecciÃ³n de comunidades con Louvain (y fallback a Greedy Modularity)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ---------- 1) Localizar el archivo de aristas ----------\n",
    "CANDIDATES = [\n",
    "    \"net_outputs/edge_weights.csv\",                  # si el notebook estÃ¡ en lab6ds/\n",
    "    \"../net_outputs/edge_weights.csv\",               # si el notebook estÃ¡ en lab6ds/notebooks/\n",
    "    \"lab6ds/net_outputs/edge_weights.csv\",           # si ejecutas desde la raÃ­z del zip\n",
    "    \"./edge_weights.csv\",                            # Ãºltimo recurso (por si moviste el archivo)\n",
    "]\n",
    "\n",
    "edges_path = None\n",
    "for p in CANDIDATES:\n",
    "    if os.path.exists(p):\n",
    "        edges_path = p\n",
    "        break\n",
    "\n",
    "if edges_path is None:\n",
    "    raise FileNotFoundError(\"No se encontrÃ³ 'edge_weights.csv'. Ajusta la ruta en CANDIDATES.\")\n",
    "\n",
    "print(f\"Usando archivo de aristas: {edges_path}\")\n",
    "\n",
    "# ---------- 2) Cargar aristas y construir el grafo dirigido ----------\n",
    "edges = pd.read_csv(edges_path)\n",
    "edges.columns = [c.strip().lower() for c in edges.columns]\n",
    "\n",
    "# Aceptamos nombres de columnas comunes\n",
    "col_src = \"src\" if \"src\" in edges.columns else (\"source\" if \"source\" in edges.columns else None)\n",
    "col_dst = \"dst\" if \"dst\" in edges.columns else (\"target\" if \"target\" in edges.columns else None)\n",
    "col_w  = \"weight\" if \"weight\" in edges.columns else None\n",
    "\n",
    "assert col_src and col_dst and col_w, f\"Se esperaban columnas tipo src/dst/weight; columnas reales: {edges.columns.tolist()}\"\n",
    "\n",
    "G_directed = nx.DiGraph()\n",
    "for row in edges.itertuples(index=False):\n",
    "    G_directed.add_edge(getattr(row, col_src), getattr(row, col_dst), weight=float(getattr(row, col_w)))\n",
    "\n",
    "print(f\"Grafo dirigido: {G_directed.number_of_nodes()} nodos, {G_directed.number_of_edges()} aristas\")\n",
    "\n",
    "# ---------- 3) ProyecciÃ³n no dirigida ponderada ----------\n",
    "G_undirected = nx.Graph()\n",
    "for u, v, d in G_directed.edges(data=True):\n",
    "    w = float(d.get(\"weight\", 1.0))\n",
    "    if G_undirected.has_edge(u, v):\n",
    "        G_undirected[u][v][\"weight\"] += w\n",
    "    else:\n",
    "        G_undirected.add_edge(u, v, weight=w)\n",
    "\n",
    "print(f\"Grafo NO dirigido para comunidades: {G_undirected.number_of_nodes()} nodos, {G_undirected.number_of_edges()} aristas\")\n",
    "\n",
    "# ---------- 4) Ejecutar Louvain (o fallback) ----------\n",
    "algo_used = None\n",
    "communities_list = None\n",
    "\n",
    "try:\n",
    "    # NetworkX >= 2.6: louvain_communities\n",
    "    from networkx.algorithms.community import louvain_communities\n",
    "    communities_list = louvain_communities(G_undirected, weight=\"weight\", resolution=1.0, seed=42)\n",
    "    algo_used = \"Louvain (NetworkX)\"\n",
    "except Exception as e:\n",
    "    # Fallback: greedy_modularity_communities\n",
    "    from networkx.algorithms.community import greedy_modularity_communities\n",
    "    communities_list = greedy_modularity_communities(G_undirected, weight=\"weight\")\n",
    "    algo_used = \"Greedy Modularity (fallback)\"\n",
    "    print(f\"[Aviso] Louvain no disponible ({e}). Usando {algo_used}.\")\n",
    "\n",
    "# communities_list es una lista de 'sets' de nodos\n",
    "num_com = len(communities_list)\n",
    "sizes = sorted([len(c) for c in communities_list], reverse=True)\n",
    "print(f\"Algoritmo: {algo_used}. Comunidades detectadas: {num_com}\")\n",
    "print(\"TamaÃ±os de las 10 comunidades mÃ¡s grandes:\", sizes[:10])\n",
    "\n",
    "# ---------- 5) AsignaciÃ³n nodo -> comunidad y resumen ----------\n",
    "node2community = {}\n",
    "for cid, com in enumerate(communities_list):\n",
    "    for n in com:\n",
    "        node2community[n] = cid\n",
    "\n",
    "communities_df = pd.DataFrame(\n",
    "    [{\"user\": n, \"community\": node2community[n]} for n in G_undirected.nodes()]\n",
    ").sort_values([\"community\",\"user\"]).reset_index(drop=True)\n",
    "\n",
    "sizes_df = (\n",
    "    communities_df.groupby(\"community\", as_index=False)\n",
    "    .size()\n",
    "    .rename(columns={\"size\": \"size_nodes\"})\n",
    "    .sort_values(\"size_nodes\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ---------- 6) Moduralidad (calidad de la particiÃ³n) ----------\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "mod_score = modularity(G_undirected, communities_list, weight=\"weight\")\n",
    "\n",
    "print(f\"Modularidad de la particiÃ³n: {mod_score:.4f}\")\n",
    "\n",
    "# ---------- 7) Mostrar cabezales Ãºtiles ----------\n",
    "display(sizes_df.head(10))\n",
    "display(communities_df.head(10))\n",
    "\n",
    "# Objetos que usaremos en 6.2/6.3\n",
    "RESULTS_61 = {\n",
    "    \"G_directed\": G_directed,\n",
    "    \"G_undirected\": G_undirected,\n",
    "    \"communities_list\": communities_list,   # lista de sets\n",
    "    \"node2community\": node2community,       # dict user -> comunidad\n",
    "    \"communities_df\": communities_df,       # tabla asignaciÃ³n\n",
    "    \"sizes_df\": sizes_df,                   # tamaÃ±os\n",
    "    \"modularity\": mod_score,\n",
    "    \"algo_used\": algo_used,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45edf546",
   "metadata": {},
   "source": [
    "## 6.2. VisualizaciÃ³n y caracterizaciÃ³n de comunidades\n",
    "\n",
    "1. Partimos de los resultados del **6.1** (`RESULTS_61` con `G_directed`, `G_undirected`, `communities_list`, `node2community`, etc.).  \n",
    "2. Hacemos una **vista general** (subgrafo con los nodos de mayor grado para legibilidad), coloreando por comunidad.  \n",
    "3. Graficamos solo las **3 comunidades mÃ¡s grandes**, dimensionando nodos por **PageRank** (en el subgrafo dirigido de esas comunidades) para sugerir influencia.  \n",
    "4. Calculamos, por comunidad:  \n",
    "   - `size_nodes`  \n",
    "   - Aristas internas dirigidas (conteo y peso)  \n",
    "   - Aristas externas (entrantes/salientes y sus pesos)  \n",
    "   - Densidad (subgrafo no dirigido)  \n",
    "5. Top-5 usuarios por **indegree local ponderado** (nodos mÃ¡s citados/etiquetados dentro de su comunidad).  \n",
    "6. Tomamos tokens de `tweets_clean_sample.csv` y listamos **tÃ©rminos frecuentes** por comunidad (aprox.) para las **Top-3**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2.A â€” VisualizaciÃ³n de comunidades\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== 1) Recuperar resultados del 6.1 o reconstruir mÃ­nimo si no existen ====\n",
    "def _rebuild_from_edges():\n",
    "    CANDIDATES = [\n",
    "        \"net_outputs/edge_weights.csv\",\n",
    "        \"../net_outputs/edge_weights.csv\",\n",
    "        \"lab6ds/net_outputs/edge_weights.csv\",\n",
    "        \"./edge_weights.csv\",\n",
    "    ]\n",
    "    edges_path = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
    "    if edges_path is None:\n",
    "        raise FileNotFoundError(\"No se encontrÃ³ edge_weights.csv; ajusta rutas en CANDIDATES.\")\n",
    "\n",
    "    edges = pd.read_csv(edges_path)\n",
    "    edges.columns = [c.strip().lower() for c in edges.columns]\n",
    "    col_src = \"src\" if \"src\" in edges.columns else (\"source\" if \"source\" in edges.columns else None)\n",
    "    col_dst = \"dst\" if \"dst\" in edges.columns else (\"target\" if \"target\" in edges.columns else None)\n",
    "    col_w  = \"weight\" if \"weight\" in edges.columns else None\n",
    "    assert col_src and col_dst and col_w, f\"Columnas esperadas tipo src/dst/weight; reales: {edges.columns.tolist()}\"\n",
    "\n",
    "    Gd = nx.DiGraph()\n",
    "    for r in edges.itertuples(index=False):\n",
    "        Gd.add_edge(getattr(r, col_src), getattr(r, col_dst), weight=float(getattr(r, col_w)))\n",
    "    Gu = nx.Graph()\n",
    "    for u, v, d in Gd.edges(data=True):\n",
    "        w = float(d.get(\"weight\", 1.0))\n",
    "        if Gu.has_edge(u, v):\n",
    "            Gu[u][v][\"weight\"] += w\n",
    "        else:\n",
    "            Gu.add_edge(u, v, weight=w)\n",
    "\n",
    "    from networkx.algorithms.community import louvain_communities\n",
    "    comms = louvain_communities(Gu, weight=\"weight\", seed=42)\n",
    "    node2com = {}\n",
    "    for cid, com in enumerate(comms):\n",
    "        for n in com:\n",
    "            node2com[n] = cid\n",
    "    sizes_df = (\n",
    "        pd.DataFrame({\"user\": list(node2com.keys()), \"community\": list(node2com.values())})\n",
    "        .groupby(\"community\", as_index=False).size()\n",
    "        .rename(columns={\"size\": \"size_nodes\"})\n",
    "        .sort_values(\"size_nodes\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return Gd, Gu, comms, node2com, sizes_df\n",
    "\n",
    "try:\n",
    "    Gu = RESULTS_61[\"G_undirected\"]\n",
    "    Gd = RESULTS_61[\"G_directed\"]\n",
    "    communities_list = RESULTS_61[\"communities_list\"]\n",
    "    node2community = RESULTS_61[\"node2community\"]\n",
    "    sizes_df = RESULTS_61[\"sizes_df\"]\n",
    "except NameError:\n",
    "    Gd, Gu, communities_list, node2community, sizes_df = _rebuild_from_edges()\n",
    "\n",
    "# ==== 2) Vista general (subgrafo con TOPN nodos por grado) ====\n",
    "TOPN = 300  # ajusta si quieres mÃ¡s/menos nodos\n",
    "deg = dict(Gu.degree())\n",
    "top_nodes = sorted(deg, key=deg.get, reverse=True)[:min(TOPN, Gu.number_of_nodes())]\n",
    "H = Gu.subgraph(top_nodes).copy()\n",
    "\n",
    "pos = nx.spring_layout(H, seed=42, iterations=50)\n",
    "node_colors = [node2community.get(n, -1) for n in H.nodes()]  # un entero por comunidad\n",
    "node_sizes = [10 + 2*deg.get(n, 1) for n in H.nodes()]        # tamaÃ±o proporcional al grado (suave)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw_networkx_nodes(H, pos, node_size=node_sizes, node_color=node_colors)\n",
    "nx.draw_networkx_edges(H, pos, alpha=0.25, width=0.6)\n",
    "plt.title(\"Comunidades â€” vista general (TOPN nodos por grado)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ==== 3) GrÃ¡fico con las 3 comunidades mÃ¡s grandes ====\n",
    "top3_ids = sizes_df.sort_values(\"size_nodes\", ascending=False).head(3)[\"community\"].tolist()\n",
    "nodes_top3 = [n for n,cid in node2community.items() if cid in top3_ids]\n",
    "H3_u = Gu.subgraph(nodes_top3).copy()\n",
    "H3_d = Gd.subgraph(nodes_top3).copy()\n",
    "\n",
    "# PageRank (dirigido; ponderado por 'weight') para destacar influencia en el grÃ¡fico\n",
    "pr = nx.pagerank(H3_d, alpha=0.85, weight=\"weight\", max_iter=100)\n",
    "pr_min, pr_max = (min(pr.values()), max(pr.values())) if pr else (0, 1)\n",
    "\n",
    "# tamaÃ±os escalados por PageRank (evitar casos extremos)\n",
    "def scale_size(x, xmin, xmax, smin=20, smax=300):\n",
    "    if xmax == xmin:\n",
    "        return (smin + smax) / 2.0\n",
    "    return smin + (smax - smin) * ((x - xmin) / (xmax - xmin))\n",
    "\n",
    "sizes_pr = [scale_size(pr.get(n, 0), pr_min, pr_max, smin=20, smax=300) for n in H3_u.nodes()]\n",
    "colors_3 = [node2community.get(n, -1) for n in H3_u.nodes()]\n",
    "\n",
    "pos3 = nx.spring_layout(H3_u, seed=123, iterations=60)\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw_networkx_nodes(H3_u, pos3, node_size=sizes_pr, node_color=colors_3)\n",
    "nx.draw_networkx_edges(H3_u, pos3, alpha=0.2, width=0.5)\n",
    "plt.title(\"Top 3 comunidades â€” nodos dimensionados por PageRank (subgrafo dirigido)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"IDs de las 3 comunidades mÃ¡s grandes:\", top3_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2.B â€” CaracterizaciÃ³n por comunidad: tamaÃ±o, interacciones, top usuarios y temas (aprox.)\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "# ==== 1) Tablas de interacciÃ³n por comunidad ====\n",
    "# - internal_edges_*: aristas con (u,v) dentro de la comunidad (conteo y suma de pesos)\n",
    "# - outgoing_*: desde la comunidad hacia fuera\n",
    "# - incoming_*: desde fuera hacia la comunidad\n",
    "# - density_undirected: densidad en subgrafo no dirigido\n",
    "\n",
    "rows = []\n",
    "node2community = dict(node2community)  # asegurar dict normal\n",
    "for cid, com in enumerate(communities_list):\n",
    "    com = set(com)\n",
    "    # Subgrafos\n",
    "    sub_u = Gu.subgraph(com).copy()\n",
    "\n",
    "    # Contadores\n",
    "    internal_count = 0\n",
    "    outgoing_count = 0\n",
    "    incoming_count = 0\n",
    "    internal_w = 0.0\n",
    "    outgoing_w = 0.0\n",
    "    incoming_w = 0.0\n",
    "\n",
    "    # Recorremos aristas dirigidas del grafo completo\n",
    "    for u, v, d in Gd.edges(data=True):\n",
    "        w = float(d.get(\"weight\", 1.0))\n",
    "        u_in, v_in = u in com, v in com\n",
    "        if u_in and v_in:\n",
    "            internal_count += 1\n",
    "            internal_w += w\n",
    "        elif u_in and not v_in:\n",
    "            outgoing_count += 1\n",
    "            outgoing_w += w\n",
    "        elif (not u_in) and v_in:\n",
    "            incoming_count += 1\n",
    "            incoming_w += w\n",
    "\n",
    "    # indegree local ponderado (quiÃ©n recibe mÃ¡s peso desde su propia comunidad)\n",
    "    indeg_local = {}\n",
    "    for u, v, d in Gd.in_edges(list(com), data=True):\n",
    "        if u in com and v in com:\n",
    "            indeg_local[v] = indeg_local.get(v, 0.0) + float(d.get(\"weight\", 1.0))\n",
    "    top5_local = sorted(indeg_local.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "    rows.append({\n",
    "        \"community\": cid,\n",
    "        \"size_nodes\": len(com),\n",
    "        \"density_undirected\": nx.density(sub_u),\n",
    "        \"internal_edges_count\": internal_count,\n",
    "        \"internal_edges_weight\": internal_w,\n",
    "        \"outgoing_edges_count\": outgoing_count,\n",
    "        \"outgoing_edges_weight\": outgoing_w,\n",
    "        \"incoming_edges_count\": incoming_count,\n",
    "        \"incoming_edges_weight\": incoming_w,\n",
    "        \"top5_users_by_local_indegree\": \"; \".join([f\"{u}:{w:.0f}\" for u,w in top5_local]),\n",
    "    })\n",
    "\n",
    "char_df = pd.DataFrame(rows).sort_values(\"size_nodes\", ascending=False).reset_index(drop=True)\n",
    "display(char_df.head(10))\n",
    "\n",
    "# ==== 2) Temas principales (aprox.) por comunidad usando tokens de la muestra limpia ====\n",
    "# Intentamos localizar tweets_clean_sample.csv:\n",
    "CANDS = [\n",
    "    \"tweets_clean_sample.csv\",\n",
    "    \"../tweets_clean_sample.csv\",\n",
    "    \"lab6ds/tweets_clean_sample.csv\",\n",
    "]\n",
    "clean_path = next((p for p in CANDS if os.path.exists(p)), None)\n",
    "if clean_path is None:\n",
    "    print(\"[Aviso] No se encontrÃ³ tweets_clean_sample.csv; omitiendo temas.\")\n",
    "else:\n",
    "    clean = pd.read_csv(clean_path)\n",
    "    # Normalizamos username para cruzar\n",
    "    clean[\"username_norm\"] = clean[\"username\"].astype(str).str.lower()\n",
    "    # Lista de tokens\n",
    "    def parse_list(s):\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "    clean[\"tokens_list\"] = clean[\"tokens_nostop\"].astype(str).apply(parse_list)\n",
    "\n",
    "    # Mapeo user -> comunidad\n",
    "    u2c = pd.DataFrame({\"user\": list(node2community.keys()),\n",
    "                        \"community\": list(node2community.values())})\n",
    "    u2c[\"user\"] = u2c[\"user\"].astype(str).str.lower()\n",
    "\n",
    "    merged = clean.merge(u2c, left_on=\"username_norm\", right_on=\"user\", how=\"left\").dropna(subset=[\"community\"])\n",
    "\n",
    "    # Top-3 comunidades por tamaÃ±o\n",
    "    top3_ids = char_df.head(3)[\"community\"].tolist()\n",
    "\n",
    "    topic_rows = []\n",
    "    for cid in top3_ids:\n",
    "        toks = []\n",
    "        for lst in merged.loc[merged[\"community\"]==cid, \"tokens_list\"]:\n",
    "            toks.extend([t for t in lst if isinstance(t, str)])\n",
    "        # Conteo y filtrito mÃ­nimo\n",
    "        cnt = Counter(toks)\n",
    "        common = [f\"{w}:{c}\" for w,c in cnt.most_common(15)]\n",
    "        topic_rows.append({\"community\": cid, \"top_terms\": \", \".join(common)})\n",
    "\n",
    "    topics_df = pd.DataFrame(topic_rows)\n",
    "    display(topics_df)\n",
    "\n",
    "# Guardamos resultados en variable para prÃ³ximos incisos\n",
    "RESULTS_62 = {\n",
    "    \"char_df\": char_df,\n",
    "    \"topics_df\": topics_df if clean_path is not None else None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230b02e",
   "metadata": {},
   "source": [
    "# 6.2. CaracterizaciÃ³n de comunidades\n",
    "\n",
    "## Resumen de resultados\n",
    "\n",
    "Se detectaron comunidades de distintos tamaÃ±os, siendo las tres mÃ¡s grandes:\n",
    "\n",
    "- Comunidad 3: 1,635 nodos  \n",
    "- Comunidad 38: 1,182 nodos  \n",
    "- Comunidad 33: 326 nodos  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. TamaÃ±o y densidad\n",
    "\n",
    "- Comunidad 3: Es la mÃ¡s grande (1,635 nodos) pero poco densa (0.0026), lo que muestra un grupo muy amplio pero disperso.  \n",
    "- Comunidad 38: TambiÃ©n es grande (1,182 nodos) y con una densidad un poco mayor (0.0038).  \n",
    "- Comunidad 33: Es mÃ¡s pequeÃ±a (326 nodos) pero mucho mÃ¡s densa (0.0269), lo que indica un grupo mÃ¡s cohesionado.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Interacciones internas y externas\n",
    "\n",
    "- Comunidad 3: Tiene 3,548 conexiones internas (peso total 8,664) y tambiÃ©n muchas conexiones externas, tanto entrantes como salientes (aprox. 1,400 cada una). Esto refleja un grupo activo y con muchas interacciones hacia afuera.  \n",
    "- Comunidad 38: Presenta 2,686 conexiones internas (peso total 5,830), pero destaca por recibir un gran nÃºmero de conexiones externas (2,389 con peso 4,410). Esto sugiere que es un grupo que recibe bastante atenciÃ³n de otros.  \n",
    "- Comunidad 33: A pesar de ser mÃ¡s pequeÃ±a, tiene una alta proporciÃ³n de conexiones internas (1,443 con peso 2,500) frente a las externas, lo que confirma que es un grupo muy cohesionado.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Usuarios mÃ¡s influyentes\n",
    "\n",
    "- Comunidad 3: Destacan cuentas como *traficogt* (4,193), *prensacomunitar* y *lahoragt*, principalmente medios de comunicaciÃ³n y cuentas relacionadas al trÃ¡fico.  \n",
    "- Comunidad 38: Sobresalen *barevalodeleon* (2,691), *ubaldomacu* y *santipalomino*, en su mayorÃ­a lÃ­deres de opiniÃ³n y cuentas personales.  \n",
    "- Comunidad 33: Los mÃ¡s influyentes son *guatemalagob* (480), *fjimenezmingob* y *diariodeca*, relacionados con instituciones oficiales y el gobierno.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Temas principales de conversaciÃ³n\n",
    "\n",
    "- Comunidad 3: Predominan palabras como *traficogt*, *gt* y *quorumgt*, con un enfoque en trÃ¡fico y noticias locales.  \n",
    "- Comunidad 38: Aparecen tÃ©rminos como *gÃ©nero*, *investigaciÃ³n* y *fiscal*, vinculados a justicia, gÃ©nero y temas sociales o polÃ­ticos.  \n",
    "- Comunidad 33: Se observan tÃ©rminos como *guatemalagob*, *lionelgaliano* y *landivarianos*, relacionados con el gobierno y la polÃ­tica institucional.  \n",
    "\n",
    "---\n",
    "\n",
    "## InterpretaciÃ³n general\n",
    "\n",
    "- La comunidad mÃ¡s grande (3) reÃºne principalmente medios y cuentas sobre trÃ¡fico y noticias cotidianas.  \n",
    "- La comunidad 38 concentra periodistas y lÃ­deres de opiniÃ³n, conectÃ¡ndose con muchas otras comunidades de la red.  \n",
    "- La comunidad 33 corresponde a un bloque institucional mÃ¡s cerrado, asociado a cuentas oficiales y al discurso gubernamental.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb9142",
   "metadata": {},
   "source": [
    "# Inciso 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357afe3",
   "metadata": {},
   "source": [
    "# 7.1. IdentificaciÃ³n de usuarios influyentes \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Centralidad de grado\n",
    "\n",
    "- **QuÃ© mide:** cuÃ¡ntas conexiones directas tiene un usuario.  \n",
    "- **CÃ³mo interpretarlo:**  \n",
    "  - *In-degree*: cuÃ¡ntas veces mencionan o retuitean a un usuario â†’ indica recepciÃ³n de atenciÃ³n.  \n",
    "  - *Out-degree*: cuÃ¡ntas veces un usuario menciona a otros â†’ indica su nivel de actividad.  \n",
    "- **CÃ³mo lo usamos:**  \n",
    "  - Se utilizan las versiones ponderadas (que consideran la frecuencia de las interacciones).  \n",
    "  - Como resumen, se puede sumar ambos valores: `grado total = in-degree + out-degree`.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Centralidad de intermediaciÃ³n \n",
    "\n",
    "- **QuÃ© mide:** cuÃ¡ntas rutas de comunicaciÃ³n â€œpasan porâ€ un usuario.  \n",
    "- **CÃ³mo interpretarlo:** ayuda a identificar cuentas que actÃºan como puentes entre comunidades o subgrupos distintos.  \n",
    "- **CÃ³mo lo usamos:**  \n",
    "  - Para reducir el costo computacional, si no existe un cÃ¡lculo previo, se utiliza una aproximaciÃ³n con muestreo sobre la red no dirigida.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Centralidad de cercanÃ­a\n",
    "\n",
    "- **QuÃ© mide:** quÃ© tan cerca estÃ¡ un usuario de todos los demÃ¡s en la red (en promedio).  \n",
    "- **CÃ³mo interpretarlo:** los usuarios con alta cercanÃ­a pueden llegar mÃ¡s rÃ¡pido al resto de la red, por lo que son importantes para la difusiÃ³n de informaciÃ³n.  \n",
    "- **CÃ³mo lo usamos:**  \n",
    "  - Se calcula sobre el componente mÃ¡s grande de la red no dirigida.  \n",
    "  - Para tener en cuenta los pesos (mÃ¡s interacciÃ³n = mayor cercanÃ­a), la distancia se define como `1 / peso`.  \n",
    "\n",
    "---\n",
    "\n",
    "## Notas de implementaciÃ³n\n",
    "\n",
    "- Se parte de los grafos construidos en el paso 6.1. Si no estÃ¡n disponibles, se reconstruyen a partir del archivo `edge_weights.csv`.  \n",
    "- Se calculan:  \n",
    "  - Grado en el grafo dirigido (conteos y pesos).  \n",
    "  - IntermediaciÃ³n aproximada en el grafo no dirigido.  \n",
    "  - CercanÃ­a en el componente mÃ¡s grande del grafo no dirigido, considerando los pesos.  \n",
    "- Se mostrarÃ¡n los **15 usuarios principales** en cada medida.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9311d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 â€” Influencers y nodos clave con tres centralidades:\n",
    "#   - Grado (dirigido, ponderado)\n",
    "#   - IntermediaciÃ³n/Betweenness (no dirigido, con distancia = 1/weight)\n",
    "#   - CercanÃ­a/Closeness (no dirigido, LCC, con distancia = 1/weight)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ============== Utilidades ==============\n",
    "def _load_graphs_from_61_or_rebuild():\n",
    "    \"\"\"Intentar tomar Gd/Gu de RESULTS_61; si no existen, reconstruir desde edge_weights.csv.\"\"\"\n",
    "    try:\n",
    "        Gd = RESULTS_61[\"G_directed\"]\n",
    "        Gu = RESULTS_61[\"G_undirected\"]\n",
    "        return Gd, Gu\n",
    "    except NameError:\n",
    "        pass  # no existe RESULTS_61\n",
    "\n",
    "    # Reconstruir desde edge_weights.csv\n",
    "    CANDIDATES = [\n",
    "        \"net_outputs/edge_weights.csv\",\n",
    "        \"../net_outputs/edge_weights.csv\",\n",
    "        \"lab6ds/net_outputs/edge_weights.csv\",\n",
    "        \"./edge_weights.csv\",\n",
    "    ]\n",
    "    edges_path = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
    "    if edges_path is None:\n",
    "        raise FileNotFoundError(\"No se encontrÃ³ edge_weights.csv; ajusta rutas en CANDIDATES.\")\n",
    "\n",
    "    edges = pd.read_csv(edges_path)\n",
    "    edges.columns = [c.strip().lower() for c in edges.columns]\n",
    "    col_src = \"src\" if \"src\" in edges.columns else (\"source\" if \"source\" in edges.columns else None)\n",
    "    col_dst = \"dst\" if \"dst\" in edges.columns else (\"target\" if \"target\" in edges.columns else None)\n",
    "    col_w  = \"weight\" if \"weight\" in edges.columns else None\n",
    "    assert col_src and col_dst and col_w, f\"Columnas esperadas tipo src/dst/weight; reales: {edges.columns.tolist()}\"\n",
    "\n",
    "    Gd = nx.DiGraph()\n",
    "    for r in edges.itertuples(index=False):\n",
    "        Gd.add_edge(getattr(r, col_src), getattr(r, col_dst), weight=float(getattr(r, col_w)))\n",
    "\n",
    "    # no dirigido (sumando pesos)\n",
    "    Gu = nx.Graph()\n",
    "    for u, v, d in Gd.edges(data=True):\n",
    "        w = float(d.get(\"weight\", 1.0))\n",
    "        if Gu.has_edge(u, v):\n",
    "            Gu[u][v][\"weight\"] += w\n",
    "        else:\n",
    "            Gu.add_edge(u, v, weight=w)\n",
    "    return Gd, Gu\n",
    "\n",
    "def _ensure_distance_attr(Gu):\n",
    "    \"\"\"Crear atributo 'distance' = 1/weight para usar en betweenness/closeness.\"\"\"\n",
    "    for u, v, d in Gu.edges(data=True):\n",
    "        w = float(d.get(\"weight\", 1.0))\n",
    "        d[\"distance\"] = 1.0 / w if w > 0 else 1e9\n",
    "    return Gu\n",
    "\n",
    "# ============== 1) Cargar grafos y preparar distancia ==============\n",
    "Gd, Gu = _load_graphs_from_61_or_rebuild()\n",
    "Gu = _ensure_distance_attr(Gu)\n",
    "\n",
    "print(f\"G_directed: {Gd.number_of_nodes()} nodos, {Gd.number_of_edges()} aristas\")\n",
    "print(f\"G_undirected: {Gu.number_of_nodes()} nodos, {Gu.number_of_edges()} aristas\")\n",
    "\n",
    "# ============== 2) Centralidad de grado (dirigido) ==============\n",
    "in_deg_cnt  = dict(Gd.in_degree())                       # sin peso\n",
    "out_deg_cnt = dict(Gd.out_degree())\n",
    "in_deg_w    = dict(Gd.in_degree(weight=\"weight\"))        # ponderado por 'weight'\n",
    "out_deg_w   = dict(Gd.out_degree(weight=\"weight\"))\n",
    "\n",
    "deg_df = pd.DataFrame({\n",
    "    \"user\": list(Gd.nodes())\n",
    "})\n",
    "deg_df[\"indegree\"]   = deg_df[\"user\"].map(in_deg_cnt).fillna(0).astype(int)\n",
    "deg_df[\"outdegree\"]  = deg_df[\"user\"].map(out_deg_cnt).fillna(0).astype(int)\n",
    "deg_df[\"indegree_w\"] = deg_df[\"user\"].map(in_deg_w).fillna(0).astype(float)\n",
    "deg_df[\"outdegree_w\"]= deg_df[\"user\"].map(out_deg_w).fillna(0).astype(float)\n",
    "deg_df[\"degree_w\"]   = deg_df[\"indegree_w\"] + deg_df[\"outdegree_w\"]\n",
    "\n",
    "top_degree_in   = deg_df.sort_values([\"indegree_w\",\"indegree\"], ascending=False).head(15)\n",
    "top_degree_sumw = deg_df.sort_values([\"degree_w\",\"indegree_w\"], ascending=False).head(15)\n",
    "\n",
    "print(\"\\nTop-15 por IN-degree ponderado (menciones/RT recibidos):\")\n",
    "display(top_degree_in[[\"user\",\"indegree_w\",\"indegree\",\"outdegree_w\",\"outdegree\"]])\n",
    "\n",
    "print(\"\\nTop-15 por Degree ponderado total (indegree_w + outdegree_w):\")\n",
    "display(top_degree_sumw[[\"user\",\"degree_w\",\"indegree_w\",\"outdegree_w\",\"indegree\",\"outdegree\"]])\n",
    "\n",
    "# ============== 3) Betweenness (no dirigido, con distancia) ==============\n",
    "# Intentar leer de archivo si existe para ahorrar tiempo\n",
    "precomp_path = None\n",
    "for p in [\"net_outputs/node_metrics.csv\", \"../net_outputs/node_metrics.csv\", \"lab6ds/net_outputs/node_metrics.csv\"]:\n",
    "    if os.path.exists(p):\n",
    "        precomp_path = p\n",
    "        break\n",
    "\n",
    "if precomp_path is not None:\n",
    "    node_metrics = pd.read_csv(precomp_path)\n",
    "    if \"betweenness\" in node_metrics.columns:\n",
    "        between = dict(zip(node_metrics[\"user\"], node_metrics[\"betweenness\"]))\n",
    "        print(\"\\n[Betweenness] Usando valores precomputados de:\", precomp_path)\n",
    "    else:\n",
    "        between = None\n",
    "else:\n",
    "    between = None\n",
    "\n",
    "if between is None:\n",
    "    # AproximaciÃ³n por muestreo para hacerlo rÃ¡pido en 5k nodos\n",
    "    # Puedes subir/bajar k segÃºn tu equipo (128â€“512 es razonable)\n",
    "    k_sample = 256\n",
    "    print(f\"\\n[Betweenness] Calculando aproximado con k={k_sample} (puede tardar unos minutos)...\")\n",
    "    between = nx.betweenness_centrality(Gu, k=k_sample, weight=\"distance\", normalized=True, seed=42)\n",
    "\n",
    "bet_df = pd.DataFrame({\"user\": list(between.keys()), \"betweenness\": list(between.values())})\n",
    "top_between = bet_df.sort_values(\"betweenness\", ascending=False).head(15)\n",
    "\n",
    "print(\"\\nTop-15 por Betweenness (puentes entre comunidades):\")\n",
    "display(top_between)\n",
    "\n",
    "# ============== 4) Closeness (no dirigido, LCC, con distancia) ==============\n",
    "# Tomamos sÃ³lo el mayor componente conexo para evitar valores triviales en nodos aislados.\n",
    "if nx.number_connected_components(Gu) > 1:\n",
    "    lcc_nodes = max(nx.connected_components(Gu), key=len)\n",
    "else:\n",
    "    lcc_nodes = set(Gu.nodes())\n",
    "\n",
    "H_lcc = Gu.subgraph(lcc_nodes).copy()\n",
    "close = nx.closeness_centrality(H_lcc, distance=\"distance\", wf_improved=True)\n",
    "close_df = pd.DataFrame({\"user\": list(close.keys()), \"closeness\": list(close.values())})\n",
    "top_close = close_df.sort_values(\"closeness\", ascending=False).head(15)\n",
    "\n",
    "print(\"\\nTop-15 por Closeness (alcance con menos saltos ponderados):\")\n",
    "display(top_close)\n",
    "\n",
    "# ============== 5) Resumen integrable ==============\n",
    "# Unimos (left join) para que puedas exportar si quieres usar en 7.2/7.3\n",
    "summary = (deg_df.merge(bet_df, on=\"user\", how=\"left\")\n",
    "                 .merge(close_df, on=\"user\", how=\"left\"))\n",
    "summary = summary.fillna({\"betweenness\": 0.0})  # por si betweenness aprox omitiÃ³ algÃºn nodo\n",
    "print(\"\\nResumen (cabezal):\")\n",
    "display(summary.head())\n",
    "\n",
    "# (Opcional) guardar en variable global para siguientes incisos\n",
    "RESULTS_71 = {\n",
    "    \"degree_table\": deg_df,\n",
    "    \"betweenness_table\": bet_df,\n",
    "    \"closeness_table\": close_df,\n",
    "    \"summary\": summary,\n",
    "    \"top_degree_in\": top_degree_in,\n",
    "    \"top_degree_sumw\": top_degree_sumw,\n",
    "    \"top_between\": top_between,\n",
    "    \"top_close\": top_close,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484be3c",
   "metadata": {},
   "source": [
    "# 7.1. IdentificaciÃ³n de usuarios influyentes (centralidades)\n",
    "\n",
    "## RevisiÃ³n de resultados\n",
    "\n",
    "- El tamaÃ±o del grafo es consistente con lo observado antes: 5,213 nodos.  \n",
    "- Al convertirlo en no dirigido, las aristas bajan de 19,476 a 19,294 lo cual es esperado.  \n",
    "\n",
    "### AtenciÃ³n recibida (in-degree ponderado)  \n",
    "- Los usuarios que mÃ¡s atenciÃ³n reciben son **barevalodeleon** y **traficogt**, con gran diferencia respecto al resto.  \n",
    "- DespuÃ©s aparecen instituciones y medios como **guatemalagob**, **mpguatemala**, **prensacomunitar** y **lahoragt**.  \n",
    "\n",
    "### Actividad global (degree ponderado total)  \n",
    "- AdemÃ¡s de los anteriores, destacan cuentas con mucha actividad hacia afuera como **batallonjalapa** y **mildred_gaitan**.  \n",
    "- Estas cuentas tienen gran alcance por la cantidad de menciones que hacen, aunque no siempre se traducen en influencia real.  \n",
    "\n",
    "### Puentes entre comunidades (betweenness)  \n",
    "- De nuevo sobresalen **barevalodeleon** y **traficogt**, que actÃºan como conectores entre distintas comunidades.  \n",
    "- TambiÃ©n aparecen cuentas institucionales y de medios que cumplen un rol de puente entre grupos.  \n",
    "\n",
    "### CercanÃ­a en la red (closeness)  \n",
    "- Los valores mayores a 1 son normales aquÃ­ porque se definiÃ³ la distancia como inversa al peso.  \n",
    "- En este indicador vuelven a sobresalir **barevalodeleon** y **traficogt**, junto con los broadcasters como **batallonjalapa** y **mildred_gaitan**, debido a su alta conectividad.  \n",
    "\n",
    "---\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "- **Influenciadores por atenciÃ³n recibida:** barevalodeleon, traficogt, guatemalagob, ubaldomacu, mpguatemala.  \n",
    "- **Puentes entre comunidades:** barevalodeleon, traficogt y nodos institucionales como guatemalagob, mpguatemala, congresoguate.  \n",
    "- **Cuentas centrales por cercanÃ­a:** ademÃ¡s de los lÃ­deres anteriores, aparecen broadcasters como batallonjalapa y mildred_gaitan.  \n",
    "\n",
    "---\n",
    "\n",
    "## Advertencia metodolÃ³gica\n",
    "\n",
    "Un alto nivel de actividad hacia afuera (out-degree) puede inflar las mÃ©tricas de grado total y cercanÃ­a sin reflejar verdadera influencia.  \n",
    "Para interpretar influencia de manera mÃ¡s confiable:  \n",
    "- Priorizar **in-degree** (atenciÃ³n recibida).  \n",
    "- Considerar **betweenness** (puentes entre comunidades).  \n",
    "- Usar **closeness** solo como una seÃ±al adicional de accesibilidad dentro de la red.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 â€” Tablas con comunidad y ranking integrado de influencia\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Recuperar insumos de 6.1 y 7.1\n",
    "Gu = RESULTS_61[\"G_undirected\"]\n",
    "node2com = RESULTS_61[\"node2community\"]\n",
    "\n",
    "deg_df      = RESULTS_71[\"degree_table\"].copy()\n",
    "betweenness = RESULTS_71[\"betweenness_table\"].copy()\n",
    "closeness   = RESULTS_71[\"closeness_table\"].copy()\n",
    "\n",
    "# AÃ±adir comunidad\n",
    "deg_df[\"community\"] = deg_df[\"user\"].map(lambda u: node2com.get(u, np.nan))\n",
    "betweenness[\"community\"] = betweenness[\"user\"].map(lambda u: node2com.get(u, np.nan))\n",
    "closeness[\"community\"]   = closeness[\"user\"].map(lambda u: node2com.get(u, np.nan))\n",
    "\n",
    "# Top-15 por mÃ©trica (con comunidad)\n",
    "top_in = (deg_df.sort_values([\"indegree_w\",\"indegree\"], ascending=False)\n",
    "                .loc[:, [\"user\",\"community\",\"indegree_w\",\"indegree\",\"outdegree_w\",\"outdegree\"]]\n",
    "                .head(15))\n",
    "\n",
    "top_degree_total = (deg_df.sort_values([\"degree_w\",\"indegree_w\"], ascending=False)\n",
    "                          .loc[:, [\"user\",\"community\",\"degree_w\",\"indegree_w\",\"outdegree_w\",\"indegree\",\"outdegree\"]]\n",
    "                          .head(15))\n",
    "\n",
    "top_between = (betweenness.sort_values(\"betweenness\", ascending=False)\n",
    "                          .loc[:, [\"user\",\"community\",\"betweenness\"]]\n",
    "                          .head(15))\n",
    "\n",
    "top_close = (closeness.sort_values(\"closeness\", ascending=False)\n",
    "                        .loc[:, [\"user\",\"community\",\"closeness\"]]\n",
    "                        .head(15))\n",
    "\n",
    "print(\"Top-15 â€” IN-degree ponderado (atenciÃ³n recibida)\")\n",
    "display(top_in)\n",
    "\n",
    "print(\"Top-15 â€” Degree ponderado total\")\n",
    "display(top_degree_total)\n",
    "\n",
    "print(\"Top-15 â€” Betweenness (puentes)\")\n",
    "display(top_between)\n",
    "\n",
    "print(\"Top-15 â€” Closeness (alcance ponderado)\")\n",
    "display(top_close)\n",
    "\n",
    "# ====== Ranking integrado (Borda simple sobre percentiles) ======\n",
    "# Normalizamos cada mÃ©trica a percentil [0,1] (mayor = mejor) y promediamos.\n",
    "df = (deg_df\n",
    "      .merge(betweenness, on=[\"user\",\"community\"], how=\"left\", suffixes=(\"\",\"\"))\n",
    "      .merge(closeness, on=[\"user\",\"community\"],  how=\"left\", suffixes=(\"\",\"\"))\n",
    "      .fillna({\"betweenness\":0.0, \"closeness\":0.0})\n",
    ")\n",
    "\n",
    "def pct_rank(s):\n",
    "    # percentil rank robusto\n",
    "    return s.rank(pct=True)\n",
    "\n",
    "df[\"score_in\"]   = pct_rank(df[\"indegree_w\"])      # atenciÃ³n recibida\n",
    "df[\"score_bet\"]  = pct_rank(df[\"betweenness\"])     # puente\n",
    "df[\"score_close\"]= pct_rank(df[\"closeness\"])       # accesibilidad\n",
    "# Peso mayor a atenciÃ³n y puentes; closeness como seÃ±al secundaria\n",
    "df[\"influence_score\"] = 0.4*df[\"score_in\"] + 0.4*df[\"score_bet\"] + 0.2*df[\"score_close\"]\n",
    "\n",
    "rank_integrado = (df.sort_values(\"influence_score\", ascending=False)\n",
    "                    .loc[:, [\"user\",\"community\",\n",
    "                             \"influence_score\",\n",
    "                             \"indegree_w\",\"betweenness\",\"closeness\",\n",
    "                             \"indegree\",\"outdegree\",\"outdegree_w\"]]\n",
    "                    .head(15))\n",
    "\n",
    "print(\"Top-15 â€” Ranking integrado (0â€“1): pondera atenciÃ³n (40%), puentes (40%), closeness (20%)\")\n",
    "display(rank_integrado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b0c40",
   "metadata": {},
   "source": [
    "# Inciso 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a945bc",
   "metadata": {},
   "source": [
    "# 8.1. DetecciÃ³n y anÃ¡lisis de grupos aislados\n",
    "\n",
    "### 1. Componentes aislados\n",
    "- En el grafo dirigido se usan los **Weakly Connected Components (WCC)**, que ignoran la direcciÃ³n de los enlaces, para encontrar islas desconectadas del resto.  \n",
    "- En el grafo no dirigido se usan los **Connected Components (CC)** para confirmar tamaÃ±o y densidad.  \n",
    "- Se reporta:  \n",
    "  - NÃºmero de componentes  \n",
    "  - Nodos aislados (componentes de tamaÃ±o 1)  \n",
    "  - Subredes pequeÃ±as (ejemplo: con 30 nodos o menos)  \n",
    "\n",
    "### 2. Nichos con baja interacciÃ³n externa\n",
    "- A partir de las comunidades Louvain identificadas en el paso 6, se calcula:  \n",
    "  - **Ratio externo = (peso de interacciones hacia afuera) / (peso total interno + externo)**  \n",
    "- Se detectan nichos con ratio externo bajo (ejemplo: menor a 0.15) y tamaÃ±o suficiente (para evitar casos triviales).  \n",
    "- Para estos nichos se muestran:  \n",
    "  - Usuarios principales segÃºn menciones recibidas (indegree local)  \n",
    "  - Palabras frecuentes en sus publicaciones, para interpretar la temÃ¡tica  \n",
    "\n",
    "---\n",
    "\n",
    "## Notas\n",
    "- Los **componentes aislados** no tienen conexiones hacia el resto por definiciÃ³n.  \n",
    "- Los **nichos** sÃ­ estÃ¡n dentro del componente principal, pero muestran poca interacciÃ³n con otras comunidades, lo que refleja un bajo nivel de conexiÃ³n externa.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823a69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
