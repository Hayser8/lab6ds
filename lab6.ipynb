{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76955651",
   "metadata": {},
   "source": [
    "## **Laboratorio 6**\n",
    "- Joaquín Campos - 22155\n",
    "- Sofía García - 22210\n",
    "- Julio García Salas - 22076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f1bdb",
   "metadata": {},
   "source": [
    "## **Inciso 1 y 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba436c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasos 1–2 (con detección de codificación y JSON doble)\n",
    "# - Lee traficogt.txt y tioberny.txt (UTF-16 / UTF-8, etc.)\n",
    "# - Parseo robusto: JSON array, JSONL, JSON doblemente codificado, o texto plano\n",
    "# - Normaliza a DataFrame con campos clave para siguientes pasos\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# -------------------- Localización de archivos --------------------\n",
    "def resolve_paths() -> List[Path]:\n",
    "    local = [Path(\"traficogt.txt\"), Path(\"tioberny.txt\")]\n",
    "    mnt = [Path(\"/mnt/data/traficogt.txt\"), Path(\"/mnt/data/tioberny.txt\")]\n",
    "    out = []\n",
    "    for pl, pm in zip(local, mnt):\n",
    "        out.append(pl if pl.exists() else (pm if pm.exists() else pl))\n",
    "    return out\n",
    "\n",
    "DATA_PATHS = resolve_paths()\n",
    "OUT_PARQUET = Path(\"tweets_raw.parquet\")\n",
    "OUT_SAMPLE_CSV = Path(\"tweets_raw_sample.csv\")\n",
    "\n",
    "# -------------------- Utilidades de lectura/decodificación --------------------\n",
    "CANDIDATE_ENCODINGS = [\"utf-8\", \"utf-16\", \"utf-16-le\", \"utf-16-be\", \"latin-1\"]\n",
    "\n",
    "def read_text_auto(path: Path) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Lee bytes y prueba varias codificaciones. Si la decodificación produce muchos \\x00,\n",
    "    intenta con UTF-16 variantes. Devuelve (texto, encoding_usada).\n",
    "    \"\"\"\n",
    "    data = path.read_bytes()\n",
    "    last_err = None\n",
    "    # Heurística rápida: si hay muchos 0x00, probablemente es UTF-16\n",
    "    zero_ratio = data.count(0) / max(1, len(data))\n",
    "    preferred = ([\"utf-16\"] + CANDIDATE_ENCODINGS) if zero_ratio > 0.01 else CANDIDATE_ENCODINGS\n",
    "    tried = []\n",
    "    for enc in preferred:\n",
    "        try:\n",
    "            txt = data.decode(enc, errors=\"strict\")\n",
    "            # si aún hay muchísimos NUL tras decodificar, seguimos probando\n",
    "            if txt.count(\"\\x00\") > 10:\n",
    "                tried.append((enc, \"nul_after_decode\"))\n",
    "                continue\n",
    "            return txt, enc\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            tried.append((enc, \"error\"))\n",
    "            continue\n",
    "    # fallback permisivo\n",
    "    return data.decode(\"utf-8\", errors=\"ignore\"), \"utf-8(ignore)\"\n",
    "\n",
    "# -------------------- Utilidades de parseo JSON --------------------\n",
    "def json_maybe_twice(s: str):\n",
    "    \"\"\"\n",
    "    Intenta json.loads; si devuelve un string que comienza con '{' o '[',\n",
    "    intenta decodificar una segunda vez (JSON doblemente codificado).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, str):\n",
    "            st = obj.strip()\n",
    "            if (st.startswith(\"{\") and st.endswith(\"}\")) or (st.startswith(\"[\") and st.endswith(\"]\")):\n",
    "                try:\n",
    "                    return json.loads(st)\n",
    "                except Exception:\n",
    "                    return obj\n",
    "        return obj\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def read_any_json_whole(raw_stripped: str):\n",
    "    \"\"\"Parsea todo el contenido como JSON array/obj (con soporte a doble carga).\"\"\"\n",
    "    obj = json_maybe_twice(raw_stripped)\n",
    "    if obj is None:\n",
    "        return None\n",
    "    if isinstance(obj, list):\n",
    "        return [x for x in obj if isinstance(x, dict)]\n",
    "    if isinstance(obj, dict):\n",
    "        if \"tweets\" in obj and isinstance(obj[\"tweets\"], list):\n",
    "            return [x for x in obj[\"tweets\"] if isinstance(x, dict)]\n",
    "        if \"data\" in obj and isinstance(obj[\"data\"], list):\n",
    "            return [x for x in obj[\"data\"] if isinstance(x, dict)]\n",
    "        return [obj]\n",
    "    return None\n",
    "\n",
    "def read_jsonl_lines(raw: str):\n",
    "    \"\"\"JSONL: una entrada por línea; soporta doble decodificación.\"\"\"\n",
    "    recs = []\n",
    "    ok = 0\n",
    "    for line in raw.splitlines():\n",
    "        s = line.strip().rstrip(\",\")\n",
    "        if not s:\n",
    "            continue\n",
    "        obj = json_maybe_twice(s)\n",
    "        if isinstance(obj, dict):\n",
    "            recs.append(obj); ok += 1\n",
    "    return recs, ok\n",
    "\n",
    "def extract_json_from_line(line: str):\n",
    "    \"\"\"Intenta rescatar el primer bloque {...} o [...] de una línea y decodificarlo.\"\"\"\n",
    "    s = line.strip()\n",
    "    start_obj = s.find(\"{\"); end_obj = s.rfind(\"}\")\n",
    "    start_arr = s.find(\"[\"); end_arr = s.rfind(\"]\")\n",
    "    cand = None\n",
    "    if start_obj != -1 and end_obj > start_obj:\n",
    "        cand = s[start_obj:end_obj+1]\n",
    "    elif start_arr != -1 and end_arr > start_arr:\n",
    "        cand = s[start_arr:end_arr+1]\n",
    "    if cand:\n",
    "        return json_maybe_twice(cand)\n",
    "    return None\n",
    "\n",
    "# -------------------- Normalización de registros --------------------\n",
    "def norm_username(u: Optional[str]) -> Optional[str]:\n",
    "    if not u:\n",
    "        return u\n",
    "    u = u.strip()\n",
    "    if u.startswith(\"@\"):\n",
    "        u = u[1:]\n",
    "    return u.lower()\n",
    "\n",
    "def extract_list_usernames(mentioned: Any) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    if isinstance(mentioned, list):\n",
    "        for m in mentioned:\n",
    "            if isinstance(m, dict):\n",
    "                un = m.get(\"username\") or m.get(\"screen_name\") or m.get(\"name\")\n",
    "                if un: out.append(norm_username(un))\n",
    "            elif isinstance(m, str):\n",
    "                out.append(norm_username(m))\n",
    "    return [x for x in out if x]\n",
    "\n",
    "def hashtags_to_list(h: Any) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    if isinstance(h, list):\n",
    "        for item in h:\n",
    "            if isinstance(item, str):\n",
    "                out.append(item.lstrip(\"#\").lower())\n",
    "            elif isinstance(item, dict):\n",
    "                txt = item.get(\"text\") or item.get(\"tag\")\n",
    "                if txt: out.append(str(txt).lstrip(\"#\").lower())\n",
    "    return out\n",
    "\n",
    "def get_text(rec: Dict[str, Any]) -> Optional[str]:\n",
    "    for k in (\"rawContent\", \"full_text\", \"text\"):\n",
    "        val = rec.get(k)\n",
    "        if isinstance(val, str) and val.strip():\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "def get_user_obj(rec: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    u = rec.get(\"user\")\n",
    "    return u if isinstance(u, dict) else None\n",
    "\n",
    "def safe_int(x):\n",
    "    try: return int(x)\n",
    "    except Exception: return None\n",
    "\n",
    "MENTION_RE = re.compile(r\"@([A-Za-z0-9_]{1,15})\")\n",
    "HASHTAG_RE = re.compile(r\"#([A-Za-z0-9_]+)\")\n",
    "\n",
    "def rows_from_json(records: List[Dict[str, Any]], source_file: str) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for r in records:\n",
    "        u = get_user_obj(r)\n",
    "        uname = u.get(\"username\") or u.get(\"screen_name\") or u.get(\"name\") if u else None\n",
    "        uid = u.get(\"id\") or u.get(\"id_str\") if u else None\n",
    "\n",
    "        # Mentions de diferentes formatos\n",
    "        mentions = []\n",
    "        if \"mentionedUsers\" in r:\n",
    "            mentions = extract_list_usernames(r.get(\"mentionedUsers\"))\n",
    "        elif \"entities\" in r and isinstance(r[\"entities\"], dict):\n",
    "            mentions = extract_list_usernames(r[\"entities\"].get(\"user_mentions\"))\n",
    "\n",
    "        # Hashtags\n",
    "        if \"hashtags\" in r:\n",
    "            tags = hashtags_to_list(r.get(\"hashtags\"))\n",
    "        elif \"entities\" in r and isinstance(r[\"entities\"], dict):\n",
    "            tags = hashtags_to_list(r[\"entities\"].get(\"hashtags\"))\n",
    "        else:\n",
    "            tags = []\n",
    "\n",
    "        # RT / Quote\n",
    "        rt = r.get(\"retweetedTweet\") or r.get(\"retweeted_status\")\n",
    "        qt = r.get(\"quotedTweet\") or r.get(\"quoted_status\")\n",
    "        is_rt = rt is not None\n",
    "        is_qt = qt is not None\n",
    "        rt_user = norm_username((rt or {}).get(\"user\", {}).get(\"username\") if isinstance(rt, dict) else None)\n",
    "        qt_user = norm_username((qt or {}).get(\"user\", {}).get(\"username\") if isinstance(qt, dict) else None)\n",
    "\n",
    "        # Reply\n",
    "        in_reply_to_user = r.get(\"inReplyToUser\")\n",
    "        reply_to_username = None\n",
    "        if isinstance(in_reply_to_user, dict):\n",
    "            reply_to_username = norm_username(in_reply_to_user.get(\"username\"))\n",
    "        if not reply_to_username and r.get(\"in_reply_to_screen_name\"):\n",
    "            reply_to_username = norm_username(r.get(\"in_reply_to_screen_name\"))\n",
    "\n",
    "        # Métricas\n",
    "        like_count  = safe_int(r.get(\"likeCount\")   or r.get(\"favorite_count\"))\n",
    "        rt_count    = safe_int(r.get(\"retweetCount\")or r.get(\"retweet_count\"))\n",
    "        reply_count = safe_int(r.get(\"replyCount\")  or r.get(\"reply_count\"))\n",
    "        quote_count = safe_int(r.get(\"quoteCount\")  or r.get(\"quote_count\"))\n",
    "        view_count  = safe_int(r.get(\"viewCount\")   or r.get(\"views\"))\n",
    "\n",
    "        # Fecha\n",
    "        date_raw = r.get(\"date\") or r.get(\"created_at\")\n",
    "        try:    date_parsed = pd.to_datetime(date_raw)\n",
    "        except: date_parsed = pd.NaT\n",
    "\n",
    "        rows.append({\n",
    "            \"source_file\": source_file,\n",
    "            \"tweet_id\": r.get(\"id\") or r.get(\"id_str\"),\n",
    "            \"date\": date_parsed,\n",
    "            \"lang\": r.get(\"lang\"),\n",
    "            \"username\": norm_username(uname),\n",
    "            \"user_id\": uid,\n",
    "            \"text\": get_text(r),\n",
    "            \"mentions\": mentions,\n",
    "            \"hashtags\": tags,\n",
    "            \"is_retweet\": bool(is_rt),\n",
    "            \"is_quote\": bool(is_qt),\n",
    "            \"retweeted_user\": rt_user,\n",
    "            \"quoted_user\": qt_user,\n",
    "            \"reply_to_user\": reply_to_username,\n",
    "            \"in_reply_to_tweet_id\": r.get(\"inReplyToTweetId\") or r.get(\"in_reply_to_status_id_str\") or r.get(\"in_reply_to_status_id\"),\n",
    "            \"like_count\": like_count,\n",
    "            \"retweet_count\": rt_count,\n",
    "            \"reply_count\": reply_count,\n",
    "            \"quote_count\": quote_count,\n",
    "            \"view_count\": view_count,\n",
    "            \"raw_record\": r,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def rows_from_plaintext(lines: List[str], source_file: str) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    for i, raw in enumerate(lines, start=1):\n",
    "        txt = raw.strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        # RT al inicio\n",
    "        is_rt = False; rt_user = None\n",
    "        m_rt = re.match(r\"^\\s*RT\\s+@([A-Za-z0-9_]{1,15})\\b\", txt)\n",
    "        if m_rt:\n",
    "            is_rt = True; rt_user = m_rt.group(1).lower()\n",
    "        # menciones / hashtags\n",
    "        mentions = [m.lower() for m in MENTION_RE.findall(txt)]\n",
    "        hashtags = [h.lower() for h in HASHTAG_RE.findall(txt)]\n",
    "        # reply si inicia con @\n",
    "        reply_to_user = None\n",
    "        m_reply = re.match(r\"^\\s*@([A-Za-z0-9_]{1,15})\\b\", txt)\n",
    "        if m_reply:\n",
    "            reply_to_user = m_reply.group(1).lower()\n",
    "\n",
    "        rows.append({\n",
    "            \"source_file\": source_file,\n",
    "            \"tweet_id\": f\"{source_file}:{i}\",\n",
    "            \"date\": pd.NaT,\n",
    "            \"lang\": None,\n",
    "            \"username\": None,\n",
    "            \"user_id\": None,\n",
    "            \"text\": txt,\n",
    "            \"mentions\": mentions,\n",
    "            \"hashtags\": hashtags,\n",
    "            \"is_retweet\": is_rt,\n",
    "            \"is_quote\": False,\n",
    "            \"retweeted_user\": rt_user,\n",
    "            \"quoted_user\": None,\n",
    "            \"reply_to_user\": reply_to_user,\n",
    "            \"in_reply_to_tweet_id\": None,\n",
    "            \"like_count\": None,\n",
    "            \"retweet_count\": None,\n",
    "            \"reply_count\": None,\n",
    "            \"quote_count\": None,\n",
    "            \"view_count\": None,\n",
    "            \"raw_record\": {\"_raw_line\": txt},\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# -------------------- Proceso principal de carga --------------------\n",
    "all_rows: List[Dict[str, Any]] = []\n",
    "summary = []\n",
    "\n",
    "for p in DATA_PATHS:\n",
    "    status = \"NO_FILE\"\n",
    "    n_lines = 0; n_json_whole = 0; n_jsonl = 0; n_json_inline = 0; used_plain = 0\n",
    "    encoding_used = \"-\"\n",
    "    if p.exists():\n",
    "        status = \"OK\"\n",
    "        raw, encoding_used = read_text_auto(p)\n",
    "        lines = raw.splitlines()\n",
    "        n_lines = len(lines)\n",
    "\n",
    "        # 1) JSON global (array/obj)\n",
    "        recs = read_any_json_whole(raw.strip())\n",
    "        if isinstance(recs, list) and recs:\n",
    "            all_rows.extend(rows_from_json(recs, p.name))\n",
    "            n_json_whole = len(recs)\n",
    "        else:\n",
    "            # 2) JSONL\n",
    "            recs_jsonl, ok = read_jsonl_lines(raw)\n",
    "            if ok > 0:\n",
    "                all_rows.extend(rows_from_json(recs_jsonl, p.name))\n",
    "                n_jsonl = ok\n",
    "            else:\n",
    "                # 3) Por línea: JSON embebido o texto plano\n",
    "                temp_rows = []\n",
    "                for line in lines:\n",
    "                    obj = extract_json_from_line(line)\n",
    "                    if isinstance(obj, dict):\n",
    "                        temp_rows.extend(rows_from_json([obj], p.name))\n",
    "                        n_json_inline += 1\n",
    "                    else:\n",
    "                        temp_rows.extend(rows_from_plaintext([line], p.name))\n",
    "                        used_plain += 1\n",
    "                all_rows.extend(temp_rows)\n",
    "\n",
    "    summary.append({\n",
    "        \"file\": p.name, \"status\": status, \"encoding\": encoding_used,\n",
    "        \"n_lines\": n_lines, \"JSON_global\": n_json_whole, \"JSONL\": n_jsonl,\n",
    "        \"JSON_inline\": n_json_inline, \"texto_plano\": used_plain\n",
    "    })\n",
    "\n",
    "# -------------------- DataFrame & salidas --------------------\n",
    "df = pd.DataFrame(all_rows)\n",
    "if not df.empty and \"date\" in df.columns:\n",
    "    df = df.sort_values(\"date\", na_position=\"last\").reset_index(drop=True)\n",
    "\n",
    "if not df.empty:\n",
    "    df.head(50).to_csv(OUT_SAMPLE_CSV, index=False)\n",
    "    try:\n",
    "        df.to_parquet(OUT_PARQUET, index=False)\n",
    "        parquet_path = str(OUT_PARQUET.resolve())\n",
    "    except Exception:\n",
    "        parquet_path = \"(No Parquet: instala 'pyarrow' o 'fastparquet')\"\n",
    "else:\n",
    "    parquet_path = \"(DataFrame vacío)\"\n",
    "\n",
    "print(\"=== RESUMEN DE CARGA (Paso 1–2) ===\")\n",
    "print(\"Archivos buscados:\")\n",
    "for p in DATA_PATHS: print(\" -\", p)\n",
    "\n",
    "for s in summary:\n",
    "    print(f\"- {s['file']:15s} | estado={s['status']:7s} | codif={s['encoding']:10s} | \"\n",
    "          f\"líneas={s['n_lines']:5d} | JSON_global={s['JSON_global']:5d} | \"\n",
    "          f\"JSONL={s['JSONL']:5d} | JSON_inline={s['JSON_inline']:5d} | \"\n",
    "          f\"texto_plano={s['texto_plano']:5d}\")\n",
    "\n",
    "print(f\"\\nTotal de filas normalizadas: {len(df):,}\")\n",
    "if not df.empty:\n",
    "    print(\"Columnas:\", list(df.columns))\n",
    "    print(f\"Muestra CSV (50 filas): {str(OUT_SAMPLE_CSV.resolve())}\")\n",
    "    print(f\"Parquet: {parquet_path}\")\n",
    "else:\n",
    "    print(\"Nota: DataFrame vacío. Si sigue vacío, compárteme 10–15 líneas crudas de un archivo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6a8d6",
   "metadata": {},
   "source": [
    "## **Inciso 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c62461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inciso 3 (3.1, 3.2, 3.3): Limpieza, extracción y normalización\n",
    "# - Convierte a minúsculas, quita URLs, emojis, puntuación, stopwords y números (opcional)\n",
    "# - Extrae/normaliza menciones, respuestas y retweets (rellena desde texto si falta)\n",
    "# - Elimina duplicados de tweets y normaliza nombres de usuario/menciones\n",
    "# - Deja columnas listas para el siguiente inciso (3.4)\n",
    "#\n",
    "# 👉 Al terminar imprime \"RESUMEN PREPROCESAMIENTO (3.1–3.3)\".\n",
    "#    Pégame ese bloque aquí para analizar y seguimos con 3.4.\n",
    "\n",
    "import re, json, ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# 0) Recupera df si no existe (vuelve a parsear JSONL UTF-16)\n",
    "# =========================\n",
    "def read_text_auto(path: Path):\n",
    "    data = path.read_bytes()\n",
    "    zero_ratio = data.count(0) / max(1, len(data))\n",
    "    cands = ([\"utf-16\",\"utf-16-le\",\"utf-16-be\",\"utf-8\",\"latin-1\"]\n",
    "             if zero_ratio > 0.01 else [\"utf-8\",\"utf-16\",\"utf-16-le\",\"utf-16-be\",\"latin-1\"])\n",
    "    for enc in cands:\n",
    "        try:\n",
    "            txt = data.decode(enc)\n",
    "            if txt.count(\"\\x00\") > 10:\n",
    "                continue\n",
    "            return txt\n",
    "        except Exception:\n",
    "            continue\n",
    "    return data.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def json_maybe_twice(s: str):\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, str):\n",
    "            st = obj.strip()\n",
    "            if (st.startswith(\"{\") and st.endswith(\"}\")) or (st.startswith(\"[\") and st.endswith(\"]\")):\n",
    "                try: return json.loads(st)\n",
    "                except Exception: return obj\n",
    "        return obj\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_jsonl_utf16(paths):\n",
    "    rows = []\n",
    "    for p in paths:\n",
    "        if not Path(p).exists():\n",
    "            continue\n",
    "        raw = read_text_auto(Path(p))\n",
    "        for line in raw.splitlines():\n",
    "            s = line.strip().rstrip(\",\")\n",
    "            if not s: \n",
    "                continue\n",
    "            obj = json_maybe_twice(s)\n",
    "            if not isinstance(obj, dict):\n",
    "                continue\n",
    "            r = obj\n",
    "            # username\n",
    "            u = r.get(\"user\") if isinstance(r.get(\"user\"), dict) else {}\n",
    "            uname = (u.get(\"username\") or u.get(\"screen_name\") or u.get(\"name\"))\n",
    "            # mentions\n",
    "            mentions = []\n",
    "            if \"mentionedUsers\" in r and isinstance(r[\"mentionedUsers\"], list):\n",
    "                for m in r[\"mentionedUsers\"]:\n",
    "                    if isinstance(m, dict):\n",
    "                        un = m.get(\"username\") or m.get(\"screen_name\") or m.get(\"name\")\n",
    "                        if un: mentions.append(un.lower().lstrip(\"@\"))\n",
    "            elif \"entities\" in r and isinstance(r[\"entities\"], dict):\n",
    "                for m in (r[\"entities\"].get(\"user_mentions\") or []):\n",
    "                    if isinstance(m, dict):\n",
    "                        un = m.get(\"screen_name\") or m.get(\"username\") or m.get(\"name\")\n",
    "                        if un: mentions.append(un.lower().lstrip(\"@\"))\n",
    "            # hashtags\n",
    "            tags = []\n",
    "            if isinstance(r.get(\"hashtags\"), list):\n",
    "                for h in r[\"hashtags\"]:\n",
    "                    if isinstance(h, str): tags.append(h.lstrip(\"#\").lower())\n",
    "                    elif isinstance(h, dict):\n",
    "                        t = h.get(\"text\") or h.get(\"tag\")\n",
    "                        if t: tags.append(str(t).lstrip(\"#\").lower())\n",
    "            elif \"entities\" in r and isinstance(r[\"entities\"], dict):\n",
    "                for h in (r[\"entities\"].get(\"hashtags\") or []):\n",
    "                    if isinstance(h, str): tags.append(h.lstrip(\"#\").lower())\n",
    "                    elif isinstance(h, dict):\n",
    "                        t = h.get(\"text\") or h.get(\"tag\")\n",
    "                        if t: tags.append(str(t).lstrip(\"#\").lower())\n",
    "            # retweet / quote\n",
    "            rt = r.get(\"retweetedTweet\") or r.get(\"retweeted_status\")\n",
    "            qt = r.get(\"quotedTweet\") or r.get(\"quoted_status\")\n",
    "            rt_user = (rt or {}).get(\"user\", {})\n",
    "            qt_user = (qt or {}).get(\"user\", {})\n",
    "            # reply\n",
    "            reply_to = None\n",
    "            if isinstance(r.get(\"inReplyToUser\"), dict):\n",
    "                reply_to = r[\"inReplyToUser\"].get(\"username\")\n",
    "            if not reply_to and isinstance(r.get(\"inReplyToUser\"), dict):\n",
    "                reply_to = r[\"inReplyToUser\"].get(\"screen_name\")\n",
    "            if not reply_to and r.get(\"in_reply_to_screen_name\"):\n",
    "                reply_to = r.get(\"in_reply_to_screen_name\")\n",
    "\n",
    "            def norm_user(x):\n",
    "                if not x: return None\n",
    "                x = str(x).strip()\n",
    "                if x.startswith(\"@\"): x = x[1:]\n",
    "                return x.lower()\n",
    "\n",
    "            rows.append({\n",
    "                \"source_file\": Path(p).name,\n",
    "                \"tweet_id\": r.get(\"id\") or r.get(\"id_str\"),\n",
    "                \"date\": pd.to_datetime(r.get(\"date\") or r.get(\"created_at\"), errors=\"coerce\"),\n",
    "                \"lang\": r.get(\"lang\"),\n",
    "                \"username\": norm_user(uname),\n",
    "                \"user_id\": (u.get(\"id\") or u.get(\"id_str\")) if isinstance(u, dict) else None,\n",
    "                \"text\": (r.get(\"rawContent\") or r.get(\"full_text\") or r.get(\"text\")),\n",
    "                \"mentions\": [m for m in mentions if m],\n",
    "                \"hashtags\": [t for t in tags if t],\n",
    "                \"is_retweet\": rt is not None,\n",
    "                \"is_quote\": qt is not None,\n",
    "                \"retweeted_user\": norm_user(rt_user.get(\"username\") if isinstance(rt_user, dict) else None),\n",
    "                \"quoted_user\": norm_user(qt_user.get(\"username\") if isinstance(qt_user, dict) else None),\n",
    "                \"reply_to_user\": norm_user(reply_to),\n",
    "                \"in_reply_to_tweet_id\": r.get(\"inReplyToTweetId\") or r.get(\"in_reply_to_status_id\") or r.get(\"in_reply_to_status_id_str\"),\n",
    "                \"like_count\": r.get(\"likeCount\") or r.get(\"favorite_count\"),\n",
    "                \"retweet_count\": r.get(\"retweetCount\") or r.get(\"retweet_count\"),\n",
    "                \"reply_count\": r.get(\"replyCount\") or r.get(\"reply_count\"),\n",
    "                \"quote_count\": r.get(\"quoteCount\") or r.get(\"quote_count\"),\n",
    "                \"view_count\": r.get(\"viewCount\") or r.get(\"views\"),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "try:\n",
    "    df\n",
    "    assert isinstance(df, pd.DataFrame) and not df.empty\n",
    "except Exception:\n",
    "    df = load_jsonl_utf16([\"traficogt.txt\",\"/mnt/data/traficogt.txt\",\n",
    "                           \"tioberny.txt\",\"/mnt/data/tioberny.txt\"])\n",
    "\n",
    "# =========================\n",
    "# 1) Funciones de limpieza\n",
    "# =========================\n",
    "# Fix de mojibake (p. ej., \"TÃ©cnica\" -> \"Técnica\")\n",
    "def fix_mojibake(s):\n",
    "    if not isinstance(s, str): \n",
    "        return s\n",
    "    if \"Ã\" in s or \"Â\" in s or \"ðŸ\" in s:\n",
    "        try:\n",
    "            return s.encode(\"latin1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "EMOJI_RE = re.compile(\n",
    "    \"[\"                         # bloques comunes de emoji\n",
    "    \"\\U0001F600-\\U0001F64F\"     # emoticonos\n",
    "    \"\\U0001F300-\\U0001F5FF\"     # símbolos & pictogramas\n",
    "    \"\\U0001F680-\\U0001F6FF\"     # transporte\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"     # banderas\n",
    "    \"\\U00002700-\\U000027BF\"     # otros\n",
    "    \"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE\n",
    ")\n",
    "# Mantener letras (incl. acentos) y espacios; quitar dígitos y puntuación\n",
    "KEEP_LETTERS_RE = re.compile(r\"[^a-záéíóúüñ\\s]\")\n",
    "\n",
    "STOP_ES = set(\"\"\"\n",
    "a al algo algunas algunos ante antes apenas aquella aquellas aquello aquellos aqui\n",
    "así aun aunque cada casi como con contigo contra cual cuales cualquier cuando\n",
    "de del desde donde dos el ella ellas ellos en entre era erais eran eras eres es esa\n",
    "esas ese eso esos esta estaba estabais estaban estabas estamos estan estaré estarás\n",
    "este esto estos estuvo fui fue fueron hemos han hasta hay hice hizo hoy la las le les\n",
    "lo los me mi mis mucha muchas mucho muchos muy nada ni no nosotros nosotras nuestra\n",
    "nuestro nuestras nuestros nunca o os para pero poco por porque que quien quienes se\n",
    "sea sean según ser si siempre sin sino sobre sois son soy su sus también te tengo tiene\n",
    "tienen toda todas todo todos tras tu tus un una uno unos usted ustedes ya y yo\n",
    "\"\"\".split())\n",
    "\n",
    "def clean_text(s: str):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return \"\"\n",
    "    s = fix_mojibake(s)\n",
    "    s = s.lower()\n",
    "    s = URL_RE.sub(\" \", s)\n",
    "    s = EMOJI_RE.sub(\" \", s)\n",
    "    # Quitar @ y # del texto (las menciones/hashtags se quedan en columnas aparte)\n",
    "    s = s.replace(\"@\", \" \").replace(\"#\", \" \")\n",
    "    # Quitar apóstrofes simples/raros\n",
    "    s = s.replace(\"’\", \"\").replace(\"‘\", \"\").replace(\"´\",\"\").replace(\"`\",\"\").replace(\"'\", \"\")\n",
    "    # Quitar puntuación y números (si deseas conservar números, comenta la siguiente línea y usa otra regex)\n",
    "    s = KEEP_LETTERS_RE.sub(\" \", s)\n",
    "    # Normalizar espacios\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def ensure_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            val = ast.literal_eval(x)\n",
    "            if isinstance(val, list): \n",
    "                return val\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def norm_user(u):\n",
    "    if not u: return None\n",
    "    u = str(u).strip()\n",
    "    if u.startswith(\"@\"): u = u[1:]\n",
    "    return u.lower()\n",
    "\n",
    "# =========================\n",
    "# 2) Aplicar preprocesamiento\n",
    "# =========================\n",
    "df_proc = df.copy()\n",
    "\n",
    "# Arregla mojibake en campos de texto/usuario que lo requieran\n",
    "for col in [\"text\",\"username\",\"retweeted_user\",\"quoted_user\",\"reply_to_user\"]:\n",
    "    if col in df_proc.columns:\n",
    "        df_proc[col] = df_proc[col].apply(lambda x: fix_mojibake(x) if isinstance(x,str) else x)\n",
    "\n",
    "# Asegura listas en menciones/hashtags\n",
    "if \"mentions\" in df_proc.columns:\n",
    "    df_proc[\"mentions\"] = df_proc[\"mentions\"].apply(ensure_list).apply(lambda xs: [norm_user(x) for x in xs if x])\n",
    "else:\n",
    "    df_proc[\"mentions\"] = [[] for _ in range(len(df_proc))]\n",
    "\n",
    "if \"hashtags\" in df_proc.columns:\n",
    "    df_proc[\"hashtags\"] = df_proc[\"hashtags\"].apply(ensure_list).apply(\n",
    "        lambda xs: [fix_mojibake(x).lstrip(\"#\").lower() for x in xs if isinstance(x,str) and x]\n",
    "    )\n",
    "else:\n",
    "    df_proc[\"hashtags\"] = [[] for _ in range(len(df_proc))]\n",
    "\n",
    "# 3.2 Completa menciones, respuestas y RT desde el texto si faltan\n",
    "MENTION_RE = re.compile(r\"@([A-Za-z0-9_]{1,15})\")\n",
    "def backfill_from_text(row):\n",
    "    txt = row.get(\"text\") or \"\"\n",
    "    # menciones desde texto\n",
    "    if isinstance(txt, str) and txt:\n",
    "        found = [m.lower() for m in MENTION_RE.findall(txt)]\n",
    "    else:\n",
    "        found = []\n",
    "    # union de las ya existentes con las del texto\n",
    "    cur = set([m for m in row[\"mentions\"] if m])\n",
    "    cur.update(found)\n",
    "    row[\"mentions\"] = sorted(cur)\n",
    "\n",
    "    # reply si inicia con @user\n",
    "    m = re.match(r\"^\\s*@([A-Za-z0-9_]{1,15})\\b\", txt or \"\")\n",
    "    if not row.get(\"reply_to_user\") and m:\n",
    "        row[\"reply_to_user\"] = norm_user(m.group(1))\n",
    "\n",
    "    # RT si inicia con \"RT @user:\"\n",
    "    m2 = re.match(r\"^\\s*rt\\s+@([A-Za-z0-9_]{1,15})\\b\", (txt or \"\").lower())\n",
    "    if (not row.get(\"is_retweet\")) and m2:\n",
    "        row[\"is_retweet\"] = True\n",
    "        row[\"retweeted_user\"] = norm_user(m2.group(1))\n",
    "    return row\n",
    "\n",
    "df_proc = df_proc.apply(backfill_from_text, axis=1)\n",
    "\n",
    "# 3.1 Limpieza de texto\n",
    "df_proc[\"text_clean\"] = df_proc[\"text\"].apply(clean_text)\n",
    "\n",
    "# Tokens (separación simple por espacio)\n",
    "df_proc[\"tokens\"] = df_proc[\"text_clean\"].apply(lambda s: [t for t in s.split() if t])\n",
    "\n",
    "# Stopwords\n",
    "df_proc[\"tokens_nostop\"] = df_proc[\"tokens\"].apply(lambda ts: [t for t in ts if t not in STOP_ES])\n",
    "\n",
    "# 3.3 Normalización de usuarios y eliminación de duplicados\n",
    "for col in [\"username\",\"reply_to_user\",\"retweeted_user\",\"quoted_user\"]:\n",
    "    if col in df_proc.columns:\n",
    "        df_proc[col] = df_proc[col].apply(norm_user)\n",
    "\n",
    "n_raw = len(df_proc)\n",
    "# elimina duplicados por tweet_id si existe, si no por (source_file,text)\n",
    "if \"tweet_id\" in df_proc.columns:\n",
    "    df_proc = df_proc.drop_duplicates(subset=[\"tweet_id\"])\n",
    "else:\n",
    "    df_proc = df_proc.drop_duplicates(subset=[\"source_file\",\"text\"])\n",
    "n_after_dedup = len(df_proc)\n",
    "dup_removed = n_raw - n_after_dedup\n",
    "\n",
    "# =========================\n",
    "# 3) Resumen para reporte\n",
    "# =========================\n",
    "n_with_mentions = int((df_proc[\"mentions\"].apply(len) > 0).sum())\n",
    "n_with_reply = int(df_proc[\"reply_to_user\"].notna().sum())\n",
    "n_is_rt = int(df_proc[\"is_retweet\"].fillna(False).sum())\n",
    "\n",
    "# hashtags más frecuentes (top 10)\n",
    "from collections import Counter\n",
    "ht_counts = Counter(h for xs in df_proc[\"hashtags\"] for h in (xs or []))\n",
    "top_hashtags = ht_counts.most_common(10)\n",
    "\n",
    "print(\"=== RESUMEN PREPROCESAMIENTO (3.1–3.3) ===\")\n",
    "print(f\"Filas originales: {n_raw:,}\")\n",
    "print(f\"Duplicados removidos: {dup_removed:,}\")\n",
    "print(f\"Filas finales: {n_after_dedup:,}\")\n",
    "print(f\"Tweets con menciones: {n_with_mentions:,}\")\n",
    "print(f\"Tweets que son replies: {n_with_reply:,}\")\n",
    "print(f\"Tweets que son retweets: {n_is_rt:,}\")\n",
    "print(\"\\nTop 10 hashtags (después de limpieza):\")\n",
    "for tag, cnt in top_hashtags:\n",
    "    print(f\"  #{tag}: {cnt}\")\n",
    "\n",
    "# Vista rápida de ejemplos (5)\n",
    "print(\"\\nEjemplos de 'text'  →  'text_clean' (5 filas):\")\n",
    "for i, row in df_proc.head(5).iterrows():\n",
    "    print(f\"- {row.get('text')!r}  ->  {row.get('text_clean')!r}\")\n",
    "\n",
    "# Guarda muestra para inspección manual (opcional)\n",
    "df_proc.head(200).to_csv(\"tweets_clean_sample.csv\", index=False)\n",
    "print(\"\\nArchivo de muestra (200 filas) guardado en: tweets_clean_sample.csv\")\n",
    "\n",
    "# Mantener df_proc en memoria para el siguiente inciso (3.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb3e213",
   "metadata": {},
   "source": [
    "## **Inciso 4A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4448f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda A — Inciso 4.1: Análisis Exploratorio (EDA) ===\n",
    "# - Usa df_proc de la celda anterior; si no existe, intenta reconstruirlo rápido desde los txt.\n",
    "# - Calcula: cuentas básicas, top hashtags, top tokens, top usuarios, timeline, interacciones.\n",
    "# - Genera gráficos (matplotlib) y guarda CSV/PNGs para documentación.\n",
    "#\n",
    "# Al final imprime: \"RESUMEN EDA (4.1)\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# ---------- Helpers de carga mínima (por si abres el notebook desde cero) ----------\n",
    "def _ensure_df_proc():\n",
    "    global df_proc\n",
    "    if \"df_proc\" in globals() and isinstance(df_proc, pd.DataFrame) and not df_proc.empty:\n",
    "        return df_proc\n",
    "\n",
    "    # Fallback 1: si dejaste la muestra limpia\n",
    "    if Path(\"tweets_clean_sample.csv\").exists():\n",
    "        tmp = pd.read_csv(\"tweets_clean_sample.csv\")\n",
    "        # columnas listas\n",
    "        for col in [\"mentions\",\"hashtags\",\"tokens\",\"tokens_nostop\"]:\n",
    "            if col in tmp.columns:\n",
    "                tmp[col] = tmp[col].apply(lambda x: [] if pd.isna(x) else eval(x) if isinstance(x,str) else [])\n",
    "        # normaliza fechas\n",
    "        if \"date\" in tmp.columns:\n",
    "            tmp[\"date\"] = pd.to_datetime(tmp[\"date\"], errors=\"coerce\")\n",
    "        dfp = tmp\n",
    "        return dfp\n",
    "\n",
    "    # Fallback 2 (reconstrucción mínima): vuelve a leer JSONL UTF-16 y limpia básico\n",
    "    import json\n",
    "    def _read_bytes_auto(path: Path) -> str:\n",
    "        data = path.read_bytes()\n",
    "        if data.count(0)/max(1,len(data)) > 0.01:\n",
    "            for enc in [\"utf-16\",\"utf-16-le\",\"utf-16-be\",\"utf-8\",\"latin-1\"]:\n",
    "                try:\n",
    "                    s = data.decode(enc)\n",
    "                    if s.count(\"\\x00\") < 5: return s\n",
    "                except: pass\n",
    "            return data.decode(\"utf-16\", errors=\"ignore\")\n",
    "        return data.decode(\"utf-8\", errors=\"ignore\")\n",
    "    def _json_maybe_twice(s):\n",
    "        try:\n",
    "            o = json.loads(s)\n",
    "            if isinstance(o,str) and o.strip() and o.strip()[0] in \"{[\":\n",
    "                try: return json.loads(o)\n",
    "                except: return o\n",
    "            return o\n",
    "        except: return None\n",
    "\n",
    "    rows=[]\n",
    "    for p in [\"traficogt.txt\",\"/mnt/data/traficogt.txt\",\"tioberny.txt\",\"/mnt/data/tioberny.txt\"]:\n",
    "        p=Path(p)\n",
    "        if not p.exists(): continue\n",
    "        raw=_read_bytes_auto(p)\n",
    "        for line in raw.splitlines():\n",
    "            s=line.strip().rstrip(\",\")\n",
    "            if not s: continue\n",
    "            obj=_json_maybe_twice(s)\n",
    "            if not isinstance(obj,dict): continue\n",
    "            r=obj; u=r.get(\"user\") if isinstance(r.get(\"user\"),dict) else {}\n",
    "            def _norm(u):\n",
    "                if not u: return None\n",
    "                u=str(u).strip()\n",
    "                if u.startswith(\"@\"): u=u[1:]\n",
    "                return u.lower()\n",
    "            # mentions\n",
    "            m=[]\n",
    "            if \"mentionedUsers\" in r and isinstance(r[\"mentionedUsers\"],list):\n",
    "                for it in r[\"mentionedUsers\"]:\n",
    "                    if isinstance(it,dict):\n",
    "                        un=it.get(\"username\") or it.get(\"screen_name\") or it.get(\"name\")\n",
    "                        if un: m.append(_norm(un))\n",
    "            # hashtags\n",
    "            hs=[]\n",
    "            if isinstance(r.get(\"hashtags\"),list):\n",
    "                for h in r[\"hashtags\"]:\n",
    "                    if isinstance(h,str): hs.append(h.lstrip(\"#\").lower())\n",
    "                    elif isinstance(h,dict):\n",
    "                        t=h.get(\"text\") or h.get(\"tag\")\n",
    "                        if t: hs.append(str(t).lstrip(\"#\").lower())\n",
    "            rows.append({\n",
    "                \"source_file\": p.name,\n",
    "                \"tweet_id\": r.get(\"id\") or r.get(\"id_str\"),\n",
    "                \"date\": pd.to_datetime(r.get(\"date\") or r.get(\"created_at\"), errors=\"coerce\"),\n",
    "                \"lang\": r.get(\"lang\"),\n",
    "                \"username\": _norm((u.get(\"username\") or u.get(\"screen_name\") or u.get(\"name\")) if u else None),\n",
    "                \"text\": (r.get(\"rawContent\") or r.get(\"full_text\") or r.get(\"text\")),\n",
    "                \"mentions\": m,\n",
    "                \"hashtags\": hs,\n",
    "                \"is_retweet\": bool(r.get(\"retweetedTweet\") or r.get(\"retweeted_status\")),\n",
    "                \"reply_to_user\": _norm((r.get(\"inReplyToUser\") or {}).get(\"username\") if isinstance(r.get(\"inReplyToUser\"),dict) else r.get(\"in_reply_to_screen_name\")),\n",
    "            })\n",
    "    dfp=pd.DataFrame(rows)\n",
    "    if dfp.empty:\n",
    "        raise RuntimeError(\"No pude reconstruir df_proc. Vuelve a ejecutar la celda 3 (preprocesamiento).\")\n",
    "    # limpieza mínima de texto para tokens\n",
    "    def _fix(s):\n",
    "        if isinstance(s,str) and (\"Ã\" in s or \"Â\" in s): \n",
    "            try: return s.encode(\"latin1\",\"ignore\").decode(\"utf-8\",\"ignore\")\n",
    "            except: return s\n",
    "        return s\n",
    "    dfp[\"text\"]=dfp[\"text\"].apply(_fix)\n",
    "    import re\n",
    "    URL_RE=re.compile(r\"https?://\\S+|www\\.\\S+\", re.I)\n",
    "    KEEP=re.compile(r\"[^a-záéíóúüñ\\s]\")\n",
    "    def _clean(s):\n",
    "        if not isinstance(s,str): return \"\"\n",
    "        s=s.lower()\n",
    "        s=URL_RE.sub(\" \", s)\n",
    "        s=s.replace(\"@\",\" \").replace(\"#\",\" \")\n",
    "        s=KEEP.sub(\" \", s)\n",
    "        s=re.sub(r\"\\s+\",\" \",s).strip()\n",
    "        return s\n",
    "    dfp[\"text_clean\"]=dfp[\"text\"].apply(_clean)\n",
    "    STOP=set(\"a al las los de del en la el y o que con por para un una uno unos unas se es no si como pero sobre ya muy sin más menos mi tu su sus lo me te le les es son ser fue fueron soy somos estoy están estaba están hasta hay\".split())\n",
    "    dfp[\"tokens\"]=dfp[\"text_clean\"].str.split()\n",
    "    dfp[\"tokens_nostop\"]=dfp[\"tokens\"].apply(lambda ts: [t for t in ts if t not in STOP])\n",
    "    return dfp\n",
    "\n",
    "df_eda = _ensure_df_proc().copy()\n",
    "\n",
    "# ---------- Métricas básicas ----------\n",
    "n_tweets = len(df_eda)\n",
    "n_users = int(df_eda[\"username\"].dropna().nunique()) if \"username\" in df_eda else 0\n",
    "n_mentions_total = int(sum(len(x or []) for x in df_eda[\"mentions\"]))\n",
    "unique_mentioned = set()\n",
    "for xs in df_eda[\"mentions\"]:\n",
    "    unique_mentioned.update(xs or [])\n",
    "n_users_mentioned = len(unique_mentioned)\n",
    "n_replies = int(df_eda[\"reply_to_user\"].notna().sum()) if \"reply_to_user\" in df_eda else 0\n",
    "n_retweets = int(df_eda[\"is_retweet\"].fillna(False).sum()) if \"is_retweet\" in df_eda else 0\n",
    "\n",
    "# ---------- Top hashtags ----------\n",
    "ht_counter = Counter(h for xs in df_eda[\"hashtags\"] for h in (xs or []))\n",
    "df_top_ht = pd.DataFrame(ht_counter.most_common(30), columns=[\"hashtag\",\"count\"])\n",
    "\n",
    "# ---------- Top tokens (sin stopwords) ----------\n",
    "tok_counter = Counter(t for xs in (df_eda[\"tokens_nostop\"] if \"tokens_nostop\" in df_eda else df_eda[\"tokens\"]) for t in (xs or []))\n",
    "df_top_tokens = pd.DataFrame(tok_counter.most_common(30), columns=[\"token\",\"count\"])\n",
    "\n",
    "# ---------- Usuarios más activos / más mencionados ----------\n",
    "df_active_users = (df_eda.dropna(subset=[\"username\"])\n",
    "                   .groupby(\"username\", as_index=False)\n",
    "                   .agg(tweets=(\"tweet_id\",\"count\")))\n",
    "# más mencionados\n",
    "mentioned_counter = Counter()\n",
    "for _, row in df_eda.iterrows():\n",
    "    src = row.get(\"username\")\n",
    "    for dst in row.get(\"mentions\") or []:\n",
    "        mentioned_counter[dst] += 1\n",
    "df_most_mentioned = pd.DataFrame(mentioned_counter.most_common(30), columns=[\"user\",\"mentions_received\"])\n",
    "\n",
    "# ---------- Timeline (por día) ----------\n",
    "if \"date\" in df_eda.columns:\n",
    "    df_eda[\"date_only\"] = df_eda[\"date\"].dt.date\n",
    "    ts_daily = (df_eda.dropna(subset=[\"date_only\"])\n",
    "                .groupby(\"date_only\", as_index=False)\n",
    "                .agg(tweets=(\"tweet_id\",\"count\"),\n",
    "                     replies=(\"reply_to_user\", lambda s: int(s.notna().sum())),\n",
    "                     retweets=(\"is_retweet\", lambda s: int(pd.Series(s).fillna(False).sum()))))\n",
    "else:\n",
    "    ts_daily = pd.DataFrame()\n",
    "\n",
    "# ---------- Construcción de edges de interacción (para vistas/CSV) ----------\n",
    "edges=[]\n",
    "for _,row in df_eda.iterrows():\n",
    "    src = row.get(\"username\")\n",
    "    if not src: continue\n",
    "    # menciones\n",
    "    for dst in (row.get(\"mentions\") or []):\n",
    "        if dst: edges.append((src,dst,\"mention\"))\n",
    "    # reply\n",
    "    dst = row.get(\"reply_to_user\")\n",
    "    if dst: edges.append((src,dst,\"reply\"))\n",
    "    # retweet\n",
    "    if row.get(\"is_retweet\") and row.get(\"retweeted_user\"):\n",
    "        edges.append((src,row.get(\"retweeted_user\"),\"retweet\"))\n",
    "\n",
    "df_edges = pd.DataFrame(edges, columns=[\"src\",\"dst\",\"type\"])\n",
    "\n",
    "# ---------- Guardados ----------\n",
    "out_dir = Path(\"eda_outputs\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "df_top_ht.to_csv(out_dir/\"top_hashtags.csv\", index=False)\n",
    "df_top_tokens.to_csv(out_dir/\"top_tokens.csv\", index=False)\n",
    "df_active_users.sort_values(\"tweets\", ascending=False).head(100).to_csv(out_dir/\"top_active_users.csv\", index=False)\n",
    "df_most_mentioned.head(100).to_csv(out_dir/\"top_most_mentioned.csv\", index=False)\n",
    "df_edges.to_csv(out_dir/\"edges_mentions_replies_retweets.csv\", index=False)\n",
    "ts_daily.to_csv(out_dir/\"timeline_daily.csv\", index=False)\n",
    "\n",
    "# ---------- Gráficas rápidas ----------\n",
    "plt.figure()\n",
    "df_top_ht.head(15).plot(kind=\"bar\", x=\"hashtag\", y=\"count\", legend=False, rot=45)\n",
    "plt.title(\"Top 15 hashtags\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir/\"top_hashtags.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "df_top_tokens.head(15).plot(kind=\"bar\", x=\"token\", y=\"count\", legend=False, rot=45)\n",
    "plt.title(\"Top 15 tokens (sin stopwords)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir/\"top_tokens.png\")\n",
    "plt.show()\n",
    "\n",
    "if not ts_daily.empty:\n",
    "    plt.figure()\n",
    "    plt.plot(ts_daily[\"date_only\"], ts_daily[\"tweets\"])\n",
    "    plt.title(\"Tweets por día\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir/\"timeline_tweets.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Word cloud opcional si tienes instalado wordcloud\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    text_wc = \" \".join(t for xs in df_eda.get(\"tokens_nostop\", df_eda[\"tokens\"]) for t in (xs or []))\n",
    "    wc = WordCloud(width=1200, height=600, background_color=\"white\").generate(text_wc)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Nube de palabras (opcional)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir/\"wordcloud.png\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Nube de palabras no generada (falta 'wordcloud' o no hay tokens). Si quieres, instala 'wordcloud'.\")\n",
    "\n",
    "# ---------- Impresión del resumen ----------\n",
    "print(\"=== RESUMEN EDA (4.1) ===\")\n",
    "print(f\"Tweets totales: {n_tweets:,}\")\n",
    "print(f\"Usuarios únicos (emitentes): {n_users:,}\")\n",
    "print(f\"Menciones totales: {n_mentions_total:,} | Usuarios distintos mencionados: {n_users_mentioned:,}\")\n",
    "print(f\"Tweets que son replies: {n_replies:,}\")\n",
    "print(f\"Tweets que son retweets: {n_retweets:,}\")\n",
    "if not df_top_ht.empty:\n",
    "    print(\"\\nTop 10 hashtags:\")\n",
    "    for h,c in df_top_ht.head(10).itertuples(index=False):\n",
    "        print(f\"  #{h}: {c}\")\n",
    "if not df_active_users.empty:\n",
    "    print(\"\\nTop 10 usuarios por # de tweets:\")\n",
    "    for u,c in df_active_users.sort_values(\"tweets\", ascending=False).head(10).itertuples(index=False):\n",
    "        print(f\"  @{u}: {c}\")\n",
    "if not df_most_mentioned.empty:\n",
    "    print(\"\\nTop 10 usuarios más mencionados:\")\n",
    "    for u,c in df_most_mentioned.head(10).itertuples(index=False):\n",
    "        print(f\"  @{u}: {c}\")\n",
    "\n",
    "print(\"\\nArchivos generados en:\", str(out_dir.resolve()))\n",
    "print(\" - top_hashtags.csv / .png\")\n",
    "print(\" - top_tokens.csv / .png\")\n",
    "print(\" - top_active_users.csv\")\n",
    "print(\" - top_most_mentioned.csv\")\n",
    "print(\" - edges_mentions_replies_retweets.csv\")\n",
    "print(\" - timeline_daily.csv\", \"(y timeline_tweets.png si hay fechas)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d29115",
   "metadata": {},
   "source": [
    "## **Inciso 4b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda B — Inciso 4.2: Preguntas interesantes y resultados calculados ===\n",
    "# Imprime 3 preguntas sugeridas y los datos que permiten responderlas.\n",
    "# Al final: \"PREGUNTAS Y RESPUESTAS (4.2) – RESULTADOS\"\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def _ensure_df_eda_again():\n",
    "    if \"df_eda\" in globals():\n",
    "        return df_eda\n",
    "    elif \"df_proc\" in globals():\n",
    "        return df_proc\n",
    "    else:\n",
    "        raise RuntimeError(\"Ejecuta primero la Celda A.\")\n",
    "\n",
    "D = _ensure_df_eda_again()\n",
    "\n",
    "# 1) ¿Qué usuarios son más mencionados (potenciales hubs de atención)?\n",
    "mentioned_counter = Counter()\n",
    "for xs in D[\"mentions\"]:\n",
    "    if not isinstance(xs, list): \n",
    "        continue\n",
    "    for m in xs:\n",
    "        if m: mentioned_counter[m] += 1\n",
    "df_q1 = pd.DataFrame(mentioned_counter.most_common(20), columns=[\"usuario\",\"menciones_recibidas\"])\n",
    "\n",
    "# 2) ¿Quiénes son los usuarios más activos (más tweets emitidos) y qué % de sus tweets son replies?\n",
    "df_q2 = (D.dropna(subset=[\"username\"])\n",
    "         .groupby(\"username\", as_index=False)\n",
    "         .agg(tweets=(\"tweet_id\",\"count\"),\n",
    "              replies=(\"reply_to_user\", lambda s: int(s.notna().sum()))))\n",
    "df_q2[\"pct_reply\"] = (df_q2[\"replies\"] / df_q2[\"tweets\"] * 100).round(1)\n",
    "df_q2 = df_q2.sort_values([\"tweets\",\"pct_reply\"], ascending=[False, False]).head(20)\n",
    "\n",
    "# 3) ¿Cuáles son los hashtags más usados y su presencia relativa?\n",
    "ht_counter = Counter(h for xs in D[\"hashtags\"] for h in (xs or []))\n",
    "total_ht = sum(ht_counter.values()) or 1\n",
    "df_q3 = (pd.DataFrame(ht_counter.most_common(20), columns=[\"hashtag\",\"freq\"])\n",
    "           .assign(pct=lambda x: (x[\"freq\"]/total_ht*100).round(2)))\n",
    "\n",
    "print(\"=== PREGUNTAS Y RESPUESTAS (4.2) – RESULTADOS ===\")\n",
    "print(\"\\nP1) Usuarios más mencionados (Top 10):\")\n",
    "print(df_q1.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nP2) Usuarios más activos y % de replies (Top 10):\")\n",
    "print(df_q2.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nP3) Hashtags más usados y participación (Top 10):\")\n",
    "print(df_q3.head(10).to_string(index=False))\n",
    "\n",
    "# Guardados\n",
    "out_dir = Path(\"eda_outputs\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "df_q1.to_csv(out_dir/\"q1_mas_mencionados.csv\", index=False)\n",
    "df_q2.to_csv(out_dir/\"q2_mas_activos_y_pct_reply.csv\", index=False)\n",
    "df_q3.to_csv(out_dir/\"q3_hashtags_top_con_pct.csv\", index=False)\n",
    "print(f\"\\nArchivos: {str((out_dir/'q1_mas_mencionados.csv').resolve())}, \"\n",
    "      f\"{str((out_dir/'q2_mas_activos_y_pct_reply.csv').resolve())}, \"\n",
    "      f\"{str((out_dir/'q3_hashtags_top_con_pct.csv').resolve())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0c600",
   "metadata": {},
   "source": [
    "# Análisis Exploratorio (Inciso 4) — Tweets de *traficogt* y *tioberny*\n",
    "\n",
    "> **Entrada**: 10,539 tweets; 4,291 usuarios emisores; 28,037 menciones (1,856 usuarios distintos mencionados); 8,552 *replies*; 0 *retweets* detectados.  \n",
    "> **Fuente**: resultados de las Celdas 4.1 y 4.2 (gráficas, tablas y CSV generados).\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Volumen y temporalidad\n",
    "\n",
    "- La serie **“Tweets por día”** muestra actividad casi nula antes de 2023 y **picos muy marcados en 2024** (días con >1,000 tweets).  \n",
    "- Esto sugiere **coyunturas noticiosas/políticas** recientes que disparan la conversación.  \n",
    "- Recomendación: marcar los **días pico** y contrastar con sucesos (congresales, judiciales, protestas, bloqueos, etc.).\n",
    "\n",
    "**Indicadores rápidos**\n",
    "\n",
    "- **% de replies**: ≈ **81%** (8,552/10,539) → conversación altamente **dialogante** (respuestas directas).  \n",
    "- **Menciones por tweet**: ≈ **2.66** (28,037/10,539) → alta **interacción dirigida** entre usuarios.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Temas, vocabulario y etiquetas\n",
    "\n",
    "- En la nube de palabras y el *top* de tokens (sin *stopwords*) destacan **usuarios/cuentas e instituciones**:  \n",
    "  `barevalodeleon`, `traficogt`, `guatemalagob`, `mpguatemala`, `presidente`, `ubaldomacu`, `santipalomov`.  \n",
    "  → Foco en **figuras públicas**, **cuentas institucionales** y **noticias**.\n",
    "\n",
    "- **Hashtags principales** (frecuencia y % del total):\n",
    "  1. `#guatemala` (40; 4.17%)  \n",
    "  2. `#ahora` (36; 3.75%)  \n",
    "  3. `#urgente` (33; 3.44%)  \n",
    "  4. `#guatemalasaleadelante` (24; 2.50%)  \n",
    "  5. `#traficogt` (22; 2.29%)  \n",
    "  6. `#ahoralh` (19; 1.98%)  \n",
    "  7. `#minfinsaleadelante` (17; 1.77%)  \n",
    "  8. `#renunciengolpistas` (15; 1.56%)  \n",
    "  9. `#presupuesto2025` (14; 1.46%)  \n",
    "  10. `#unpresupuestoparalapoblación` (13; 1.35%)\n",
    "\n",
    "**Lectura**: mezcla de etiquetas **noticiosas genéricas** (#ahora/#urgente), **institucionales** y **coyunturales** (presupuesto/consignas).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Actividad vs. atención: quién habla y a quién escuchan\n",
    "\n",
    "**Usuarios más activos (emiten más tweets)**  \n",
    "`@traficogt` (781), `@batallonjalapa` (134), `@mildred_gaitan` (105), `@lahoragt` (74), `@chofito63569841` (49), `@angeln8` (46), `@prensacomunitar` (45), `@papaabumario` (43), `@elrevoltijogt` (40), `@hellboy17oc` (39).\n",
    "\n",
    "- `@traficogt` concentra ≈ **7.4%** de los tweets y solo **5%** son *replies* → **rol difusor/broadcast**.  \n",
    "- Varias cuentas *top* son **100% replies** (p. ej., `@batallonjalapa`, `@mildred_gaitan`, `@chofito63569841`), lo que sugiere **participación reactiva** (respuesta directa, potencial coordinación/activismo o usuarios muy conversacionales).\n",
    "\n",
    "**Usuarios más mencionados (reciben más atención)**  \n",
    "`@barevalodeleon` (**5,168**), `@traficogt` (**4,232**), `@guatemalagob` (961), `@mpguatemala` (602), `@ubaldomacu` (572), `@santipalomov` (518), `@fjimenezmingob` (411), `@drgiammattei` (318), `@mingobguate` (309), `@congresoguate` (309).\n",
    "\n",
    "- **Concentración**: las 2 cuentas principales acumulan ~**33–34%** de todas las menciones; el **top-10** supera **~48%** → **estructura fuertemente centralizada** alrededor de pocos **hubs** (figuras públicas y cuentas informativas/institucionales).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Dinámica de interacción esperada en la red\n",
    "\n",
    "Dado el predominio de **menciones** y **replies**, el grafo dirigido (usuarios como nodos; “menciona/responde a” como aristas) probablemente muestre:\n",
    "\n",
    "- **Alta indegree** en **@barevalodeleon**, **@traficogt**, **@guatemalagob**, **@mpguatemala** (hubs de atención).  \n",
    "- **Comunidades** articuladas alrededor de **medios/noticieros** y de **cuentas institucionales**; probablemente puentes entre comunidades a través de estas cuentas.  \n",
    "- **Difusión** inferida por menciones/respuestas (no por RTs), pues **no se detectaron retweets nativos** en la recolección.\n",
    "\n",
    "> En el inciso 3.4/5 construiremos el grafo con esas interacciones (menciones + replies) y calcularemos **densidad, diámetro y coeficiente de agrupamiento**, además de centralidades para revelar “poder”/influencia.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Respuestas a las 3 preguntas (4.2)\n",
    "\n",
    "**P1. ¿Quiénes concentran la atención?**  \n",
    "Top menciones: **@barevalodeleon (5,168)** y **@traficogt (4,232)**; les siguen **@guatemalagob** y **@mpguatemala**. La curva es muy concentrada (estructura de hubs).\n",
    "\n",
    "**P2. ¿Quiénes son más activos y qué tan dialogantes?**  \n",
    "`@traficogt` lidera en volumen con bajo % de *replies* (**5%**), típico de **cuenta difusora**. Varias cuentas *top* son **100% replies**, lo que indica **participación reactiva** (interpelación a otros, conversación o campañas de respuesta).\n",
    "\n",
    "**P3. ¿Qué hashtags organizan la conversación?**  \n",
    "Predominio de **etiquetas noticiosas** (#ahora/#urgente), **institucionales** (#guatemalasaleadelante, #minfinsaleadelante) y **temas coyunturales** (#presupuesto2025, consignas). → Los **picos diarios** probablemente se explican por **eventos** asociados a estas etiquetas.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Limitaciones y consideraciones\n",
    "\n",
    "- **Retweets = 0**: la colección/parseo no incluye RTs nativos → la noción de “difusión” se analizará vía **menciones** y **replies**.  \n",
    "- **Cobertura temporal** sesgada a 2023–2024 (picos en 2024).  \n",
    "- **Calidad de texto**: se corrigió **mojibake** y se normalizó UTF-16→UTF-8; limpieza de URLs, emojis, puntuación, *stopwords* y números.  \n",
    "- Se eliminaron **84 duplicados** (≈0.8%).\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Líneas de investigación sugeridas\n",
    "\n",
    "1. **Picos** temporales: aislar días >P95 de actividad y revisar **hashtags/usuarios dominantes**.  \n",
    "2. **Comunidades**: detectar módulos en el grafo (Louvain/Leiden) y perfilar sus **temas/medios**.  \n",
    "3. **Hubs y puentes**: comparar **in-degree/out-degree**, **PageRank** y **betweenness** de cuentas institucionales y figuras públicas.  \n",
    "4. **Conversación dirigida**: medir qué fracción de *replies* se concentra en los **top hubs**.  \n",
    "5. **Estabilidad**: comparar métricas por **mes** para ver **persistencia** de influencia vs. **eventos puntuales**.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "El corpus exhibe una **conversación intensa y dirigida**, con **altas tasas de respuesta** y **concentración de menciones** en pocas cuentas (hubs informativos/institucionales y figuras públicas). La **temporalidad** apunta a eventos coyunturales recientes. El siguiente paso natural es construir el **grafo dirigido** (menciones + replies) y analizar su **topología** (densidad, diámetro, *clustering*) y **centralidades** para evidenciar relaciones de poder e influencia en la red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eca938",
   "metadata": {},
   "source": [
    "## **Inciso 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inciso 5 (5.1 + 5.2) — Topología de la red: construcción, visualización y métricas ===\n",
    "# Requisitos:\n",
    "#   - Haber generado 'eda_outputs/edges_mentions_replies_retweets.csv' en el inciso 4 (Celda A),\n",
    "#     o bien tener 'df_proc' en memoria (reconstruiremos el edge list si hace falta).\n",
    "# Qué hace:\n",
    "#   1) Carga/arma aristas dirigidas (src -> dst) de menciones, replies y retweets (si existieran).\n",
    "#   2) Construye un grafo dirigido con pesos (conteos de interacciones).\n",
    "#   3) Calcula métricas clave: densidad, diámetro (en el mayor componente), coef. de agrupamiento,\n",
    "#      in/out-degree (ponderados y no), PageRank y betweenness (aprox. para escalar).\n",
    "#   4) Detecta comunidades (Louvain si disponible, si no Greedy Modularity) en un subgrafo top-200.\n",
    "#   5) Visualiza el subgrafo (color = comunidad; tamaño = PageRank) y guarda artefactos.\n",
    "#\n",
    "# Al terminar, imprime un bloque: \"=== RESUMEN TOPOLOGÍA (5.1–5.2) ===\"\n",
    "# Envíame ese bloque y te hago el análisis en Markdown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# -----------------------\n",
    "# 0) Cargar el edge list\n",
    "# -----------------------\n",
    "def _norm_user(u):\n",
    "    if u is None or (isinstance(u, float) and np.isnan(u)):\n",
    "        return None\n",
    "    u = str(u).strip()\n",
    "    if not u:\n",
    "        return None\n",
    "    if u.startswith(\"@\"):\n",
    "        u = u[1:]\n",
    "    return u.lower()\n",
    "\n",
    "edges_path = Path(\"eda_outputs/edges_mentions_replies_retweets.csv\")\n",
    "if edges_path.exists():\n",
    "    df_edges = pd.read_csv(edges_path)\n",
    "else:\n",
    "    # Fallback: reconstruir rápidamente desde df_proc si está en memoria\n",
    "    if \"df_proc\" not in globals():\n",
    "        raise RuntimeError(\"No encuentro 'eda_outputs/edges_mentions_replies_retweets.csv' ni 'df_proc'. Vuelve a ejecutar la Celda A del inciso 4.\")\n",
    "    rows = []\n",
    "    for _, row in df_proc.iterrows():\n",
    "        src = _norm_user(row.get(\"username\"))\n",
    "        if not src:\n",
    "            continue\n",
    "        # menciones\n",
    "        for dst in (row.get(\"mentions\") or []):\n",
    "            dst = _norm_user(dst)\n",
    "            if dst:\n",
    "                rows.append((src, dst, \"mention\"))\n",
    "        # reply\n",
    "        dst = _norm_user(row.get(\"reply_to_user\"))\n",
    "        if dst:\n",
    "            rows.append((src, dst, \"reply\"))\n",
    "        # retweet\n",
    "        if bool(row.get(\"is_retweet\")) and row.get(\"retweeted_user\"):\n",
    "            dst = _norm_user(row.get(\"retweeted_user\"))\n",
    "            if dst:\n",
    "                rows.append((src, dst, \"retweet\"))\n",
    "    df_edges = pd.DataFrame(rows, columns=[\"src\",\"dst\",\"type\"])\n",
    "\n",
    "# Normalización básica\n",
    "if \"type\" not in df_edges.columns:\n",
    "    df_edges[\"type\"] = \"mention\"\n",
    "df_edges[\"src\"] = df_edges[\"src\"].apply(_norm_user)\n",
    "df_edges[\"dst\"] = df_edges[\"dst\"].apply(_norm_user)\n",
    "df_edges = df_edges.dropna(subset=[\"src\",\"dst\"])\n",
    "df_edges = df_edges[df_edges[\"src\"] != df_edges[\"dst\"]]  # quita self-loops\n",
    "df_edges[\"type\"] = df_edges[\"type\"].fillna(\"mention\").str.lower()\n",
    "\n",
    "# Conteo por tipo (para el resumen)\n",
    "type_counts = df_edges.groupby(\"type\").size().to_dict()\n",
    "\n",
    "# Agregar pesos por (src,dst,type) y por (src,dst)\n",
    "df_edges_type = (\n",
    "    df_edges\n",
    "    .groupby([\"src\",\"dst\",\"type\"], as_index=False)\n",
    "    .size()\n",
    "    .rename(columns={\"size\":\"weight\"})\n",
    ")\n",
    "df_edges_total = (\n",
    "    df_edges\n",
    "    .groupby([\"src\",\"dst\"], as_index=False)\n",
    "    .size()\n",
    "    .rename(columns={\"size\":\"weight\"})\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1) Construir grafo dirigido con pesos\n",
    "# ---------------------------------------\n",
    "G = nx.DiGraph()\n",
    "for r in df_edges_type.itertuples(index=False):\n",
    "    # Nota: si hay múltiples tipos para el mismo par, añadimos/actualizamos acumulando peso total en 'weight'\n",
    "    if G.has_edge(r.src, r.dst):\n",
    "        G[r.src][r.dst][\"weight\"] = G[r.src][r.dst].get(\"weight\", 0) + r.weight\n",
    "        # Guardamos una lista de tipos para referencia\n",
    "        prev_types = set(G[r.src][r.dst].get(\"types\", []))\n",
    "        prev_types.add(r.type)\n",
    "        G[r.src][r.dst][\"types\"] = list(prev_types)\n",
    "    else:\n",
    "        G.add_edge(r.src, r.dst, weight=r.weight, types=[r.type])\n",
    "\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges()\n",
    "density_dir = nx.density(G)  # m / (n*(n-1))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) Métricas: clustering, diámetro y componentes\n",
    "# ----------------------------------------------------\n",
    "# Usamos versión no dirigida para clustering/diámetro\n",
    "Gu = G.to_undirected()\n",
    "avg_clustering = nx.average_clustering(Gu) if n_nodes > 0 else float(\"nan\")\n",
    "\n",
    "# Diámetro en el mayor componente conexo (no dirigido)\n",
    "diameter_val = None\n",
    "if n_nodes > 1 and n_edges > 0:\n",
    "    cc = list(nx.connected_components(Gu))\n",
    "    if cc:\n",
    "        largest_cc_nodes = max(cc, key=len)\n",
    "        Gcc = Gu.subgraph(largest_cc_nodes).copy()\n",
    "        try:\n",
    "            # Si el componente no es muy grande, cálculo exacto\n",
    "            if Gcc.number_of_nodes() <= 2000:\n",
    "                diameter_val = nx.diameter(Gcc)\n",
    "            else:\n",
    "                # Aproximación para grafos grandes\n",
    "                from networkx.algorithms.approximation.distance_measures import diameter as approx_diameter\n",
    "                diameter_val = approx_diameter(Gcc)\n",
    "        except Exception:\n",
    "            diameter_val = None\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) Centralidades y rankings\n",
    "# ----------------------------------------------------\n",
    "# In/Out-degree (no ponderado y ponderado)\n",
    "in_deg = dict(G.in_degree(weight=None))\n",
    "out_deg = dict(G.out_degree(weight=None))\n",
    "in_deg_w = dict(G.in_degree(weight=\"weight\"))\n",
    "out_deg_w = dict(G.out_degree(weight=\"weight\"))\n",
    "\n",
    "# PageRank (ponderado)\n",
    "pagerank = nx.pagerank(G, alpha=0.85, weight=\"weight\") if n_nodes > 0 else {}\n",
    "\n",
    "# Betweenness aproximado (no dirigido) para escalar mejor\n",
    "k_sample = min(500, max(10, int(n_nodes * 0.2)))  # muestreo de pivotes\n",
    "try:\n",
    "    betw = nx.betweenness_centrality(Gu, k=k_sample, seed=42)\n",
    "except Exception:\n",
    "    betw = {}\n",
    "\n",
    "# DataFrame de métricas por nodo\n",
    "metrics = pd.DataFrame({\"user\": list(G.nodes())})\n",
    "metrics[\"indegree\"] = metrics[\"user\"].map(in_deg).fillna(0).astype(int)\n",
    "metrics[\"outdegree\"] = metrics[\"user\"].map(out_deg).fillna(0).astype(int)\n",
    "metrics[\"indegree_w\"] = metrics[\"user\"].map(in_deg_w).fillna(0).astype(int)\n",
    "metrics[\"outdegree_w\"] = metrics[\"user\"].map(out_deg_w).fillna(0).astype(int)\n",
    "metrics[\"pagerank\"] = metrics[\"user\"].map(pagerank).fillna(0.0)\n",
    "metrics[\"betweenness\"] = metrics[\"user\"].map(betw).fillna(0.0)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) Comunidades y visualización (subgrafo top-200)\n",
    "# ----------------------------------------------------\n",
    "topN = 200\n",
    "top_nodes = (\n",
    "    metrics.sort_values([\"indegree_w\",\"pagerank\"], ascending=False)\n",
    "           .head(topN)[\"user\"]\n",
    "           .tolist()\n",
    ")\n",
    "H = G.subgraph(top_nodes).copy()\n",
    "Hu = H.to_undirected()\n",
    "\n",
    "# Detección de comunidades\n",
    "try:\n",
    "    from networkx.algorithms.community import louvain_communities\n",
    "    comms = louvain_communities(Hu, seed=42)\n",
    "except Exception:\n",
    "    from networkx.algorithms.community import greedy_modularity_communities\n",
    "    comms = list(greedy_modularity_communities(Hu))\n",
    "\n",
    "comm_map = {}\n",
    "for cid, com in enumerate(comms):\n",
    "    for n in com:\n",
    "        comm_map[n] = cid\n",
    "\n",
    "# Layout y dibujo\n",
    "pos = nx.spring_layout(Hu, seed=42)\n",
    "node_sizes = []\n",
    "for n in Hu.nodes():\n",
    "    pr = metrics.loc[metrics[\"user\"] == n, \"pagerank\"]\n",
    "    pr = pr.values[0] if len(pr) else 0.0\n",
    "    node_sizes.append(200 + 8000 * pr)  # tamaño ~ PageRank\n",
    "\n",
    "node_colors = [comm_map.get(n, -1) for n in Hu.nodes()]\n",
    "edge_widths = [max(0.4, min(4.0, H[u][v].get(\"weight\",1) / 5.0)) for u,v in H.edges()]\n",
    "\n",
    "out_dir = Path(\"net_outputs\"); out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "nx.draw_networkx_edges(Hu, pos, alpha=0.15, width=edge_widths)\n",
    "nodes = nx.draw_networkx_nodes(Hu, pos, node_size=node_sizes, node_color=node_colors, cmap=plt.cm.tab20, alpha=0.9)\n",
    "# Etiquetas solo para los 20 nodos con mayor indegree ponderado\n",
    "label_nodes = (\n",
    "    metrics.sort_values(\"indegree_w\", ascending=False)\n",
    "           .head(20)[\"user\"].tolist()\n",
    ")\n",
    "nx.draw_networkx_labels(Hu, pos, labels={n:n for n in Hu.nodes() if n in label_nodes}, font_size=9)\n",
    "plt.title(\"Grafo de interacciones (subgrafo top-200 por in-degree ponderado)\\nColor=Comunidad  |  Tamaño=PageRank  |  Arista=Peso\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"graph_top200.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5) Guardar artefactos y reportar resumen\n",
    "# ----------------------------------------------------\n",
    "metrics.sort_values(\"indegree_w\", ascending=False).to_csv(out_dir / \"node_metrics.csv\", index=False)\n",
    "df_edges_total.to_csv(out_dir / \"edge_weights.csv\", index=False)\n",
    "\n",
    "print(\"=== RESUMEN TOPOLOGÍA (5.1–5.2) ===\")\n",
    "print(f\"Nodos: {n_nodes:,}  |  Aristas (dirigidas): {n_edges:,}  |  Densidad (dirigida): {density_dir:.6f}\")\n",
    "print(f\"Coeficiente de agrupamiento (promedio, no dirigido): {avg_clustering:.4f}\")\n",
    "print(\"Diámetro (en el mayor componente no dirigido):\", \"N/A\" if diameter_val is None else diameter_val)\n",
    "print(\"Interacciones por tipo:\", type_counts)\n",
    "\n",
    "print(\"\\nTop 10 por in-degree ponderado:\")\n",
    "print(metrics.sort_values(\"indegree_w\", ascending=False).head(10)[[\"user\",\"indegree_w\",\"pagerank\",\"betweenness\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 por PageRank:\")\n",
    "print(metrics.sort_values(\"pagerank\", ascending=False).head(10)[[\"user\",\"pagerank\",\"indegree_w\",\"outdegree_w\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 por betweenness (aprox, no dirigido):\")\n",
    "print(metrics.sort_values(\"betweenness\", ascending=False).head(10)[[\"user\",\"betweenness\",\"indegree_w\",\"outdegree_w\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nArtefactos guardados en:\", str(out_dir.resolve()))\n",
    "print(\" - net_outputs/graph_top200.png\")\n",
    "print(\" - net_outputs/node_metrics.csv\")\n",
    "print(\" - net_outputs/edge_weights.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0faedfb",
   "metadata": {},
   "source": [
    "# Inciso 5 — Análisis de la topología de la red (menciones + respuestas)\n",
    "\n",
    "> **Datos del grafo**  \n",
    "> Nodos: **5,213** · Aristas dirigidas: **19,476** · **Densidad**: 0.000717  \n",
    "> **Coef. de agrupamiento (promedio, no dirigido)**: **0.3129** · **Diámetro** (mayor componente): **7**  \n",
    "> **Interacciones por tipo**: menciones = **28,035**, respuestas = **8,409** (RT nativos no presentes en la muestra)\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 Construcción y visualización de grafos\n",
    "\n",
    "- La visualización del **subgrafo top-200 por in-degree ponderado** muestra un **núcleo denso** donde se concentran las interacciones y múltiples **comunidades** (colores distintos).  \n",
    "  - Nodos etiquetados visibles: `barevalodeleon`, `traficogt`, `guatemalagob`, `mpguatemala`, etc.  \n",
    "  - Los **nodos periféricos** alrededor del dibujo aparecen aislados porque, aunque reciben muchas interacciones desde *fuera* del top-200, **no forman lazos fuertes entre sí** dentro de este subgrafo.\n",
    "- **Baja densidad (0.000717)**: típica de redes de mención/respuesta a gran escala (sparsas) donde pocos pares se conectan directamente.\n",
    "- **Agrupamiento medio alto (0.313)**: sugiere **formación de clústeres**/comunidades locales (p. ej., cuentas institucionales, medios, activismo).\n",
    "- **Diámetro = 7** sobre el mayor componente: propiedad **“small-world”** — la mayoría de los nodos se conectan en pocos saltos a través de hubs.\n",
    "\n",
    "**Lectura de poder / visibilidad en la figura**\n",
    "- El **tamaño** (PageRank) y la **posición central** del núcleo resaltan a **@barevalodeleon** y **@traficogt** como focos de atención.  \n",
    "- La mezcla de colores cerca del núcleo indica que **varias comunidades** están **acopladas** por estos hubs (actúan como puntos de convergencia entre grupos temáticos).\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 Métricas de red clave e interpretación\n",
    "\n",
    "### Hubs de atención (in-degree ponderado)\n",
    "Top por **indegree_w**:\n",
    "1. **@barevalodeleon** (5,917) — PR **0.1305**, betweenness **0.5678**, **outdegree_w = 0**  \n",
    "2. **@traficogt** (5,622) — PR **0.0719**, betweenness **0.4770**, **outdegree_w = 150**  \n",
    "3. **@guatemalagob** (1,171) — PR **0.0177**  \n",
    "4. **@ubaldomacu** (1,022) — PR **0.0088**  \n",
    "5. **@mpguatemala** (605) — PR **0.0109**\n",
    "\n",
    "**Insight**  \n",
    "- **@barevalodeleon** y **@traficogt** concentran **gran parte de las menciones/respuestas**.  \n",
    "  - `@barevalodeleon` domina en **in-degree** y **PageRank** aun con **cero salida**, típico de **“sumidero” de atención** (muchos hablan *a* él/ella, casi no responde).  \n",
    "  - `@traficogt` combina **alta atención** con **actividad de salida** (outdegree_w=150), perfil de **difusor** que además conecta comunidades.\n",
    "\n",
    "### Influencia estructural (PageRank y betweenness)\n",
    "Top por **PageRank** incluye, además de los dos anteriores, cuentas **institucionales**:\n",
    "- `@guatemalagob`, `@drgiammattei`, `@mpguatemala`, `@cc_guatemala`, `@usaidguate`, `@ivanduque`.  \n",
    "→ Indica que, aun sin ser los más mencionados, **canalizan flujos de interacción** relevantes en el grafo.\n",
    "\n",
    "Top por **betweenness** (puentes):\n",
    "- Además de `@barevalodeleon` y `@traficogt`, destacan **@mildred_gaitan** y **@batallonjalapa** con **betweenness alto** pero **indegree bajo** y **outdegree muy alto** (398 y 461).  \n",
    "→ **Nodos “conectores”**: responden a muchos y **unen clusters** que de otro modo quedarían separados (posible activismo/brigadas de respuesta, cuentas muy conversacionales o coordinadores).\n",
    "\n",
    "### Conclusión de métricas\n",
    "- La red está **altamente centralizada** en dos hubs principales, con **comunidades bien definidas** alrededor de **instituciones** y **figuras públicas**.  \n",
    "- La combinación de **clustering elevado** y **diámetro corto** confirma un **patrón small-world**: pocos nodos estratégicos bastan para conectar a la mayoría.  \n",
    "- La ausencia de RT nativos en los datos **desplaza la noción de difusión** hacia **menciones y respuestas**; aun así, los **hubs y conectores** emergen con claridad.\n",
    "\n",
    "---\n",
    "\n",
    "## Recomendaciones de profundización\n",
    "\n",
    "1. **Comunidades**: cuantificar tamaños y etiquetarlas por **hashtags/tokens predominantes** para perfilar temas (Louvain/Leiden).  \n",
    "2. **Roles**: contrastar **indegree_w vs. outdegree_w** para clasificar **sumideros de atención**, **difusores** y **conectores**.  \n",
    "3. **Evolución**: repetir métricas por **ventanas temporales** (mensual/semanal) para detectar **picos** y **cambios de liderazgo**.  \n",
    "4. **Robustez**: si se dispone de RTs nativos en otra extracción, incorporar **retweets** para medir **alcance** y **cascadeo**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69637ad1",
   "metadata": {},
   "source": [
    "# Inciso 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5df0e",
   "metadata": {},
   "source": [
    "## 6.1. Identificación y análisis de comunidades (elección del algoritmo)\n",
    "\n",
    "### ¿Qué algoritmo usar?\n",
    "\n",
    "El **método Louvain** (Blondel et al., 2008) es el estándar histórico más difundido para detección de comunidades por su rapidez y calidad al optimizar modularidad; aparece en reseñas clásicas y está disponible en librerías comunes (e.g., *NetworkX*), lo que facilita la reproducibilidad.  \n",
    "\n",
    "**Referencias:**\n",
    "- [arXiv](https://arxiv.org/)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 — Detección de comunidades con Louvain (y fallback a Greedy Modularity)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ---------- 1) Localizar el archivo de aristas ----------\n",
    "CANDIDATES = [\n",
    "    \"net_outputs/edge_weights.csv\",                  # si el notebook está en lab6ds/\n",
    "    \"../net_outputs/edge_weights.csv\",               # si el notebook está en lab6ds/notebooks/\n",
    "    \"lab6ds/net_outputs/edge_weights.csv\",           # si ejecutas desde la raíz del zip\n",
    "    \"./edge_weights.csv\",                            # último recurso (por si moviste el archivo)\n",
    "]\n",
    "\n",
    "edges_path = None\n",
    "for p in CANDIDATES:\n",
    "    if os.path.exists(p):\n",
    "        edges_path = p\n",
    "        break\n",
    "\n",
    "if edges_path is None:\n",
    "    raise FileNotFoundError(\"No se encontró 'edge_weights.csv'. Ajusta la ruta en CANDIDATES.\")\n",
    "\n",
    "print(f\"Usando archivo de aristas: {edges_path}\")\n",
    "\n",
    "# ---------- 2) Cargar aristas y construir el grafo dirigido ----------\n",
    "edges = pd.read_csv(edges_path)\n",
    "edges.columns = [c.strip().lower() for c in edges.columns]\n",
    "\n",
    "# Aceptamos nombres de columnas comunes\n",
    "col_src = \"src\" if \"src\" in edges.columns else (\"source\" if \"source\" in edges.columns else None)\n",
    "col_dst = \"dst\" if \"dst\" in edges.columns else (\"target\" if \"target\" in edges.columns else None)\n",
    "col_w  = \"weight\" if \"weight\" in edges.columns else None\n",
    "\n",
    "assert col_src and col_dst and col_w, f\"Se esperaban columnas tipo src/dst/weight; columnas reales: {edges.columns.tolist()}\"\n",
    "\n",
    "G_directed = nx.DiGraph()\n",
    "for row in edges.itertuples(index=False):\n",
    "    G_directed.add_edge(getattr(row, col_src), getattr(row, col_dst), weight=float(getattr(row, col_w)))\n",
    "\n",
    "print(f\"Grafo dirigido: {G_directed.number_of_nodes()} nodos, {G_directed.number_of_edges()} aristas\")\n",
    "\n",
    "# ---------- 3) Proyección no dirigida ponderada ----------\n",
    "G_undirected = nx.Graph()\n",
    "for u, v, d in G_directed.edges(data=True):\n",
    "    w = float(d.get(\"weight\", 1.0))\n",
    "    if G_undirected.has_edge(u, v):\n",
    "        G_undirected[u][v][\"weight\"] += w\n",
    "    else:\n",
    "        G_undirected.add_edge(u, v, weight=w)\n",
    "\n",
    "print(f\"Grafo NO dirigido para comunidades: {G_undirected.number_of_nodes()} nodos, {G_undirected.number_of_edges()} aristas\")\n",
    "\n",
    "# ---------- 4) Ejecutar Louvain (o fallback) ----------\n",
    "algo_used = None\n",
    "communities_list = None\n",
    "\n",
    "try:\n",
    "    # NetworkX >= 2.6: louvain_communities\n",
    "    from networkx.algorithms.community import louvain_communities\n",
    "    communities_list = louvain_communities(G_undirected, weight=\"weight\", resolution=1.0, seed=42)\n",
    "    algo_used = \"Louvain (NetworkX)\"\n",
    "except Exception as e:\n",
    "    # Fallback: greedy_modularity_communities\n",
    "    from networkx.algorithms.community import greedy_modularity_communities\n",
    "    communities_list = greedy_modularity_communities(G_undirected, weight=\"weight\")\n",
    "    algo_used = \"Greedy Modularity (fallback)\"\n",
    "    print(f\"[Aviso] Louvain no disponible ({e}). Usando {algo_used}.\")\n",
    "\n",
    "# communities_list es una lista de 'sets' de nodos\n",
    "num_com = len(communities_list)\n",
    "sizes = sorted([len(c) for c in communities_list], reverse=True)\n",
    "print(f\"Algoritmo: {algo_used}. Comunidades detectadas: {num_com}\")\n",
    "print(\"Tamaños de las 10 comunidades más grandes:\", sizes[:10])\n",
    "\n",
    "# ---------- 5) Asignación nodo -> comunidad y resumen ----------\n",
    "node2community = {}\n",
    "for cid, com in enumerate(communities_list):\n",
    "    for n in com:\n",
    "        node2community[n] = cid\n",
    "\n",
    "communities_df = pd.DataFrame(\n",
    "    [{\"user\": n, \"community\": node2community[n]} for n in G_undirected.nodes()]\n",
    ").sort_values([\"community\",\"user\"]).reset_index(drop=True)\n",
    "\n",
    "sizes_df = (\n",
    "    communities_df.groupby(\"community\", as_index=False)\n",
    "    .size()\n",
    "    .rename(columns={\"size\": \"size_nodes\"})\n",
    "    .sort_values(\"size_nodes\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ---------- 6) Moduralidad (calidad de la partición) ----------\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "mod_score = modularity(G_undirected, communities_list, weight=\"weight\")\n",
    "\n",
    "print(f\"Modularidad de la partición: {mod_score:.4f}\")\n",
    "\n",
    "# ---------- 7) Mostrar cabezales útiles ----------\n",
    "display(sizes_df.head(10))\n",
    "display(communities_df.head(10))\n",
    "\n",
    "# Objetos que usaremos en 6.2/6.3\n",
    "RESULTS_61 = {\n",
    "    \"G_directed\": G_directed,\n",
    "    \"G_undirected\": G_undirected,\n",
    "    \"communities_list\": communities_list,   # lista de sets\n",
    "    \"node2community\": node2community,       # dict user -> comunidad\n",
    "    \"communities_df\": communities_df,       # tabla asignación\n",
    "    \"sizes_df\": sizes_df,                   # tamaños\n",
    "    \"modularity\": mod_score,\n",
    "    \"algo_used\": algo_used,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45edf546",
   "metadata": {},
   "source": [
    "## 6.2. Visualización y caracterización de comunidades\n",
    "\n",
    "1. Partimos de los resultados del **6.1** (`RESULTS_61` con `G_directed`, `G_undirected`, `communities_list`, `node2community`, etc.).  \n",
    "2. Hacemos una **vista general** (subgrafo con los nodos de mayor grado para legibilidad), coloreando por comunidad.  \n",
    "3. Graficamos solo las **3 comunidades más grandes**, dimensionando nodos por **PageRank** (en el subgrafo dirigido de esas comunidades) para sugerir influencia.  \n",
    "4. Calculamos, por comunidad:  \n",
    "   - `size_nodes`  \n",
    "   - Aristas internas dirigidas (conteo y peso)  \n",
    "   - Aristas externas (entrantes/salientes y sus pesos)  \n",
    "   - Densidad (subgrafo no dirigido)  \n",
    "5. Top-5 usuarios por **indegree local ponderado** (nodos más citados/etiquetados dentro de su comunidad).  \n",
    "6. Tomamos tokens de `tweets_clean_sample.csv` y listamos **términos frecuentes** por comunidad (aprox.) para las **Top-3**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2.A — Visualización de comunidades\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== 1) Recuperar resultados del 6.1 o reconstruir mínimo si no existen ====\n",
    "def _rebuild_from_edges():\n",
    "    CANDIDATES = [\n",
    "        \"net_outputs/edge_weights.csv\",\n",
    "        \"../net_outputs/edge_weights.csv\",\n",
    "        \"lab6ds/net_outputs/edge_weights.csv\",\n",
    "        \"./edge_weights.csv\",\n",
    "    ]\n",
    "    edges_path = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
    "    if edges_path is None:\n",
    "        raise FileNotFoundError(\"No se encontró edge_weights.csv; ajusta rutas en CANDIDATES.\")\n",
    "\n",
    "    edges = pd.read_csv(edges_path)\n",
    "    edges.columns = [c.strip().lower() for c in edges.columns]\n",
    "    col_src = \"src\" if \"src\" in edges.columns else (\"source\" if \"source\" in edges.columns else None)\n",
    "    col_dst = \"dst\" if \"dst\" in edges.columns else (\"target\" if \"target\" in edges.columns else None)\n",
    "    col_w  = \"weight\" if \"weight\" in edges.columns else None\n",
    "    assert col_src and col_dst and col_w, f\"Columnas esperadas tipo src/dst/weight; reales: {edges.columns.tolist()}\"\n",
    "\n",
    "    Gd = nx.DiGraph()\n",
    "    for r in edges.itertuples(index=False):\n",
    "        Gd.add_edge(getattr(r, col_src), getattr(r, col_dst), weight=float(getattr(r, col_w)))\n",
    "    Gu = nx.Graph()\n",
    "    for u, v, d in Gd.edges(data=True):\n",
    "        w = float(d.get(\"weight\", 1.0))\n",
    "        if Gu.has_edge(u, v):\n",
    "            Gu[u][v][\"weight\"] += w\n",
    "        else:\n",
    "            Gu.add_edge(u, v, weight=w)\n",
    "\n",
    "    from networkx.algorithms.community import louvain_communities\n",
    "    comms = louvain_communities(Gu, weight=\"weight\", seed=42)\n",
    "    node2com = {}\n",
    "    for cid, com in enumerate(comms):\n",
    "        for n in com:\n",
    "            node2com[n] = cid\n",
    "    sizes_df = (\n",
    "        pd.DataFrame({\"user\": list(node2com.keys()), \"community\": list(node2com.values())})\n",
    "        .groupby(\"community\", as_index=False).size()\n",
    "        .rename(columns={\"size\": \"size_nodes\"})\n",
    "        .sort_values(\"size_nodes\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return Gd, Gu, comms, node2com, sizes_df\n",
    "\n",
    "try:\n",
    "    Gu = RESULTS_61[\"G_undirected\"]\n",
    "    Gd = RESULTS_61[\"G_directed\"]\n",
    "    communities_list = RESULTS_61[\"communities_list\"]\n",
    "    node2community = RESULTS_61[\"node2community\"]\n",
    "    sizes_df = RESULTS_61[\"sizes_df\"]\n",
    "except NameError:\n",
    "    Gd, Gu, communities_list, node2community, sizes_df = _rebuild_from_edges()\n",
    "\n",
    "# ==== 2) Vista general (subgrafo con TOPN nodos por grado) ====\n",
    "TOPN = 300  # ajusta si quieres más/menos nodos\n",
    "deg = dict(Gu.degree())\n",
    "top_nodes = sorted(deg, key=deg.get, reverse=True)[:min(TOPN, Gu.number_of_nodes())]\n",
    "H = Gu.subgraph(top_nodes).copy()\n",
    "\n",
    "pos = nx.spring_layout(H, seed=42, iterations=50)\n",
    "node_colors = [node2community.get(n, -1) for n in H.nodes()]  # un entero por comunidad\n",
    "node_sizes = [10 + 2*deg.get(n, 1) for n in H.nodes()]        # tamaño proporcional al grado (suave)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw_networkx_nodes(H, pos, node_size=node_sizes, node_color=node_colors)\n",
    "nx.draw_networkx_edges(H, pos, alpha=0.25, width=0.6)\n",
    "plt.title(\"Comunidades — vista general (TOPN nodos por grado)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ==== 3) Gráfico con las 3 comunidades más grandes ====\n",
    "top3_ids = sizes_df.sort_values(\"size_nodes\", ascending=False).head(3)[\"community\"].tolist()\n",
    "nodes_top3 = [n for n,cid in node2community.items() if cid in top3_ids]\n",
    "H3_u = Gu.subgraph(nodes_top3).copy()\n",
    "H3_d = Gd.subgraph(nodes_top3).copy()\n",
    "\n",
    "# PageRank (dirigido; ponderado por 'weight') para destacar influencia en el gráfico\n",
    "pr = nx.pagerank(H3_d, alpha=0.85, weight=\"weight\", max_iter=100)\n",
    "pr_min, pr_max = (min(pr.values()), max(pr.values())) if pr else (0, 1)\n",
    "\n",
    "# tamaños escalados por PageRank (evitar casos extremos)\n",
    "def scale_size(x, xmin, xmax, smin=20, smax=300):\n",
    "    if xmax == xmin:\n",
    "        return (smin + smax) / 2.0\n",
    "    return smin + (smax - smin) * ((x - xmin) / (xmax - xmin))\n",
    "\n",
    "sizes_pr = [scale_size(pr.get(n, 0), pr_min, pr_max, smin=20, smax=300) for n in H3_u.nodes()]\n",
    "colors_3 = [node2community.get(n, -1) for n in H3_u.nodes()]\n",
    "\n",
    "pos3 = nx.spring_layout(H3_u, seed=123, iterations=60)\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw_networkx_nodes(H3_u, pos3, node_size=sizes_pr, node_color=colors_3)\n",
    "nx.draw_networkx_edges(H3_u, pos3, alpha=0.2, width=0.5)\n",
    "plt.title(\"Top 3 comunidades — nodos dimensionados por PageRank (subgrafo dirigido)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"IDs de las 3 comunidades más grandes:\", top3_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2.B — Caracterización por comunidad: tamaño, interacciones, top usuarios y temas (aprox.)\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "# ==== 1) Tablas de interacción por comunidad ====\n",
    "# - internal_edges_*: aristas con (u,v) dentro de la comunidad (conteo y suma de pesos)\n",
    "# - outgoing_*: desde la comunidad hacia fuera\n",
    "# - incoming_*: desde fuera hacia la comunidad\n",
    "# - density_undirected: densidad en subgrafo no dirigido\n",
    "\n",
    "rows = []\n",
    "node2community = dict(node2community)  # asegurar dict normal\n",
    "for cid, com in enumerate(communities_list):\n",
    "    com = set(com)\n",
    "    # Subgrafos\n",
    "    sub_u = Gu.subgraph(com).copy()\n",
    "\n",
    "    # Contadores\n",
    "    internal_count = 0\n",
    "    outgoing_count = 0\n",
    "    incoming_count = 0\n",
    "    internal_w = 0.0\n",
    "    outgoing_w = 0.0\n",
    "    incoming_w = 0.0\n",
    "\n",
    "    # Recorremos aristas dirigidas del grafo completo\n",
    "    for u, v, d in Gd.edges(data=True):\n",
    "        w = float(d.get(\"weight\", 1.0))\n",
    "        u_in, v_in = u in com, v in com\n",
    "        if u_in and v_in:\n",
    "            internal_count += 1\n",
    "            internal_w += w\n",
    "        elif u_in and not v_in:\n",
    "            outgoing_count += 1\n",
    "            outgoing_w += w\n",
    "        elif (not u_in) and v_in:\n",
    "            incoming_count += 1\n",
    "            incoming_w += w\n",
    "\n",
    "    # indegree local ponderado (quién recibe más peso desde su propia comunidad)\n",
    "    indeg_local = {}\n",
    "    for u, v, d in Gd.in_edges(list(com), data=True):\n",
    "        if u in com and v in com:\n",
    "            indeg_local[v] = indeg_local.get(v, 0.0) + float(d.get(\"weight\", 1.0))\n",
    "    top5_local = sorted(indeg_local.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "    rows.append({\n",
    "        \"community\": cid,\n",
    "        \"size_nodes\": len(com),\n",
    "        \"density_undirected\": nx.density(sub_u),\n",
    "        \"internal_edges_count\": internal_count,\n",
    "        \"internal_edges_weight\": internal_w,\n",
    "        \"outgoing_edges_count\": outgoing_count,\n",
    "        \"outgoing_edges_weight\": outgoing_w,\n",
    "        \"incoming_edges_count\": incoming_count,\n",
    "        \"incoming_edges_weight\": incoming_w,\n",
    "        \"top5_users_by_local_indegree\": \"; \".join([f\"{u}:{w:.0f}\" for u,w in top5_local]),\n",
    "    })\n",
    "\n",
    "char_df = pd.DataFrame(rows).sort_values(\"size_nodes\", ascending=False).reset_index(drop=True)\n",
    "display(char_df.head(10))\n",
    "\n",
    "# ==== 2) Temas principales (aprox.) por comunidad usando tokens de la muestra limpia ====\n",
    "# Intentamos localizar tweets_clean_sample.csv:\n",
    "CANDS = [\n",
    "    \"tweets_clean_sample.csv\",\n",
    "    \"../tweets_clean_sample.csv\",\n",
    "    \"lab6ds/tweets_clean_sample.csv\",\n",
    "]\n",
    "clean_path = next((p for p in CANDS if os.path.exists(p)), None)\n",
    "if clean_path is None:\n",
    "    print(\"[Aviso] No se encontró tweets_clean_sample.csv; omitiendo temas.\")\n",
    "else:\n",
    "    clean = pd.read_csv(clean_path)\n",
    "    # Normalizamos username para cruzar\n",
    "    clean[\"username_norm\"] = clean[\"username\"].astype(str).str.lower()\n",
    "    # Lista de tokens\n",
    "    def parse_list(s):\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "    clean[\"tokens_list\"] = clean[\"tokens_nostop\"].astype(str).apply(parse_list)\n",
    "\n",
    "    # Mapeo user -> comunidad\n",
    "    u2c = pd.DataFrame({\"user\": list(node2community.keys()),\n",
    "                        \"community\": list(node2community.values())})\n",
    "    u2c[\"user\"] = u2c[\"user\"].astype(str).str.lower()\n",
    "\n",
    "    merged = clean.merge(u2c, left_on=\"username_norm\", right_on=\"user\", how=\"left\").dropna(subset=[\"community\"])\n",
    "\n",
    "    # Top-3 comunidades por tamaño\n",
    "    top3_ids = char_df.head(3)[\"community\"].tolist()\n",
    "\n",
    "    topic_rows = []\n",
    "    for cid in top3_ids:\n",
    "        toks = []\n",
    "        for lst in merged.loc[merged[\"community\"]==cid, \"tokens_list\"]:\n",
    "            toks.extend([t for t in lst if isinstance(t, str)])\n",
    "        # Conteo y filtrito mínimo\n",
    "        cnt = Counter(toks)\n",
    "        common = [f\"{w}:{c}\" for w,c in cnt.most_common(15)]\n",
    "        topic_rows.append({\"community\": cid, \"top_terms\": \", \".join(common)})\n",
    "\n",
    "    topics_df = pd.DataFrame(topic_rows)\n",
    "    display(topics_df)\n",
    "\n",
    "# Guardamos resultados en variable para próximos incisos\n",
    "RESULTS_62 = {\n",
    "    \"char_df\": char_df,\n",
    "    \"topics_df\": topics_df if clean_path is not None else None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230b02e",
   "metadata": {},
   "source": [
    "# 6.2. Caracterización de comunidades\n",
    "\n",
    "## Resumen de resultados\n",
    "\n",
    "Se detectaron comunidades de distintos tamaños, siendo las tres más grandes:\n",
    "\n",
    "- Comunidad 3: 1,635 nodos  \n",
    "- Comunidad 38: 1,182 nodos  \n",
    "- Comunidad 33: 326 nodos  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Tamaño y densidad\n",
    "\n",
    "- Comunidad 3: Es la más grande (1,635 nodos) pero poco densa (0.0026), lo que muestra un grupo muy amplio pero disperso.  \n",
    "- Comunidad 38: También es grande (1,182 nodos) y con una densidad un poco mayor (0.0038).  \n",
    "- Comunidad 33: Es más pequeña (326 nodos) pero mucho más densa (0.0269), lo que indica un grupo más cohesionado.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Interacciones internas y externas\n",
    "\n",
    "- Comunidad 3: Tiene 3,548 conexiones internas (peso total 8,664) y también muchas conexiones externas, tanto entrantes como salientes (aprox. 1,400 cada una). Esto refleja un grupo activo y con muchas interacciones hacia afuera.  \n",
    "- Comunidad 38: Presenta 2,686 conexiones internas (peso total 5,830), pero destaca por recibir un gran número de conexiones externas (2,389 con peso 4,410). Esto sugiere que es un grupo que recibe bastante atención de otros.  \n",
    "- Comunidad 33: A pesar de ser más pequeña, tiene una alta proporción de conexiones internas (1,443 con peso 2,500) frente a las externas, lo que confirma que es un grupo muy cohesionado.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Usuarios más influyentes\n",
    "\n",
    "- Comunidad 3: Destacan cuentas como *traficogt* (4,193), *prensacomunitar* y *lahoragt*, principalmente medios de comunicación y cuentas relacionadas al tráfico.  \n",
    "- Comunidad 38: Sobresalen *barevalodeleon* (2,691), *ubaldomacu* y *santipalomino*, en su mayoría líderes de opinión y cuentas personales.  \n",
    "- Comunidad 33: Los más influyentes son *guatemalagob* (480), *fjimenezmingob* y *diariodeca*, relacionados con instituciones oficiales y el gobierno.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Temas principales de conversación\n",
    "\n",
    "- Comunidad 3: Predominan palabras como *traficogt*, *gt* y *quorumgt*, con un enfoque en tráfico y noticias locales.  \n",
    "- Comunidad 38: Aparecen términos como *género*, *investigación* y *fiscal*, vinculados a justicia, género y temas sociales o políticos.  \n",
    "- Comunidad 33: Se observan términos como *guatemalagob*, *lionelgaliano* y *landivarianos*, relacionados con el gobierno y la política institucional.  \n",
    "\n",
    "---\n",
    "\n",
    "## Interpretación general\n",
    "\n",
    "- La comunidad más grande (3) reúne principalmente medios y cuentas sobre tráfico y noticias cotidianas.  \n",
    "- La comunidad 38 concentra periodistas y líderes de opinión, conectándose con muchas otras comunidades de la red.  \n",
    "- La comunidad 33 corresponde a un bloque institucional más cerrado, asociado a cuentas oficiales y al discurso gubernamental.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb9142",
   "metadata": {},
   "source": [
    "# Inciso 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357afe3",
   "metadata": {},
   "source": [
    "# 7.1. Identificación de usuarios influyentes \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Centralidad de grado\n",
    "\n",
    "- **Qué mide:** cuántas conexiones directas tiene un usuario.  \n",
    "- **Cómo interpretarlo:**  \n",
    "  - *In-degree*: cuántas veces mencionan o retuitean a un usuario → indica recepción de atención.  \n",
    "  - *Out-degree*: cuántas veces un usuario menciona a otros → indica su nivel de actividad.  \n",
    "- **Cómo lo usamos:**  \n",
    "  - Se utilizan las versiones ponderadas (que consideran la frecuencia de las interacciones).  \n",
    "  - Como resumen, se puede sumar ambos valores: `grado total = in-degree + out-degree`.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Centralidad de intermediación \n",
    "\n",
    "- **Qué mide:** cuántas rutas de comunicación “pasan por” un usuario.  \n",
    "- **Cómo interpretarlo:** ayuda a identificar cuentas que actúan como puentes entre comunidades o subgrupos distintos.  \n",
    "- **Cómo lo usamos:**  \n",
    "  - Para reducir el costo computacional, si no existe un cálculo previo, se utiliza una aproximación con muestreo sobre la red no dirigida.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Centralidad de cercanía\n",
    "\n",
    "- **Qué mide:** qué tan cerca está un usuario de todos los demás en la red (en promedio).  \n",
    "- **Cómo interpretarlo:** los usuarios con alta cercanía pueden llegar más rápido al resto de la red, por lo que son importantes para la difusión de información.  \n",
    "- **Cómo lo usamos:**  \n",
    "  - Se calcula sobre el componente más grande de la red no dirigida.  \n",
    "  - Para tener en cuenta los pesos (más interacción = mayor cercanía), la distancia se define como `1 / peso`.  \n",
    "\n",
    "---\n",
    "\n",
    "## Notas de implementación\n",
    "\n",
    "- Se parte de los grafos construidos en el paso 6.1. Si no están disponibles, se reconstruyen a partir del archivo `edge_weights.csv`.  \n",
    "- Se calculan:  \n",
    "  - Grado en el grafo dirigido (conteos y pesos).  \n",
    "  - Intermediación aproximada en el grafo no dirigido.  \n",
    "  - Cercanía en el componente más grande del grafo no dirigido, considerando los pesos.  \n",
    "- Se mostrarán los **15 usuarios principales** en cada medida.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9311d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 — Influencers y nodos clave con tres centralidades:\n",
    "#   - Grado (dirigido, ponderado)\n",
    "#   - Intermediación/Betweenness (no dirigido, con distancia = 1/weight)\n",
    "#   - Cercanía/Closeness (no dirigido, LCC, con distancia = 1/weight)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ============== Utilidades ==============\n",
    "def _load_graphs_from_61_or_rebuild():\n",
    "    \"\"\"Intentar tomar Gd/Gu de RESULTS_61; si no existen, reconstruir desde edge_weights.csv.\"\"\"\n",
    "    try:\n",
    "        Gd = RESULTS_61[\"G_directed\"]\n",
    "        Gu = RESULTS_61[\"G_undirected\"]\n",
    "        return Gd, Gu\n",
    "    except NameError:\n",
    "        pass  # no existe RESULTS_61\n",
    "\n",
    "    # Reconstruir desde edge_weights.csv\n",
    "    CANDIDATES = [\n",
    "        \"net_outputs/edge_weights.csv\",\n",
    "        \"../net_outputs/edge_weights.csv\",\n",
    "        \"lab6ds/net_outputs/edge_weights.csv\",\n",
    "        \"./edge_weights.csv\",\n",
    "    ]\n",
    "    edges_path = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
    "    if edges_path is None:\n",
    "        raise FileNotFoundError(\"No se encontró edge_weights.csv; ajusta rutas en CANDIDATES.\")\n",
    "\n",
    "    edges = pd.read_csv(edges_path)\n",
    "    edges.columns = [c.strip().lower() for c in edges.columns]\n",
    "    col_src = \"src\" if \"src\" in edges.columns else (\"source\" if \"source\" in edges.columns else None)\n",
    "    col_dst = \"dst\" if \"dst\" in edges.columns else (\"target\" if \"target\" in edges.columns else None)\n",
    "    col_w  = \"weight\" if \"weight\" in edges.columns else None\n",
    "    assert col_src and col_dst and col_w, f\"Columnas esperadas tipo src/dst/weight; reales: {edges.columns.tolist()}\"\n",
    "\n",
    "    Gd = nx.DiGraph()\n",
    "    for r in edges.itertuples(index=False):\n",
    "        Gd.add_edge(getattr(r, col_src), getattr(r, col_dst), weight=float(getattr(r, col_w)))\n",
    "\n",
    "    # no dirigido (sumando pesos)\n",
    "    Gu = nx.Graph()\n",
    "    for u, v, d in Gd.edges(data=True):\n",
    "        w = float(d.get(\"weight\", 1.0))\n",
    "        if Gu.has_edge(u, v):\n",
    "            Gu[u][v][\"weight\"] += w\n",
    "        else:\n",
    "            Gu.add_edge(u, v, weight=w)\n",
    "    return Gd, Gu\n",
    "\n",
    "def _ensure_distance_attr(Gu):\n",
    "    \"\"\"Crear atributo 'distance' = 1/weight para usar en betweenness/closeness.\"\"\"\n",
    "    for u, v, d in Gu.edges(data=True):\n",
    "        w = float(d.get(\"weight\", 1.0))\n",
    "        d[\"distance\"] = 1.0 / w if w > 0 else 1e9\n",
    "    return Gu\n",
    "\n",
    "# ============== 1) Cargar grafos y preparar distancia ==============\n",
    "Gd, Gu = _load_graphs_from_61_or_rebuild()\n",
    "Gu = _ensure_distance_attr(Gu)\n",
    "\n",
    "print(f\"G_directed: {Gd.number_of_nodes()} nodos, {Gd.number_of_edges()} aristas\")\n",
    "print(f\"G_undirected: {Gu.number_of_nodes()} nodos, {Gu.number_of_edges()} aristas\")\n",
    "\n",
    "# ============== 2) Centralidad de grado (dirigido) ==============\n",
    "in_deg_cnt  = dict(Gd.in_degree())                       # sin peso\n",
    "out_deg_cnt = dict(Gd.out_degree())\n",
    "in_deg_w    = dict(Gd.in_degree(weight=\"weight\"))        # ponderado por 'weight'\n",
    "out_deg_w   = dict(Gd.out_degree(weight=\"weight\"))\n",
    "\n",
    "deg_df = pd.DataFrame({\n",
    "    \"user\": list(Gd.nodes())\n",
    "})\n",
    "deg_df[\"indegree\"]   = deg_df[\"user\"].map(in_deg_cnt).fillna(0).astype(int)\n",
    "deg_df[\"outdegree\"]  = deg_df[\"user\"].map(out_deg_cnt).fillna(0).astype(int)\n",
    "deg_df[\"indegree_w\"] = deg_df[\"user\"].map(in_deg_w).fillna(0).astype(float)\n",
    "deg_df[\"outdegree_w\"]= deg_df[\"user\"].map(out_deg_w).fillna(0).astype(float)\n",
    "deg_df[\"degree_w\"]   = deg_df[\"indegree_w\"] + deg_df[\"outdegree_w\"]\n",
    "\n",
    "top_degree_in   = deg_df.sort_values([\"indegree_w\",\"indegree\"], ascending=False).head(15)\n",
    "top_degree_sumw = deg_df.sort_values([\"degree_w\",\"indegree_w\"], ascending=False).head(15)\n",
    "\n",
    "print(\"\\nTop-15 por IN-degree ponderado (menciones/RT recibidos):\")\n",
    "display(top_degree_in[[\"user\",\"indegree_w\",\"indegree\",\"outdegree_w\",\"outdegree\"]])\n",
    "\n",
    "print(\"\\nTop-15 por Degree ponderado total (indegree_w + outdegree_w):\")\n",
    "display(top_degree_sumw[[\"user\",\"degree_w\",\"indegree_w\",\"outdegree_w\",\"indegree\",\"outdegree\"]])\n",
    "\n",
    "# ============== 3) Betweenness (no dirigido, con distancia) ==============\n",
    "# Intentar leer de archivo si existe para ahorrar tiempo\n",
    "precomp_path = None\n",
    "for p in [\"net_outputs/node_metrics.csv\", \"../net_outputs/node_metrics.csv\", \"lab6ds/net_outputs/node_metrics.csv\"]:\n",
    "    if os.path.exists(p):\n",
    "        precomp_path = p\n",
    "        break\n",
    "\n",
    "if precomp_path is not None:\n",
    "    node_metrics = pd.read_csv(precomp_path)\n",
    "    if \"betweenness\" in node_metrics.columns:\n",
    "        between = dict(zip(node_metrics[\"user\"], node_metrics[\"betweenness\"]))\n",
    "        print(\"\\n[Betweenness] Usando valores precomputados de:\", precomp_path)\n",
    "    else:\n",
    "        between = None\n",
    "else:\n",
    "    between = None\n",
    "\n",
    "if between is None:\n",
    "    # Aproximación por muestreo para hacerlo rápido en 5k nodos\n",
    "    # Puedes subir/bajar k según tu equipo (128–512 es razonable)\n",
    "    k_sample = 256\n",
    "    print(f\"\\n[Betweenness] Calculando aproximado con k={k_sample} (puede tardar unos minutos)...\")\n",
    "    between = nx.betweenness_centrality(Gu, k=k_sample, weight=\"distance\", normalized=True, seed=42)\n",
    "\n",
    "bet_df = pd.DataFrame({\"user\": list(between.keys()), \"betweenness\": list(between.values())})\n",
    "top_between = bet_df.sort_values(\"betweenness\", ascending=False).head(15)\n",
    "\n",
    "print(\"\\nTop-15 por Betweenness (puentes entre comunidades):\")\n",
    "display(top_between)\n",
    "\n",
    "# ============== 4) Closeness (no dirigido, LCC, con distancia) ==============\n",
    "# Tomamos sólo el mayor componente conexo para evitar valores triviales en nodos aislados.\n",
    "if nx.number_connected_components(Gu) > 1:\n",
    "    lcc_nodes = max(nx.connected_components(Gu), key=len)\n",
    "else:\n",
    "    lcc_nodes = set(Gu.nodes())\n",
    "\n",
    "H_lcc = Gu.subgraph(lcc_nodes).copy()\n",
    "close = nx.closeness_centrality(H_lcc, distance=\"distance\", wf_improved=True)\n",
    "close_df = pd.DataFrame({\"user\": list(close.keys()), \"closeness\": list(close.values())})\n",
    "top_close = close_df.sort_values(\"closeness\", ascending=False).head(15)\n",
    "\n",
    "print(\"\\nTop-15 por Closeness (alcance con menos saltos ponderados):\")\n",
    "display(top_close)\n",
    "\n",
    "# ============== 5) Resumen integrable ==============\n",
    "# Unimos (left join) para que puedas exportar si quieres usar en 7.2/7.3\n",
    "summary = (deg_df.merge(bet_df, on=\"user\", how=\"left\")\n",
    "                 .merge(close_df, on=\"user\", how=\"left\"))\n",
    "summary = summary.fillna({\"betweenness\": 0.0})  # por si betweenness aprox omitió algún nodo\n",
    "print(\"\\nResumen (cabezal):\")\n",
    "display(summary.head())\n",
    "\n",
    "# (Opcional) guardar en variable global para siguientes incisos\n",
    "RESULTS_71 = {\n",
    "    \"degree_table\": deg_df,\n",
    "    \"betweenness_table\": bet_df,\n",
    "    \"closeness_table\": close_df,\n",
    "    \"summary\": summary,\n",
    "    \"top_degree_in\": top_degree_in,\n",
    "    \"top_degree_sumw\": top_degree_sumw,\n",
    "    \"top_between\": top_between,\n",
    "    \"top_close\": top_close,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484be3c",
   "metadata": {},
   "source": [
    "# 7.1. Identificación de usuarios influyentes (centralidades)\n",
    "\n",
    "## Revisión de resultados\n",
    "\n",
    "- El tamaño del grafo es consistente con lo observado antes: 5,213 nodos.  \n",
    "- Al convertirlo en no dirigido, las aristas bajan de 19,476 a 19,294 lo cual es esperado.  \n",
    "\n",
    "### Atención recibida (in-degree ponderado)  \n",
    "- Los usuarios que más atención reciben son **barevalodeleon** y **traficogt**, con gran diferencia respecto al resto.  \n",
    "- Después aparecen instituciones y medios como **guatemalagob**, **mpguatemala**, **prensacomunitar** y **lahoragt**.  \n",
    "\n",
    "### Actividad global (degree ponderado total)  \n",
    "- Además de los anteriores, destacan cuentas con mucha actividad hacia afuera como **batallonjalapa** y **mildred_gaitan**.  \n",
    "- Estas cuentas tienen gran alcance por la cantidad de menciones que hacen, aunque no siempre se traducen en influencia real.  \n",
    "\n",
    "### Puentes entre comunidades (betweenness)  \n",
    "- De nuevo sobresalen **barevalodeleon** y **traficogt**, que actúan como conectores entre distintas comunidades.  \n",
    "- También aparecen cuentas institucionales y de medios que cumplen un rol de puente entre grupos.  \n",
    "\n",
    "### Cercanía en la red (closeness)  \n",
    "- Los valores mayores a 1 son normales aquí porque se definió la distancia como inversa al peso.  \n",
    "- En este indicador vuelven a sobresalir **barevalodeleon** y **traficogt**, junto con los broadcasters como **batallonjalapa** y **mildred_gaitan**, debido a su alta conectividad.  \n",
    "\n",
    "---\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "- **Influenciadores por atención recibida:** barevalodeleon, traficogt, guatemalagob, ubaldomacu, mpguatemala.  \n",
    "- **Puentes entre comunidades:** barevalodeleon, traficogt y nodos institucionales como guatemalagob, mpguatemala, congresoguate.  \n",
    "- **Cuentas centrales por cercanía:** además de los líderes anteriores, aparecen broadcasters como batallonjalapa y mildred_gaitan.  \n",
    "\n",
    "---\n",
    "\n",
    "## Advertencia metodológica\n",
    "\n",
    "Un alto nivel de actividad hacia afuera (out-degree) puede inflar las métricas de grado total y cercanía sin reflejar verdadera influencia.  \n",
    "Para interpretar influencia de manera más confiable:  \n",
    "- Priorizar **in-degree** (atención recibida).  \n",
    "- Considerar **betweenness** (puentes entre comunidades).  \n",
    "- Usar **closeness** solo como una señal adicional de accesibilidad dentro de la red.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 — Tablas con comunidad y ranking integrado de influencia\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Recuperar insumos de 6.1 y 7.1\n",
    "Gu = RESULTS_61[\"G_undirected\"]\n",
    "node2com = RESULTS_61[\"node2community\"]\n",
    "\n",
    "deg_df      = RESULTS_71[\"degree_table\"].copy()\n",
    "betweenness = RESULTS_71[\"betweenness_table\"].copy()\n",
    "closeness   = RESULTS_71[\"closeness_table\"].copy()\n",
    "\n",
    "# Añadir comunidad\n",
    "deg_df[\"community\"] = deg_df[\"user\"].map(lambda u: node2com.get(u, np.nan))\n",
    "betweenness[\"community\"] = betweenness[\"user\"].map(lambda u: node2com.get(u, np.nan))\n",
    "closeness[\"community\"]   = closeness[\"user\"].map(lambda u: node2com.get(u, np.nan))\n",
    "\n",
    "# Top-15 por métrica (con comunidad)\n",
    "top_in = (deg_df.sort_values([\"indegree_w\",\"indegree\"], ascending=False)\n",
    "                .loc[:, [\"user\",\"community\",\"indegree_w\",\"indegree\",\"outdegree_w\",\"outdegree\"]]\n",
    "                .head(15))\n",
    "\n",
    "top_degree_total = (deg_df.sort_values([\"degree_w\",\"indegree_w\"], ascending=False)\n",
    "                          .loc[:, [\"user\",\"community\",\"degree_w\",\"indegree_w\",\"outdegree_w\",\"indegree\",\"outdegree\"]]\n",
    "                          .head(15))\n",
    "\n",
    "top_between = (betweenness.sort_values(\"betweenness\", ascending=False)\n",
    "                          .loc[:, [\"user\",\"community\",\"betweenness\"]]\n",
    "                          .head(15))\n",
    "\n",
    "top_close = (closeness.sort_values(\"closeness\", ascending=False)\n",
    "                        .loc[:, [\"user\",\"community\",\"closeness\"]]\n",
    "                        .head(15))\n",
    "\n",
    "print(\"Top-15 — IN-degree ponderado (atención recibida)\")\n",
    "display(top_in)\n",
    "\n",
    "print(\"Top-15 — Degree ponderado total\")\n",
    "display(top_degree_total)\n",
    "\n",
    "print(\"Top-15 — Betweenness (puentes)\")\n",
    "display(top_between)\n",
    "\n",
    "print(\"Top-15 — Closeness (alcance ponderado)\")\n",
    "display(top_close)\n",
    "\n",
    "# ====== Ranking integrado (Borda simple sobre percentiles) ======\n",
    "# Normalizamos cada métrica a percentil [0,1] (mayor = mejor) y promediamos.\n",
    "df = (deg_df\n",
    "      .merge(betweenness, on=[\"user\",\"community\"], how=\"left\", suffixes=(\"\",\"\"))\n",
    "      .merge(closeness, on=[\"user\",\"community\"],  how=\"left\", suffixes=(\"\",\"\"))\n",
    "      .fillna({\"betweenness\":0.0, \"closeness\":0.0})\n",
    ")\n",
    "\n",
    "def pct_rank(s):\n",
    "    # percentil rank robusto\n",
    "    return s.rank(pct=True)\n",
    "\n",
    "df[\"score_in\"]   = pct_rank(df[\"indegree_w\"])      # atención recibida\n",
    "df[\"score_bet\"]  = pct_rank(df[\"betweenness\"])     # puente\n",
    "df[\"score_close\"]= pct_rank(df[\"closeness\"])       # accesibilidad\n",
    "# Peso mayor a atención y puentes; closeness como señal secundaria\n",
    "df[\"influence_score\"] = 0.4*df[\"score_in\"] + 0.4*df[\"score_bet\"] + 0.2*df[\"score_close\"]\n",
    "\n",
    "rank_integrado = (df.sort_values(\"influence_score\", ascending=False)\n",
    "                    .loc[:, [\"user\",\"community\",\n",
    "                             \"influence_score\",\n",
    "                             \"indegree_w\",\"betweenness\",\"closeness\",\n",
    "                             \"indegree\",\"outdegree\",\"outdegree_w\"]]\n",
    "                    .head(15))\n",
    "\n",
    "print(\"Top-15 — Ranking integrado (0–1): pondera atención (40%), puentes (40%), closeness (20%)\")\n",
    "display(rank_integrado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b0c40",
   "metadata": {},
   "source": [
    "# Inciso 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a945bc",
   "metadata": {},
   "source": [
    "# 8.1. Detección y análisis de grupos aislados\n",
    "\n",
    "### 1. Componentes aislados\n",
    "- En el grafo dirigido se usan los **Weakly Connected Components (WCC)**, que ignoran la dirección de los enlaces, para encontrar islas desconectadas del resto.  \n",
    "- En el grafo no dirigido se usan los **Connected Components (CC)** para confirmar tamaño y densidad.  \n",
    "- Se reporta:  \n",
    "  - Número de componentes  \n",
    "  - Nodos aislados (componentes de tamaño 1)  \n",
    "  - Subredes pequeñas (ejemplo: con 30 nodos o menos)  \n",
    "\n",
    "### 2. Nichos con baja interacción externa\n",
    "- A partir de las comunidades Louvain identificadas en el paso 6, se calcula:  \n",
    "  - **Ratio externo = (peso de interacciones hacia afuera) / (peso total interno + externo)**  \n",
    "- Se detectan nichos con ratio externo bajo (ejemplo: menor a 0.15) y tamaño suficiente (para evitar casos triviales).  \n",
    "- Para estos nichos se muestran:  \n",
    "  - Usuarios principales según menciones recibidas (indegree local)  \n",
    "  - Palabras frecuentes en sus publicaciones, para interpretar la temática  \n",
    "\n",
    "---\n",
    "\n",
    "## Notas\n",
    "- Los **componentes aislados** no tienen conexiones hacia el resto por definición.  \n",
    "- Los **nichos** sí están dentro del componente principal, pero muestran poca interacción con otras comunidades, lo que refleja un bajo nivel de conexión externa.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823a69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
