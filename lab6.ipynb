{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76955651",
   "metadata": {},
   "source": [
    "## **Laboratorio 6**\n",
    "- Joaquín Campos - 22155\n",
    "- Sofía García - 22210\n",
    "- Julio García Salas - 22076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f1bdb",
   "metadata": {},
   "source": [
    "## **Inciso 1 y 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba436c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESUMEN DE CARGA (Paso 1–2) ===\n",
      "Archivos buscados (existentes marcados como OK más abajo):\n",
      " - traficogt.txt\n",
      " - tioberny.txt\n",
      "- traficogt.txt   | estado=OK      | líneas=11209 | objetos_JSON=    0\n",
      "- tioberny.txt    | estado=OK      | líneas=10039 | objetos_JSON=    0\n",
      "\n",
      "Total de filas normalizadas: 0\n",
      "Nota: No se generaron archivos de salida porque no se detectaron objetos JSON válidos.\n"
     ]
    }
   ],
   "source": [
    "# Paso 1–2 (versión robusta): Cargar y normalizar tweets\n",
    "# Soporta:\n",
    "#  - JSON array / objeto / JSONL (una por línea)\n",
    "#  - Texto plano: 1 tweet por línea (fallback), con extracción de @menciones, #hashtags, RT y reply\n",
    "#\n",
    "# Salidas:\n",
    "#  - Imprime \"RESUMEN DE CARGA (Paso 1–2)\" al final\n",
    "#  - Guarda 'tweets_raw_sample.csv' con 50 filas si hay datos\n",
    "#  - Intenta guardar 'tweets_raw.parquet' si tienes pyarrow/fastparquet\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# --- Localiza archivos en el directorio actual o en /mnt/data ---\n",
    "def resolve_paths():\n",
    "    local = [Path(\"traficogt.txt\"), Path(\"tioberny.txt\")]\n",
    "    mnt = [Path(\"/mnt/data/traficogt.txt\"), Path(\"/mnt/data/tioberny.txt\")]\n",
    "    final = []\n",
    "    for p_local, p_mnt in zip(local, mnt):\n",
    "        if p_local.exists():\n",
    "            final.append(p_local)\n",
    "        elif p_mnt.exists():\n",
    "            final.append(p_mnt)\n",
    "        else:\n",
    "            final.append(p_local)  # por si están en otra ruta; lo marcamos como NO_FILE más abajo\n",
    "    return final\n",
    "\n",
    "DATA_PATHS = resolve_paths()\n",
    "OUT_PARQUET = Path(\"tweets_raw.parquet\")\n",
    "OUT_SAMPLE_CSV = Path(\"tweets_raw_sample.csv\")\n",
    "\n",
    "# --- Utilidades de parseo ---\n",
    "def read_any_json_whole(raw_stripped: str):\n",
    "    \"\"\"Intenta parsear el contenido completo como JSON válido.\"\"\"\n",
    "    try:\n",
    "        obj = json.loads(raw_stripped)\n",
    "        if isinstance(obj, list):\n",
    "            return [x for x in obj if isinstance(x, dict)]\n",
    "        if isinstance(obj, dict):\n",
    "            if \"tweets\" in obj and isinstance(obj[\"tweets\"], list):\n",
    "                return [x for x in obj[\"tweets\"] if isinstance(x, dict)]\n",
    "            if \"data\" in obj and isinstance(obj[\"data\"], list):\n",
    "                return [x for x in obj[\"data\"] if isinstance(x, dict)]\n",
    "            return [obj]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def read_jsonl_lines(raw: str):\n",
    "    \"\"\"Intenta parsear línea por línea como JSONL. Devuelve (records, n_json_ok).\"\"\"\n",
    "    recs = []\n",
    "    ok = 0\n",
    "    for line in raw.splitlines():\n",
    "        s = line.strip().rstrip(\",\")\n",
    "        if not s:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(s)\n",
    "            if isinstance(obj, dict):\n",
    "                recs.append(obj)\n",
    "                ok += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "    return recs, ok\n",
    "\n",
    "def extract_json_from_line(line: str):\n",
    "    \"\"\"\n",
    "    Intenta rescatar un bloque {...} en una línea que no es JSON puro.\n",
    "    Toma del primer '{' al último '}' y hace json.loads.\n",
    "    \"\"\"\n",
    "    start = line.find(\"{\")\n",
    "    end = line.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        blob = line[start:end+1]\n",
    "        try:\n",
    "            obj = json.loads(blob)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def norm_username(u: Optional[str]) -> Optional[str]:\n",
    "    if not u:\n",
    "        return u\n",
    "    u = u.strip()\n",
    "    if u.startswith(\"@\"):\n",
    "        u = u[1:]\n",
    "    return u.lower()\n",
    "\n",
    "def extract_list_usernames(mentioned: Any) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    if isinstance(mentioned, list):\n",
    "        for m in mentioned:\n",
    "            if isinstance(m, dict):\n",
    "                un = m.get(\"username\") or m.get(\"screen_name\") or m.get(\"name\")\n",
    "                if un:\n",
    "                    out.append(norm_username(un))\n",
    "            elif isinstance(m, str):\n",
    "                out.append(norm_username(m))\n",
    "    return [x for x in out if x]\n",
    "\n",
    "def hashtags_to_list(h: Any) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    if isinstance(h, list):\n",
    "        for item in h:\n",
    "            if isinstance(item, str):\n",
    "                out.append(item.lstrip(\"#\").lower())\n",
    "            elif isinstance(item, dict):\n",
    "                txt = item.get(\"text\") or item.get(\"tag\")\n",
    "                if txt:\n",
    "                    out.append(str(txt).lstrip(\"#\").lower())\n",
    "    return out\n",
    "\n",
    "def get_text(rec: Dict[str, Any]) -> Optional[str]:\n",
    "    for k in (\"rawContent\", \"full_text\", \"text\"):\n",
    "        val = rec.get(k)\n",
    "        if isinstance(val, str) and val.strip():\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "def get_user_obj(rec: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    u = rec.get(\"user\")\n",
    "    return u if isinstance(u, dict) else None\n",
    "\n",
    "def safe_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Regex para fallback de texto plano\n",
    "MENTION_RE = re.compile(r\"@([A-Za-z0-9_]{1,15})\")\n",
    "HASHTAG_RE = re.compile(r\"#([A-Za-z0-9_]+)\")\n",
    "\n",
    "def rows_from_json(records: List[Dict[str, Any]], source_file: str) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for r in records:\n",
    "        u = get_user_obj(r)\n",
    "        uname = None\n",
    "        uid = None\n",
    "        if u:\n",
    "            uname = u.get(\"username\") or u.get(\"screen_name\") or u.get(\"name\")\n",
    "            uid = u.get(\"id\") or u.get(\"id_str\")\n",
    "\n",
    "        # Mentions\n",
    "        mentions = []\n",
    "        if \"mentionedUsers\" in r:\n",
    "            mentions = extract_list_usernames(r.get(\"mentionedUsers\"))\n",
    "        elif \"entities\" in r and isinstance(r[\"entities\"], dict):\n",
    "            mentions = extract_list_usernames(r[\"entities\"].get(\"user_mentions\"))\n",
    "\n",
    "        # Hashtags\n",
    "        if \"hashtags\" in r:\n",
    "            tags = hashtags_to_list(r.get(\"hashtags\"))\n",
    "        elif \"entities\" in r and isinstance(r[\"entities\"], dict):\n",
    "            tags = hashtags_to_list(r[\"entities\"].get(\"hashtags\"))\n",
    "        else:\n",
    "            tags = []\n",
    "\n",
    "        # RT / Quote\n",
    "        rt = r.get(\"retweetedTweet\")\n",
    "        qt = r.get(\"quotedTweet\")\n",
    "        is_rt = rt is not None\n",
    "        is_qt = qt is not None\n",
    "        rt_user = norm_username(rt.get(\"user\", {}).get(\"username\")) if isinstance(rt, dict) else None\n",
    "        qt_user = norm_username(qt.get(\"user\", {}).get(\"username\")) if isinstance(qt, dict) else None\n",
    "\n",
    "        # Reply\n",
    "        in_reply_to_user = r.get(\"inReplyToUser\")\n",
    "        reply_to_username = None\n",
    "        if isinstance(in_reply_to_user, dict):\n",
    "            reply_to_username = norm_username(in_reply_to_user.get(\"username\"))\n",
    "        if not reply_to_username and r.get(\"in_reply_to_screen_name\"):\n",
    "            reply_to_username = norm_username(r.get(\"in_reply_to_screen_name\"))\n",
    "\n",
    "        # Métricas\n",
    "        like_count = safe_int(r.get(\"likeCount\") or r.get(\"favorite_count\"))\n",
    "        rt_count = safe_int(r.get(\"retweetCount\") or r.get(\"retweet_count\"))\n",
    "        reply_count = safe_int(r.get(\"replyCount\") or r.get(\"reply_count\"))\n",
    "        quote_count = safe_int(r.get(\"quoteCount\") or r.get(\"quote_count\"))\n",
    "        view_count = safe_int(r.get(\"viewCount\") or r.get(\"views\"))\n",
    "\n",
    "        # Fecha\n",
    "        date_raw = r.get(\"date\") or r.get(\"created_at\")\n",
    "        try:\n",
    "            date_parsed = pd.to_datetime(date_raw)\n",
    "        except Exception:\n",
    "            date_parsed = pd.NaT\n",
    "\n",
    "        rows.append({\n",
    "            \"source_file\": source_file,\n",
    "            \"tweet_id\": r.get(\"id\") or r.get(\"id_str\"),\n",
    "            \"date\": date_parsed,\n",
    "            \"lang\": r.get(\"lang\"),\n",
    "            \"username\": norm_username(uname),\n",
    "            \"user_id\": uid,\n",
    "            \"text\": get_text(r),\n",
    "            \"mentions\": mentions,\n",
    "            \"hashtags\": tags,\n",
    "            \"is_retweet\": bool(is_rt),\n",
    "            \"is_quote\": bool(is_qt),\n",
    "            \"retweeted_user\": rt_user,\n",
    "            \"quoted_user\": qt_user,\n",
    "            \"reply_to_user\": reply_to_username,\n",
    "            \"in_reply_to_tweet_id\": r.get(\"inReplyToTweetId\") or r.get(\"in_reply_to_status_id_str\") or r.get(\"in_reply_to_status_id\"),\n",
    "            \"like_count\": like_count,\n",
    "            \"retweet_count\": rt_count,\n",
    "            \"reply_count\": reply_count,\n",
    "            \"quote_count\": quote_count,\n",
    "            \"view_count\": view_count,\n",
    "            \"raw_record\": r,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def rows_from_plaintext(lines: List[str], source_file: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fallback para texto plano: 1 tweet por línea.\n",
    "    Deriva menciones/hashtags/RT/reply desde el texto.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for i, raw in enumerate(lines, start=1):\n",
    "        txt = raw.strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        # Detecta RT y usuario retuiteado (formato típico \"RT @usuario: ...\")\n",
    "        is_rt = False\n",
    "        rt_user = None\n",
    "        m_rt = re.match(r\"^\\s*RT\\s+@([A-Za-z0-9_]{1,15})\\b\", txt)\n",
    "        if m_rt:\n",
    "            is_rt = True\n",
    "            rt_user = m_rt.group(1).lower()\n",
    "\n",
    "        # Menciones y hashtags\n",
    "        mentions = [m.lower() for m in MENTION_RE.findall(txt)]\n",
    "        hashtags = [h.lower() for h in HASHTAG_RE.findall(txt)]\n",
    "\n",
    "        # Reply si inicia con @usuario\n",
    "        reply_to_user = None\n",
    "        m_reply = re.match(r\"^\\s*@([A-Za-z0-9_]{1,15})\\b\", txt)\n",
    "        if m_reply:\n",
    "            reply_to_user = m_reply.group(1).lower()\n",
    "\n",
    "        rows.append({\n",
    "            \"source_file\": source_file,\n",
    "            \"tweet_id\": f\"{source_file}:{i}\",  # ID sintético basado en línea\n",
    "            \"date\": pd.NaT,\n",
    "            \"lang\": None,\n",
    "            \"username\": None,        # desconocido en texto plano\n",
    "            \"user_id\": None,\n",
    "            \"text\": txt,\n",
    "            \"mentions\": mentions,\n",
    "            \"hashtags\": hashtags,\n",
    "            \"is_retweet\": is_rt,\n",
    "            \"is_quote\": False,       # no detectable con solo texto plano\n",
    "            \"retweeted_user\": rt_user,\n",
    "            \"quoted_user\": None,\n",
    "            \"reply_to_user\": reply_to_user,\n",
    "            \"in_reply_to_tweet_id\": None,\n",
    "            \"like_count\": None,\n",
    "            \"retweet_count\": None,\n",
    "            \"reply_count\": None,\n",
    "            \"quote_count\": None,\n",
    "            \"view_count\": None,\n",
    "            \"raw_record\": {\"_raw_line\": txt},\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# --- Proceso de carga para cada archivo ---\n",
    "all_rows: List[Dict[str, Any]] = []\n",
    "summary = []\n",
    "for p in DATA_PATHS:\n",
    "    status = \"NO_FILE\"\n",
    "    n_lines = 0\n",
    "    n_json_whole = 0\n",
    "    n_jsonl = 0\n",
    "    n_json_inline = 0\n",
    "    used_plaintext = 0\n",
    "\n",
    "    if p.exists():\n",
    "        status = \"OK\"\n",
    "        raw = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        lines = raw.splitlines()\n",
    "        n_lines = len(lines)\n",
    "\n",
    "        # 1) Intenta JSON global\n",
    "        recs = read_any_json_whole(raw.strip())\n",
    "        if isinstance(recs, list) and recs:\n",
    "            all_rows.extend(rows_from_json(recs, p.name))\n",
    "            n_json_whole = len(recs)\n",
    "        else:\n",
    "            # 2) JSONL\n",
    "            recs_jsonl, ok = read_jsonl_lines(raw)\n",
    "            if ok > 0:\n",
    "                all_rows.extend(rows_from_json(recs_jsonl, p.name))\n",
    "                n_jsonl = ok\n",
    "            else:\n",
    "                # 3) Intento por línea: JSON incrustado o texto plano\n",
    "                temp_rows = []\n",
    "                for line in lines:\n",
    "                    obj = extract_json_from_line(line)\n",
    "                    if obj is not None:\n",
    "                        n_json_inline += 1\n",
    "                        temp_rows.extend(rows_from_json([obj], p.name))\n",
    "                    else:\n",
    "                        # fallback texto plano\n",
    "                        temp_rows.extend(rows_from_plaintext([line], p.name))\n",
    "                        used_plaintext += 1\n",
    "                all_rows.extend(temp_rows)\n",
    "\n",
    "    summary.append({\n",
    "        \"file\": p.name,\n",
    "        \"status\": status,\n",
    "        \"n_lines\": n_lines,\n",
    "        \"n_json_whole\": n_json_whole,\n",
    "        \"n_jsonl\": n_jsonl,\n",
    "        \"n_json_inline\": n_json_inline,\n",
    "        \"used_plaintext\": used_plaintext,\n",
    "    })\n",
    "\n",
    "# --- DataFrame final ---\n",
    "df = pd.DataFrame(all_rows)\n",
    "if not df.empty:\n",
    "    # Ordena por fecha si existe\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.sort_values(\"date\", na_position=\"last\").reset_index(drop=True)\n",
    "\n",
    "# --- Guardados para siguientes incisos ---\n",
    "if not df.empty:\n",
    "    OUT_SAMPLE_CSV.write_text(\"\")  # asegura que el path exista en algunos entornos\n",
    "    df.head(50).to_csv(OUT_SAMPLE_CSV, index=False)\n",
    "    try:\n",
    "        df.to_parquet(OUT_PARQUET, index=False)\n",
    "        parquet_path = str(OUT_PARQUET.resolve())\n",
    "    except Exception:\n",
    "        parquet_path = \"(No se guardó Parquet: instala 'pyarrow' o 'fastparquet')\"\n",
    "else:\n",
    "    parquet_path = \"(DataFrame vacío)\"\n",
    "\n",
    "# --- Resumen ---\n",
    "print(\"=== RESUMEN DE CARGA (Paso 1–2) ===\")\n",
    "print(\"Archivos buscados:\")\n",
    "for p in DATA_PATHS:\n",
    "    print(\" -\", p)\n",
    "\n",
    "for s in summary:\n",
    "    print(f\"- {s['file']:15s} | estado={s['status']:7s} | líneas={s['n_lines']:5d} | \"\n",
    "          f\"JSON_global={s['n_json_whole']:5d} | JSONL={s['n_jsonl']:5d} | \"\n",
    "          f\"JSON_inline={s['n_json_inline']:5d} | texto_plano={s['used_plaintext']:5d}\")\n",
    "\n",
    "print(f\"\\nTotal de filas normalizadas: {len(df):,}\")\n",
    "if not df.empty:\n",
    "    print(\"Columnas:\", list(df.columns))\n",
    "    print(f\"Muestra CSV (50 filas): {str(OUT_SAMPLE_CSV.resolve())}\")\n",
    "    print(f\"Parquet: {parquet_path}\")\n",
    "else:\n",
    "    print(\"Nota: DataFrame vacío. Si tus archivos tienen un formato distinto, compárteme 10–15 líneas de ejemplo.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
